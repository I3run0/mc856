{"1.0.1":{"info":{"author":"ZhangDun","author_email":"dunnzhang0@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 2 - Pre-Alpha","Intended Audience :: Developers","Intended Audience :: Science/Research","License :: OSI Approved :: MIT License","Operating System :: MacOS","Operating System :: Microsoft :: Windows","Operating System :: POSIX","Operating System :: Unix","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/DunZhang/KnowledgeDistillation","keywords":"Transformer Networks BERT XLNet PyTorch NLP deep learning","license":"MIT","maintainer":"","maintainer_email":"","name":"KnowledgeDistillation","package_url":"https://pypi.org/project/KnowledgeDistillation/","platform":"","project_url":"https://pypi.org/project/KnowledgeDistillation/","project_urls":{"Homepage":"https://github.com/DunZhang/KnowledgeDistillation"},"provides_extra":null,"release_url":"https://pypi.org/project/KnowledgeDistillation/1.0.1/","requires_dist":null,"requires_python":"","summary":"A general knowledge distillation framework","version":"1.0.1","yanked":false,"yanked_reason":null},"last_serial":8334991,"urls":[{"comment_text":"","digests":{"blake2b_256":"4e3f01c99670cb0c507344686a52164f7705cf0bea0438c70460ccbc655612be","md5":"ad1e3e7a8fedd91c68f8bc74079ba498","sha256":"6d6d4f76894fc8681dae4b1d873efa36ecffc89c38df7339f1a7e8746666c9f5"},"downloads":-1,"filename":"KnowledgeDistillation-1.0.1.tar.gz","has_sig":false,"md5_digest":"ad1e3e7a8fedd91c68f8bc74079ba498","packagetype":"sdist","python_version":"source","requires_python":null,"size":7433,"upload_time":"2020-03-08T07:37:27","upload_time_iso_8601":"2020-03-08T07:37:27.787997Z","url":"https://files.pythonhosted.org/packages/4e/3f/01c99670cb0c507344686a52164f7705cf0bea0438c70460ccbc655612be/KnowledgeDistillation-1.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.2":{"info":{"author":"ZhangDun","author_email":"dunnzhang0@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 2 - Pre-Alpha","Intended Audience :: Developers","Intended Audience :: Science/Research","License :: OSI Approved :: MIT License","Operating System :: MacOS","Operating System :: Microsoft :: Windows","Operating System :: POSIX","Operating System :: Unix","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/DunZhang/KnowledgeDistillation","keywords":"Transformer Networks BERT XLNet PyTorch NLP deep learning","license":"MIT","maintainer":"","maintainer_email":"","name":"KnowledgeDistillation","package_url":"https://pypi.org/project/KnowledgeDistillation/","platform":"","project_url":"https://pypi.org/project/KnowledgeDistillation/","project_urls":{"Homepage":"https://github.com/DunZhang/KnowledgeDistillation"},"provides_extra":null,"release_url":"https://pypi.org/project/KnowledgeDistillation/1.0.2/","requires_dist":null,"requires_python":"","summary":"A general knowledge distillation framework","version":"1.0.2","yanked":false,"yanked_reason":null},"last_serial":8334991,"urls":[{"comment_text":"","digests":{"blake2b_256":"bc90d881040a14dc208482913870722627a884bb675f1b947e4451dbec7adf8e","md5":"d7291d2e8dc3a83dfe485fc28d8fd880","sha256":"f5a408fc40d977341d229ef5f86a9c4e26cc0d1b4f71536f2130eaa6d748479e"},"downloads":-1,"filename":"KnowledgeDistillation-1.0.2.tar.gz","has_sig":false,"md5_digest":"d7291d2e8dc3a83dfe485fc28d8fd880","packagetype":"sdist","python_version":"source","requires_python":null,"size":8002,"upload_time":"2020-03-10T13:29:37","upload_time_iso_8601":"2020-03-10T13:29:37.265383Z","url":"https://files.pythonhosted.org/packages/bc/90/d881040a14dc208482913870722627a884bb675f1b947e4451dbec7adf8e/KnowledgeDistillation-1.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.4":{"info":{"author":"ZhangDun","author_email":"dunnzhang0@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 2 - Pre-Alpha","Intended Audience :: Developers","Intended Audience :: Science/Research","License :: OSI Approved :: MIT License","Operating System :: MacOS","Operating System :: Microsoft :: Windows","Operating System :: POSIX","Operating System :: Unix","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/DunZhang/KnowledgeDistillation","keywords":"Transformer Networks BERT XLNet PyTorch NLP deep learning","license":"MIT","maintainer":"","maintainer_email":"","name":"KnowledgeDistillation","package_url":"https://pypi.org/project/KnowledgeDistillation/","platform":"","project_url":"https://pypi.org/project/KnowledgeDistillation/","project_urls":{"Homepage":"https://github.com/DunZhang/KnowledgeDistillation"},"provides_extra":null,"release_url":"https://pypi.org/project/KnowledgeDistillation/1.0.4/","requires_dist":null,"requires_python":"","summary":"A general knowledge distillation framework","version":"1.0.4","yanked":false,"yanked_reason":null},"last_serial":8334991,"urls":[{"comment_text":"","digests":{"blake2b_256":"33f0c0abf7613f49534a60976bb7db867985be96046e055f96f7fec38e00d713","md5":"857ac9743af3e2b34cf89406d04b1e8e","sha256":"8de4aae3460ad4eff39c6b636f6ff2706f4286247277ed529a6ec401e7e2aa8a"},"downloads":-1,"filename":"KnowledgeDistillation-1.0.4.tar.gz","has_sig":false,"md5_digest":"857ac9743af3e2b34cf89406d04b1e8e","packagetype":"sdist","python_version":"source","requires_python":null,"size":11120,"upload_time":"2020-10-04T02:02:50","upload_time_iso_8601":"2020-10-04T02:02:50.465067Z","url":"https://files.pythonhosted.org/packages/33/f0/c0abf7613f49534a60976bb7db867985be96046e055f96f7fec38e00d713/KnowledgeDistillation-1.0.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}