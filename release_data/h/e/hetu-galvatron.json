{"0.0.1":{"info":{"author":"Yujie Wang, Shenhan Zhu","author_email":"alfredwang@pku.edu.cn, shenhan.zhu@pku.edu.cn","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"hetu-galvatron","package_url":"https://pypi.org/project/hetu-galvatron/","platform":null,"project_url":"https://pypi.org/project/hetu-galvatron/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/hetu-galvatron/0.0.1/","requires_dist":["transformers>=4.31.0","h5py>=3.6.0","pybind11>=2.9.1","timm>=0.5.4","attrs>=21.4.0","yacs>=0.1.8","sentencepiece","flash-attn>=2.0.8"],"requires_python":">=3.8","summary":"Galvatron, a Efficient Transformer Training Framework for Multiple GPUs Using Automatic Parallelism","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":20838223,"urls":[{"comment_text":"","digests":{"blake2b_256":"0e11d972c4208757bfd420488ff68f65626dbc20e331a3d3cb5264fb7e8c0b3b","md5":"914be80c05c4cf36bfc8cbbf89ee70f7","sha256":"0b8741bc907b4e7b2662c7f194df95e0b6706e8af280d1930177c12f1489c50d"},"downloads":-1,"filename":"hetu-galvatron-0.0.1.tar.gz","has_sig":false,"md5_digest":"914be80c05c4cf36bfc8cbbf89ee70f7","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":323636,"upload_time":"2023-10-18T13:54:47","upload_time_iso_8601":"2023-10-18T13:54:47.864040Z","url":"https://files.pythonhosted.org/packages/0e/11/d972c4208757bfd420488ff68f65626dbc20e331a3d3cb5264fb7e8c0b3b/hetu-galvatron-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.2":{"info":{"author":"Yujie Wang, Shenhan Zhu","author_email":"alfredwang@pku.edu.cn, shenhan.zhu@pku.edu.cn","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"hetu-galvatron","package_url":"https://pypi.org/project/hetu-galvatron/","platform":null,"project_url":"https://pypi.org/project/hetu-galvatron/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/hetu-galvatron/0.0.2/","requires_dist":["transformers>=4.31.0","h5py>=3.6.0","pybind11>=2.9.1","timm>=0.5.4","attrs>=21.4.0","yacs>=0.1.8","sentencepiece"],"requires_python":">=3.8","summary":"Galvatron, a Efficient Transformer Training Framework for Multiple GPUs Using Automatic Parallelism","version":"0.0.2","yanked":false,"yanked_reason":null},"last_serial":20838223,"urls":[{"comment_text":"","digests":{"blake2b_256":"bfed1279f2ea828bf211be1010b2dc2d8f3a575eff37e313d96cae002de0ddcf","md5":"a5b403c39d4bf6e635afe8aaec183413","sha256":"1ce343961b3b71e56963f143d9352949b47b4890072e526e2a31dd499411b3e5"},"downloads":-1,"filename":"hetu-galvatron-0.0.2.tar.gz","has_sig":false,"md5_digest":"a5b403c39d4bf6e635afe8aaec183413","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":323673,"upload_time":"2023-11-06T09:45:06","upload_time_iso_8601":"2023-11-06T09:45:06.781539Z","url":"https://files.pythonhosted.org/packages/bf/ed/1279f2ea828bf211be1010b2dc2d8f3a575eff37e313d96cae002de0ddcf/hetu-galvatron-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.0":{"info":{"author":"Yujie Wang, Shenhan Zhu","author_email":"alfredwang@pku.edu.cn, shenhan.zhu@pku.edu.cn","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"hetu-galvatron","package_url":"https://pypi.org/project/hetu-galvatron/","platform":null,"project_url":"https://pypi.org/project/hetu-galvatron/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/hetu-galvatron/1.0.0/","requires_dist":["torch==2.0.1","torchvision==0.15.2","transformers>=4.31.0","h5py>=3.6.0","attrs>=21.4.0","yacs>=0.1.8","six>=1.15.0","sentencepiece>=0.1.95"],"requires_python":">=3.8","summary":"Galvatron, a Efficient Transformer Training Framework for Multiple GPUs Using Automatic Parallelism","version":"1.0.0","yanked":false,"yanked_reason":null},"last_serial":20838223,"urls":[{"comment_text":"","digests":{"blake2b_256":"a1924d498934d1717e8b85131f16abd7f93d6f04961ffe9ccb4416957ebf6fab","md5":"441b4cddea981179366f175e26eee336","sha256":"06218a820213b67f428adfe90e98a896a728e1b1936c93befba6fd6ad714974c"},"downloads":-1,"filename":"hetu-galvatron-1.0.0.tar.gz","has_sig":false,"md5_digest":"441b4cddea981179366f175e26eee336","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":356448,"upload_time":"2023-11-29T14:51:42","upload_time_iso_8601":"2023-11-29T14:51:42.295231Z","url":"https://files.pythonhosted.org/packages/a1/92/4d498934d1717e8b85131f16abd7f93d6f04961ffe9ccb4416957ebf6fab/hetu-galvatron-1.0.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}