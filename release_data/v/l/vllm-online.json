{"0.4.1":{"info":{"author":"vLLM Team","author_email":null,"bugtrack_url":null,"classifiers":["License :: OSI Approved :: Apache Software License","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/vllm-project/vllm","keywords":null,"license":"Apache 2.0","maintainer":null,"maintainer_email":null,"name":"vllm-online","package_url":"https://pypi.org/project/vllm-online/","platform":null,"project_url":"https://pypi.org/project/vllm-online/","project_urls":{"Documentation":"https://vllm.readthedocs.io/en/latest/","Homepage":"https://github.com/vllm-project/vllm"},"provides_extra":null,"release_url":"https://pypi.org/project/vllm-online/0.4.1/","requires_dist":null,"requires_python":">=3.8","summary":"A high-throughput and memory-efficient inference and serving engine for LLMs","version":"0.4.1","yanked":false,"yanked_reason":null},"last_serial":22978197,"urls":[{"comment_text":"","digests":{"blake2b_256":"c74b6e8ae4d7aafda394488c066645844cd3210bc475f2ad68305e6d94e40c60","md5":"c5b7213f68dcad9dfa856ab2c9be5dd2","sha256":"4ee16122f2c9a19a789cd0d3f40ad6a32d909dfabad4acd137922f7fc656de8c"},"downloads":-1,"filename":"vllm-online-0.4.1.tar.gz","has_sig":false,"md5_digest":"c5b7213f68dcad9dfa856ab2c9be5dd2","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":531323,"upload_time":"2024-04-29T00:17:12","upload_time_iso_8601":"2024-04-29T00:17:12.021007Z","url":"https://files.pythonhosted.org/packages/c7/4b/6e8ae4d7aafda394488c066645844cd3210bc475f2ad68305e6d94e40c60/vllm-online-0.4.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.4.2":{"info":{"author":"vLLM Team","author_email":null,"bugtrack_url":null,"classifiers":["License :: OSI Approved :: Apache Software License","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/vllm-project/vllm","keywords":null,"license":"Apache 2.0","maintainer":null,"maintainer_email":null,"name":"vllm-online","package_url":"https://pypi.org/project/vllm-online/","platform":null,"project_url":"https://pypi.org/project/vllm-online/","project_urls":{"Documentation":"https://vllm.readthedocs.io/en/latest/","Homepage":"https://github.com/vllm-project/vllm"},"provides_extra":null,"release_url":"https://pypi.org/project/vllm-online/0.4.2/","requires_dist":null,"requires_python":">=3.8","summary":"A high-throughput and memory-efficient inference and serving engine for LLMs","version":"0.4.2","yanked":false,"yanked_reason":null},"last_serial":22978197,"urls":[{"comment_text":"","digests":{"blake2b_256":"a905c8d528b9c67eff3e1ff451ab0126433513df4455ada65ca3722516fc7b20","md5":"049b2a4e1e03edd2f9cdc55086f634a2","sha256":"3759f3b5f048459185ebd11449ad2dfcc8a3d8f5485fc06c2c8fd0da7f4a9d65"},"downloads":-1,"filename":"vllm-online-0.4.2.tar.gz","has_sig":false,"md5_digest":"049b2a4e1e03edd2f9cdc55086f634a2","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":532913,"upload_time":"2024-04-29T02:49:29","upload_time_iso_8601":"2024-04-29T02:49:29.215868Z","url":"https://files.pythonhosted.org/packages/a9/05/c8d528b9c67eff3e1ff451ab0126433513df4455ada65ca3722516fc7b20/vllm-online-0.4.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}