{"0.0.1":{"info":{"author":"demoriarty","author_email":"sahbanjan@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.0.1/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"f16799e58f78921488e4e9b7eba21f3e96ea074bd6520cbddadd9492fe357b64","md5":"d35c829eca46d7672d90f200e173cbf0","sha256":"dd527b9864838ec02fb0a20f46b16e3e2a4562fc870afef2ae32d3a9b949c9e1"},"downloads":-1,"filename":"warp_attention-0.0.1.tar.gz","has_sig":false,"md5_digest":"d35c829eca46d7672d90f200e173cbf0","packagetype":"sdist","python_version":"source","requires_python":null,"size":4571,"upload_time":"2023-08-06T23:03:18","upload_time_iso_8601":"2023-08-06T23:03:18.805692Z","url":"https://files.pythonhosted.org/packages/f1/67/99e58f78921488e4e9b7eba21f3e96ea074bd6520cbddadd9492fe357b64/warp_attention-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.2":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.0.2/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.0.2","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"77428d7de8a8b0307ac69f9a904e0d8c5ffaa7961ce5b926cd8f546bf58f8d3a","md5":"eeb43503bee08cee90784a50c7bebbc0","sha256":"619b6bf3d4a78e53f4fd83951e477bd549db6ef6b58cccdd1f989d4c6fbb9c9b"},"downloads":-1,"filename":"warp_attention-0.0.2.tar.gz","has_sig":false,"md5_digest":"eeb43503bee08cee90784a50c7bebbc0","packagetype":"sdist","python_version":"source","requires_python":null,"size":30399138,"upload_time":"2023-08-06T23:13:42","upload_time_iso_8601":"2023-08-06T23:13:42.270829Z","url":"https://files.pythonhosted.org/packages/77/42/8d7de8a8b0307ac69f9a904e0d8c5ffaa7961ce5b926cd8f546bf58f8d3a/warp_attention-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.0":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.0/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.0","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"6561f2d8f1c56091984eda765af476d0f2ab98e4968b4656b90a9a2468b6b605","md5":"eb7015a332bf8f41002b4e67e3d2aadc","sha256":"41ba8ebafcd8894de7807e56502334cafc5e2c737cfc50efda6abfcc86aad1c9"},"downloads":-1,"filename":"warp_attention-0.1.0.tar.gz","has_sig":false,"md5_digest":"eb7015a332bf8f41002b4e67e3d2aadc","packagetype":"sdist","python_version":"source","requires_python":null,"size":92525143,"upload_time":"2023-09-25T16:47:13","upload_time_iso_8601":"2023-09-25T16:47:13.806044Z","url":"https://files.pythonhosted.org/packages/65/61/f2d8f1c56091984eda765af476d0f2ab98e4968b4656b90a9a2468b6b605/warp_attention-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.3/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"70826cfc152c9ac94b1b38bc63bc0492ec725444be8c83573b9d56b23c9709a4","md5":"a70bd4a80a158df900c6c6168bb33605","sha256":"b9ca7f0406374f76953c970bccf2f71ee8bebda91a6f435e430d4c04b401b3d8"},"downloads":-1,"filename":"warp_attention-0.1.3.tar.gz","has_sig":false,"md5_digest":"a70bd4a80a158df900c6c6168bb33605","packagetype":"sdist","python_version":"source","requires_python":null,"size":149621,"upload_time":"2023-10-04T16:27:21","upload_time_iso_8601":"2023-10-04T16:27:21.020369Z","url":"https://files.pythonhosted.org/packages/70/82/6cfc152c9ac94b1b38bc63bc0492ec725444be8c83573b9d56b23c9709a4/warp_attention-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.4/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"69a7c26fd5dd33df76a2902fc155dfbcf634f60bf80b688076ae0fe304174bee","md5":"5af2a651729c4312d10eeec4218857b2","sha256":"88774330fc95a8797eedbc322a830ebcb251c0cd9e3b3714986e477f8ddddd78"},"downloads":-1,"filename":"warp_attention-0.1.4.tar.gz","has_sig":false,"md5_digest":"5af2a651729c4312d10eeec4218857b2","packagetype":"sdist","python_version":"source","requires_python":null,"size":45806256,"upload_time":"2023-10-04T16:36:19","upload_time_iso_8601":"2023-10-04T16:36:19.766966Z","url":"https://files.pythonhosted.org/packages/69/a7/c26fd5dd33df76a2902fc155dfbcf634f60bf80b688076ae0fe304174bee/warp_attention-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.5/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"a2fd431d0d9d63e95a0d286ecd98c69f3e8f8e53721878bb1f45fd86c57a671b","md5":"c6c9656bd86d61bde554b265f56f5829","sha256":"0d793f9a20f8cd39495f7dda92ea2c415d6bc94d86e90bdd32b15b35dc86d861"},"downloads":-1,"filename":"warp_attention-0.1.5.tar.gz","has_sig":false,"md5_digest":"c6c9656bd86d61bde554b265f56f5829","packagetype":"sdist","python_version":"source","requires_python":null,"size":83222399,"upload_time":"2023-10-04T22:00:34","upload_time_iso_8601":"2023-10-04T22:00:34.516013Z","url":"https://files.pythonhosted.org/packages/a2/fd/431d0d9d63e95a0d286ecd98c69f3e8f8e53721878bb1f45fd86c57a671b/warp_attention-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.6":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.6/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.6","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"6035d867b6c345428d633b17b139201f8918bc6619159fa56b26a46bee6581f2","md5":"5988239c3f3594f5c81b9428bc8463c2","sha256":"e4e06d567574dbc8a84e4a173cf424ee335c45d95c78436c7c5d25adfd8fe639"},"downloads":-1,"filename":"warp_attention-0.1.6.tar.gz","has_sig":false,"md5_digest":"5988239c3f3594f5c81b9428bc8463c2","packagetype":"sdist","python_version":"source","requires_python":null,"size":83222914,"upload_time":"2023-10-05T11:35:29","upload_time_iso_8601":"2023-10-05T11:35:29.050237Z","url":"https://files.pythonhosted.org/packages/60/35/d867b6c345428d633b17b139201f8918bc6619159fa56b26a46bee6581f2/warp_attention-0.1.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.7":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.7/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.7","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"2d36ad1766fce0e49a064f4036c50e11e81005a98ecc7c8d95edfe36268a9e8c","md5":"4c9e7af026dd685f59431f20ee43f21a","sha256":"eb18e7278b35cb342c47acc8668e7c613635da5834053802cd78ff55f657a90e"},"downloads":-1,"filename":"warp_attention-0.1.7.tar.gz","has_sig":false,"md5_digest":"4c9e7af026dd685f59431f20ee43f21a","packagetype":"sdist","python_version":"source","requires_python":null,"size":83222983,"upload_time":"2023-10-06T17:11:08","upload_time_iso_8601":"2023-10-06T17:11:08.086076Z","url":"https://files.pythonhosted.org/packages/2d/36/ad1766fce0e49a064f4036c50e11e81005a98ecc7c8d95edfe36268a9e8c/warp_attention-0.1.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.8":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.8/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.8","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"b21151512e590cb0844f7b8192003599885b593946361f38c27234a2992c505d","md5":"751a2fbed08bfc457f3076bb70b3f585","sha256":"9e2ea5340567b163ae8704b1ace3c17d50624920fc3821650ee4242a7ba53a50"},"downloads":-1,"filename":"warp_attention-0.1.8.tar.gz","has_sig":false,"md5_digest":"751a2fbed08bfc457f3076bb70b3f585","packagetype":"sdist","python_version":"source","requires_python":null,"size":83222997,"upload_time":"2023-10-07T13:14:06","upload_time_iso_8601":"2023-10-07T13:14:06.763556Z","url":"https://files.pythonhosted.org/packages/b2/11/51512e590cb0844f7b8192003599885b593946361f38c27234a2992c505d/warp_attention-0.1.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.9":{"info":{"author":"demoriarty","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"transformers,attention,scaled dot product attention,pytorch","license":"","maintainer":"","maintainer_email":"","name":"warp-attention","package_url":"https://pypi.org/project/warp-attention/","platform":null,"project_url":"https://pypi.org/project/warp-attention/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/warp-attention/0.1.9/","requires_dist":null,"requires_python":"","summary":"Warp attention: hardware efficient implementation of scaled dot product attention.","version":"0.1.9","yanked":false,"yanked_reason":null},"last_serial":20064754,"urls":[{"comment_text":"","digests":{"blake2b_256":"dd0c657c25b612250bfb2a032a138157b97819a543142e445c394d42f7049c13","md5":"bd2b0ac1265fe17c8ea31559eb4ec4a9","sha256":"5e661cfea1c5b962b3b5d268e76e564a1fe21ceab785a1a175ee493d2e93db03"},"downloads":-1,"filename":"warp_attention-0.1.9.tar.gz","has_sig":false,"md5_digest":"bd2b0ac1265fe17c8ea31559eb4ec4a9","packagetype":"sdist","python_version":"source","requires_python":null,"size":83223084,"upload_time":"2023-10-07T13:53:07","upload_time_iso_8601":"2023-10-07T13:53:07.551700Z","url":"https://files.pythonhosted.org/packages/dd/0c/657c25b612250bfb2a032a138157b97819a543142e445c394d42f7049c13/warp_attention-0.1.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}