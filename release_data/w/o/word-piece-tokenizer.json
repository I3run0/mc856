{"1.0.0":{"info":{"author":"","author_email":"Jing Hua <mail@dev.tjh.sg>","bugtrack_url":null,"classifiers":["License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication","Programming Language :: Python","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"word,piece,tokenizer,transformer,bert,uncased,nlp,natural,language,processing","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"word-piece-tokenizer","package_url":"https://pypi.org/project/word-piece-tokenizer/","platform":null,"project_url":"https://pypi.org/project/word-piece-tokenizer/","project_urls":{"Repository":"https://github.com/ztjhz/word-piece-tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/word-piece-tokenizer/1.0.0/","requires_dist":["transformers ; extra == 'test'"],"requires_python":">=3.9","summary":"A Lightweight Word Piece Tokenizer","version":"1.0.0","yanked":false,"yanked_reason":null},"last_serial":15223996,"urls":[{"comment_text":"","digests":{"blake2b_256":"b1730e2ddab45ad7a8af6ae463074cc2af0d68ad994b28eb6ac3605dc86b81d9","md5":"dd8fdb7869587f653ef0a26189f5fe7a","sha256":"d2de03358bcbe01acc9827f3e7256e0cc4debcdcc48dd170cd8f4039abdc926f"},"downloads":-1,"filename":"word_piece_tokenizer-1.0.0-py3-none-any.whl","has_sig":false,"md5_digest":"dd8fdb7869587f653ef0a26189f5fe7a","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":119620,"upload_time":"2022-09-24T08:33:54","upload_time_iso_8601":"2022-09-24T08:33:54.481654Z","url":"https://files.pythonhosted.org/packages/b1/73/0e2ddab45ad7a8af6ae463074cc2af0d68ad994b28eb6ac3605dc86b81d9/word_piece_tokenizer-1.0.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1a12ef710a371775acac63ac07aa75caeae9a3e6d985d284576b91c23a7f57c5","md5":"b35878715c2d9c1a7dd4e4d97f5b6d1a","sha256":"9eec64839995153eda13de124407b6f97056ff416f6ed99825621d01259175ae"},"downloads":-1,"filename":"word-piece-tokenizer-1.0.0.tar.gz","has_sig":false,"md5_digest":"b35878715c2d9c1a7dd4e4d97f5b6d1a","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":119825,"upload_time":"2022-09-24T08:33:57","upload_time_iso_8601":"2022-09-24T08:33:57.410489Z","url":"https://files.pythonhosted.org/packages/1a/12/ef710a371775acac63ac07aa75caeae9a3e6d985d284576b91c23a7f57c5/word-piece-tokenizer-1.0.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.1":{"info":{"author":"","author_email":"Jing Hua <mail@dev.tjh.sg>","bugtrack_url":null,"classifiers":["License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication","Programming Language :: Python","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"word,piece,tokenizer,transformer,bert,uncased,nlp,natural,language,processing","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"word-piece-tokenizer","package_url":"https://pypi.org/project/word-piece-tokenizer/","platform":null,"project_url":"https://pypi.org/project/word-piece-tokenizer/","project_urls":{"Repository":"https://github.com/ztjhz/word-piece-tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/word-piece-tokenizer/1.0.1/","requires_dist":["transformers ; extra == 'test'"],"requires_python":">=3.9","summary":"A Lightweight Word Piece Tokenizer","version":"1.0.1","yanked":false,"yanked_reason":null},"last_serial":15223996,"urls":[{"comment_text":"","digests":{"blake2b_256":"48f74bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8","md5":"a95321ddddc3bdeac359711ed1cdf72e","sha256":"655b8e918cebddfd1a13d11d77ca2c670939443e3dc9c4802d4472cc3eb6322c"},"downloads":-1,"filename":"word_piece_tokenizer-1.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"a95321ddddc3bdeac359711ed1cdf72e","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":119886,"upload_time":"2022-09-27T06:56:17","upload_time_iso_8601":"2022-09-27T06:56:17.487740Z","url":"https://files.pythonhosted.org/packages/48/f7/4bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8/word_piece_tokenizer-1.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c1e57a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b","md5":"e059e09ca96646a424f66a8868d67164","sha256":"46348d0a75fb4df364fa20c1dcf050b813debe7dae9e2971cadd05e3c8c0b4ba"},"downloads":-1,"filename":"word-piece-tokenizer-1.0.1.tar.gz","has_sig":false,"md5_digest":"e059e09ca96646a424f66a8868d67164","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":120267,"upload_time":"2022-09-27T06:56:20","upload_time_iso_8601":"2022-09-27T06:56:20.375605Z","url":"https://files.pythonhosted.org/packages/c1/e5/7a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b/word-piece-tokenizer-1.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}