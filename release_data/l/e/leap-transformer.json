{"0.1.0":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.0/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.0","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"6ed266b907b88917b94bcdb58f5b3cafb2e468773c1dc3f18e3b951befad5ca8","md5":"461e337d86be292badaf690b7ba816a2","sha256":"cf1b834718578fb21f4336fc19b00bad35d07e917c8a7f005352682c65594b5a"},"downloads":-1,"filename":"leap-transformer-0.1.0.tar.gz","has_sig":false,"md5_digest":"461e337d86be292badaf690b7ba816a2","packagetype":"sdist","python_version":"source","requires_python":null,"size":39365,"upload_time":"2022-08-29T08:07:38","upload_time_iso_8601":"2022-08-29T08:07:38.556802Z","url":"https://files.pythonhosted.org/packages/6e/d2/66b907b88917b94bcdb58f5b3cafb2e468773c1dc3f18e3b951befad5ca8/leap-transformer-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.1/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"278ecf062c182c52a18ed61929f943d4a3a346ca134f22084d2f8465bc1f124d","md5":"4bd520838f0eb8386a668a7b779e40b7","sha256":"978ed08bd8911bd9ccd86b053601cf6fe4101e5ac09dd135ec7c36bbae4d01e2"},"downloads":-1,"filename":"leap-transformer-0.1.1.tar.gz","has_sig":false,"md5_digest":"4bd520838f0eb8386a668a7b779e40b7","packagetype":"sdist","python_version":"source","requires_python":null,"size":40907,"upload_time":"2022-08-29T08:22:59","upload_time_iso_8601":"2022-08-29T08:22:59.742069Z","url":"https://files.pythonhosted.org/packages/27/8e/cf062c182c52a18ed61929f943d4a3a346ca134f22084d2f8465bc1f124d/leap-transformer-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.10":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.10/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.10","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"66f138e6826547cbe1570df603611c5cc0b830e45fb036f949aa298d606eeb59","md5":"abdbd3661b2516dfe24566e369e760bd","sha256":"852e7b56ec0aad9945fd055ecf3e7a2f3d930dab2e9ac03416aeed595ee8c7c7"},"downloads":-1,"filename":"leap-transformer-0.1.10.tar.gz","has_sig":false,"md5_digest":"abdbd3661b2516dfe24566e369e760bd","packagetype":"sdist","python_version":"source","requires_python":null,"size":43375,"upload_time":"2022-09-09T07:36:39","upload_time_iso_8601":"2022-09-09T07:36:39.220866Z","url":"https://files.pythonhosted.org/packages/66/f1/38e6826547cbe1570df603611c5cc0b830e45fb036f949aa298d606eeb59/leap-transformer-0.1.10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.2":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.2/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.2","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"b7678865ee4a1682fa6f0e7a7af526f793dfb1f5732e0d7dcf342622b51bdced","md5":"7c3bb82496b86f3a052d3be2746f5ec3","sha256":"85f46fffaf82671d30c0d8926a8faa36ac1b5938746150d3781339a1ea6fd691"},"downloads":-1,"filename":"leap-transformer-0.1.2.tar.gz","has_sig":false,"md5_digest":"7c3bb82496b86f3a052d3be2746f5ec3","packagetype":"sdist","python_version":"source","requires_python":null,"size":47648,"upload_time":"2022-09-03T19:56:56","upload_time_iso_8601":"2022-09-03T19:56:56.365388Z","url":"https://files.pythonhosted.org/packages/b7/67/8865ee4a1682fa6f0e7a7af526f793dfb1f5732e0d7dcf342622b51bdced/leap-transformer-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.3/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"e441fbcb9efed7b3163e064c95d3d056d443f570718fbf8ff7f8a77c38e368e5","md5":"08c89fc4290f494cf40f4ae5cc7253a9","sha256":"547fbd70f9ddae274873016c81bc66cd99756c5f61f7b31cb61c32dce7bfb7d7"},"downloads":-1,"filename":"leap-transformer-0.1.3.tar.gz","has_sig":false,"md5_digest":"08c89fc4290f494cf40f4ae5cc7253a9","packagetype":"sdist","python_version":"source","requires_python":null,"size":47615,"upload_time":"2022-09-03T20:04:47","upload_time_iso_8601":"2022-09-03T20:04:47.406629Z","url":"https://files.pythonhosted.org/packages/e4/41/fbcb9efed7b3163e064c95d3d056d443f570718fbf8ff7f8a77c38e368e5/leap-transformer-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.4/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"905f2469bf1770d9eb154696fb1d2ff648ff2c1ff95f4cf11bd88330ea19ed15","md5":"9205a3e8e014491e28dfc795c2912d1e","sha256":"03226319f3ae06c58b1660a968264c478a4dc425374d01e80a2f89cd125711b1"},"downloads":-1,"filename":"leap-transformer-0.1.4.tar.gz","has_sig":false,"md5_digest":"9205a3e8e014491e28dfc795c2912d1e","packagetype":"sdist","python_version":"source","requires_python":null,"size":47623,"upload_time":"2022-09-03T20:07:16","upload_time_iso_8601":"2022-09-03T20:07:16.044246Z","url":"https://files.pythonhosted.org/packages/90/5f/2469bf1770d9eb154696fb1d2ff648ff2c1ff95f4cf11bd88330ea19ed15/leap-transformer-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.5/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"4095a07aa6f10ef57ddd7dcc94dbe3e06213d6674d08d96e04211d90b2c53405","md5":"75e65776068682f364e79cadae4936cf","sha256":"e0948cbf57519f68e275744b365a24e908aebcac0bfa1b326e2b3f2641840722"},"downloads":-1,"filename":"leap-transformer-0.1.5.tar.gz","has_sig":false,"md5_digest":"75e65776068682f364e79cadae4936cf","packagetype":"sdist","python_version":"source","requires_python":null,"size":47631,"upload_time":"2022-09-03T20:14:58","upload_time_iso_8601":"2022-09-03T20:14:58.482832Z","url":"https://files.pythonhosted.org/packages/40/95/a07aa6f10ef57ddd7dcc94dbe3e06213d6674d08d96e04211d90b2c53405/leap-transformer-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.6":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.6/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.6","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"cd1da9f83536c2a573d2133aaf9b8d05049eb1861314b7121894ee0c4f2cce93","md5":"03e9e30ca61a93d7a1ce4433732c35dc","sha256":"0afa77506be47de8a781901e5d70dfb6ed8b13637f3938525d67c0afffe06660"},"downloads":-1,"filename":"leap-transformer-0.1.6.tar.gz","has_sig":false,"md5_digest":"03e9e30ca61a93d7a1ce4433732c35dc","packagetype":"sdist","python_version":"source","requires_python":null,"size":43616,"upload_time":"2022-09-04T08:19:33","upload_time_iso_8601":"2022-09-04T08:19:33.596227Z","url":"https://files.pythonhosted.org/packages/cd/1d/a9f83536c2a573d2133aaf9b8d05049eb1861314b7121894ee0c4f2cce93/leap-transformer-0.1.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.7":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.7/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.7","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"e911bd4ae6340fa06cce9c8fd999f60bf2f8c03b73ef5e925046eeaf086a56b7","md5":"f3336905ca69a36dae3c8a9e4cecbd21","sha256":"cf402dd375889f3e53c717aebf9a62b78c118f9373174240ae7cd751d440ab04"},"downloads":-1,"filename":"leap-transformer-0.1.7.tar.gz","has_sig":false,"md5_digest":"f3336905ca69a36dae3c8a9e4cecbd21","packagetype":"sdist","python_version":"source","requires_python":null,"size":45760,"upload_time":"2022-09-07T20:41:51","upload_time_iso_8601":"2022-09-07T20:41:51.586440Z","url":"https://files.pythonhosted.org/packages/e9/11/bd4ae6340fa06cce9c8fd999f60bf2f8c03b73ef5e925046eeaf086a56b7/leap-transformer-0.1.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.8":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.8/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.8","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"0f77bfac52d5de2fce88e388996582c7ae7cfd6e107501a8ff211716b9445a3e","md5":"8cd1d1b571702ff8ffaaf7ea2c8eca3e","sha256":"88099b76f0e8de63c9403853edae1698e75817b6c05b47b35cca34835e668634"},"downloads":-1,"filename":"leap-transformer-0.1.8.tar.gz","has_sig":false,"md5_digest":"8cd1d1b571702ff8ffaaf7ea2c8eca3e","packagetype":"sdist","python_version":"source","requires_python":null,"size":45751,"upload_time":"2022-09-08T03:59:22","upload_time_iso_8601":"2022-09-08T03:59:22.000210Z","url":"https://files.pythonhosted.org/packages/0f/77/bfac52d5de2fce88e388996582c7ae7cfd6e107501a8ff211716b9445a3e/leap-transformer-0.1.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.9":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/0.1.9/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"0.1.9","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"36a864df58b67c911cffae077d04b6b5a60462fbcee349d691ca046e45640c27","md5":"d567a68ac20256db2dfd558bb594708b","sha256":"b94cf4da348ad88a4c631bacc74e984d04e477eb56af8a3425baefe01e5da0b8"},"downloads":-1,"filename":"leap-transformer-0.1.9.tar.gz","has_sig":false,"md5_digest":"d567a68ac20256db2dfd558bb594708b","packagetype":"sdist","python_version":"source","requires_python":null,"size":45738,"upload_time":"2022-09-08T04:12:01","upload_time_iso_8601":"2022-09-08T04:12:01.782441Z","url":"https://files.pythonhosted.org/packages/36/a8/64df58b67c911cffae077d04b6b5a60462fbcee349d691ca046e45640c27/leap-transformer-0.1.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.0":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/1.0.0/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"1.0.0","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"95329e050bb617e43572d8fe11a6ad80fdb8f5803e534742a97f7042cf96069b","md5":"48b9d3647d3ad8a84c4649a9749c6287","sha256":"31c347abe846aaa5f889e183174b1cf45cddafe4bd442aa20e5066b9d967d953"},"downloads":-1,"filename":"leap-transformer-1.0.0.tar.gz","has_sig":false,"md5_digest":"48b9d3647d3ad8a84c4649a9749c6287","packagetype":"sdist","python_version":"source","requires_python":null,"size":23605,"upload_time":"2022-09-26T05:31:06","upload_time_iso_8601":"2022-09-26T05:31:06.386382Z","url":"https://files.pythonhosted.org/packages/95/32/9e050bb617e43572d8fe11a6ad80fdb8f5803e534742a97f7042cf96069b/leap-transformer-1.0.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.1":{"info":{"author":"Michael Hu","author_email":"prmhu@yahoo.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe","keywords":"linear transformer NLP deep learning pytorch","license":"CC0 1.0 Universal","maintainer":"","maintainer_email":"","name":"leap-transformer","package_url":"https://pypi.org/project/leap-transformer/","platform":null,"project_url":"https://pypi.org/project/leap-transformer/","project_urls":{"Homepage":"https://github.com/mtanghu/Additive-Attention-Is-Not-All-You-Need-Maybe"},"provides_extra":null,"release_url":"https://pypi.org/project/leap-transformer/1.0.1/","requires_dist":null,"requires_python":"","summary":"Linear Explainable Attention in Parallel (LEAP) for causal language modeling (also implements fastformer)","version":"1.0.1","yanked":false,"yanked_reason":null},"last_serial":15234861,"urls":[{"comment_text":"","digests":{"blake2b_256":"726340468fbe039089c06680648a85bc1b506402db504f9a15ec50205f2911fc","md5":"dbbc88608895340dbb3ebff49aab7645","sha256":"c0fac7b1ae0ff47c3a5c3caad2050dfa0f490b00ed5dde7b26c32bb9b33a6d53"},"downloads":-1,"filename":"leap-transformer-1.0.1.tar.gz","has_sig":false,"md5_digest":"dbbc88608895340dbb3ebff49aab7645","packagetype":"sdist","python_version":"source","requires_python":null,"size":23758,"upload_time":"2022-09-27T23:28:21","upload_time_iso_8601":"2022-09-27T23:28:21.793225Z","url":"https://files.pythonhosted.org/packages/72/63/40468fbe039089c06680648a85bc1b506402db504f9a15ec50205f2911fc/leap-transformer-1.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}