{"0.1.1":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1/","requires_dist":["flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'","nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"548e9fe6a0510f11218a7fe9c2f978f81e4abb3d7785aa03c9ad6e729652a2b0","md5":"4ce49ba858b05152d8fa4e5d22afceaa","sha256":"9e0c061531699f7646e44bfc16413e6402ce9adc898b534edcbbab33a3cee344"},"downloads":-1,"filename":"llmlingua-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"4ce49ba858b05152d8fa4e5d22afceaa","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":12898,"upload_time":"2023-10-08T12:28:38","upload_time_iso_8601":"2023-10-08T12:28:38.615680Z","url":"https://files.pythonhosted.org/packages/54/8e/9fe6a0510f11218a7fe9c2f978f81e4abb3d7785aa03c9ad6e729652a2b0/llmlingua-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"e2e53c00aef8cae4ac7d08e2ebfa849d40c829c5058a5c98435e38be7d5489c4","md5":"477e7e04a2e661198be769a0dacd9f0a","sha256":"2dc6a29c07acf1f5ed393259046823e1b69f67db8ff42f08c5d8ebb03aca496c"},"downloads":-1,"filename":"llmlingua-0.1.1.tar.gz","has_sig":false,"md5_digest":"477e7e04a2e661198be769a0dacd9f0a","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16072,"upload_time":"2023-10-08T12:28:40","upload_time_iso_8601":"2023-10-08T12:28:40.218863Z","url":"https://files.pythonhosted.org/packages/e2/e5/3c00aef8cae4ac7d08e2ebfa849d40c829c5058a5c98435e38be7d5489c4/llmlingua-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev0":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev0/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev0","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"77e384f69c8d5aeb7847ac9c56678a64dfe89093254d42992fa4db4b4df2a7b5","md5":"c51260ccdb0e3ba4edad74129c50457a","sha256":"8c2828fe1eca11a69245d7ef76642f0d993adc3e75809640c883d8f70b0f5339"},"downloads":-1,"filename":"llmlingua-0.1.1.dev0-py3-none-any.whl","has_sig":false,"md5_digest":"c51260ccdb0e3ba4edad74129c50457a","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":12972,"upload_time":"2023-10-08T15:00:47","upload_time_iso_8601":"2023-10-08T15:00:47.993894Z","url":"https://files.pythonhosted.org/packages/77/e3/84f69c8d5aeb7847ac9c56678a64dfe89093254d42992fa4db4b4df2a7b5/llmlingua-0.1.1.dev0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1e855156d352ff1b76d3c1af1c775d42f336466e8798040045d3715e84329ed7","md5":"594fecfd3157c0212a4e461b7b6342ea","sha256":"d3fc744a34414c7d4cf56c49fc429d317724bb703ae02609603b56d6d33b08a4"},"downloads":-1,"filename":"llmlingua-0.1.1.dev0.tar.gz","has_sig":false,"md5_digest":"594fecfd3157c0212a4e461b7b6342ea","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16091,"upload_time":"2023-10-08T15:00:49","upload_time_iso_8601":"2023-10-08T15:00:49.653116Z","url":"https://files.pythonhosted.org/packages/1e/85/5156d352ff1b76d3c1af1c775d42f336466e8798040045d3715e84329ed7/llmlingua-0.1.1.dev0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev1":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev1/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev1","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"fc0014975cef46b0dbf9786373cdfc253de4b2caa370c6a02fe4627fd460769f","md5":"4e2fb71748aec2f04d4181b75f4b5a09","sha256":"9875169529370cfea72d81ef08b131a7859b6244b4da52ae47bff6a00f48e893"},"downloads":-1,"filename":"llmlingua-0.1.1.dev1-py3-none-any.whl","has_sig":false,"md5_digest":"4e2fb71748aec2f04d4181b75f4b5a09","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":12991,"upload_time":"2023-10-08T15:54:47","upload_time_iso_8601":"2023-10-08T15:54:47.315613Z","url":"https://files.pythonhosted.org/packages/fc/00/14975cef46b0dbf9786373cdfc253de4b2caa370c6a02fe4627fd460769f/llmlingua-0.1.1.dev1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"8e9a4d785a36f631d307778a330a6fbd2211928620848d293dee1035f79a2ad1","md5":"25a18746946c586e9f795d7ed7adb475","sha256":"22bb317206ff1e10bca0538b7f00185b998a0e6f533f8d43a9e914d4970a676c"},"downloads":-1,"filename":"llmlingua-0.1.1.dev1.tar.gz","has_sig":false,"md5_digest":"25a18746946c586e9f795d7ed7adb475","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16106,"upload_time":"2023-10-08T15:54:48","upload_time_iso_8601":"2023-10-08T15:54:48.372947Z","url":"https://files.pythonhosted.org/packages/8e/9a/4d785a36f631d307778a330a6fbd2211928620848d293dee1035f79a2ad1/llmlingua-0.1.1.dev1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev2":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev2/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev2","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"1462c5f0746bae3039dc2597dcf44694d17ae7a7620e40e0b3a4ed2ae2c852d5","md5":"4c869beb34b1b8d3ddd8aa72fb014ae5","sha256":"b6085642e8a55788ce028263bc36efdda3a6090cbb799ab35b8bcbf32e553603"},"downloads":-1,"filename":"llmlingua-0.1.1.dev2-py3-none-any.whl","has_sig":false,"md5_digest":"4c869beb34b1b8d3ddd8aa72fb014ae5","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":12997,"upload_time":"2023-10-08T16:04:51","upload_time_iso_8601":"2023-10-08T16:04:51.537385Z","url":"https://files.pythonhosted.org/packages/14/62/c5f0746bae3039dc2597dcf44694d17ae7a7620e40e0b3a4ed2ae2c852d5/llmlingua-0.1.1.dev2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c6b006641b7fc2218ee833863f0caa2a9b2d42f7fcc934bcbf12f4b68f03ad67","md5":"92a13f210cf4d4f2916bd633bdddff74","sha256":"37f1d20b5a301e0c50d4112c994b5e4865f7f580f0c3d257f6e894b1921abe1e"},"downloads":-1,"filename":"llmlingua-0.1.1.dev2.tar.gz","has_sig":false,"md5_digest":"92a13f210cf4d4f2916bd633bdddff74","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16117,"upload_time":"2023-10-08T16:04:53","upload_time_iso_8601":"2023-10-08T16:04:53.092429Z","url":"https://files.pythonhosted.org/packages/c6/b0/06641b7fc2218ee833863f0caa2a9b2d42f7fcc934bcbf12f4b68f03ad67/llmlingua-0.1.1.dev2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev3":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev3/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev3","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"1a0462379fc8f6a1d7c3afd926ffeb323c5d26c8f42804f2ec2cf39b49094136","md5":"fa8103048c759ed7df14b64a07b93850","sha256":"47e8d91f603cb0c60c05424f1a3a5d8a5a800c99442a704fc6cc9b30cbe60ca0"},"downloads":-1,"filename":"llmlingua-0.1.1.dev3-py3-none-any.whl","has_sig":false,"md5_digest":"fa8103048c759ed7df14b64a07b93850","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":13008,"upload_time":"2023-10-09T04:46:23","upload_time_iso_8601":"2023-10-09T04:46:23.985295Z","url":"https://files.pythonhosted.org/packages/1a/04/62379fc8f6a1d7c3afd926ffeb323c5d26c8f42804f2ec2cf39b49094136/llmlingua-0.1.1.dev3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"209d05a84957ff4aa56102bc4cdc275201a633ba25e2f7e92df17557cba28a0a","md5":"cc8bef20a07df1eefc1deed673fa6ae0","sha256":"910144e83a0b457dc46c0a3dc3ba5733edfe2cdd7a9d7fd3354752596c8b1352"},"downloads":-1,"filename":"llmlingua-0.1.1.dev3.tar.gz","has_sig":false,"md5_digest":"cc8bef20a07df1eefc1deed673fa6ae0","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16122,"upload_time":"2023-10-09T04:46:25","upload_time_iso_8601":"2023-10-09T04:46:25.573046Z","url":"https://files.pythonhosted.org/packages/20/9d/05a84957ff4aa56102bc4cdc275201a633ba25e2f7e92df17557cba28a0a/llmlingua-0.1.1.dev3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev4":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev4/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev4","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"920b767e4b85727ad0faf282077099ec073ee2109add128358a384b556d81b0a","md5":"d0dbaa1c4553328f377d2072a2132ecd","sha256":"d5ba9ac4398a2d27b4a76f79bdbc443cb6f2933349516a9c106c484d47eb4fa7"},"downloads":-1,"filename":"llmlingua-0.1.1.dev4-py3-none-any.whl","has_sig":false,"md5_digest":"d0dbaa1c4553328f377d2072a2132ecd","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":13023,"upload_time":"2023-10-09T05:21:56","upload_time_iso_8601":"2023-10-09T05:21:56.401146Z","url":"https://files.pythonhosted.org/packages/92/0b/767e4b85727ad0faf282077099ec073ee2109add128358a384b556d81b0a/llmlingua-0.1.1.dev4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"290daaa2444a98484086f9743ffc9de3056c65549e707d92dfe993c1d1972979","md5":"2a9ffbe4d6cc9077c5c77a3963a3282d","sha256":"e49dbf2f9f5fb090abddb6afc3a4ef04edd10cec45b5f72dece1a043e08577f0"},"downloads":-1,"filename":"llmlingua-0.1.1.dev4.tar.gz","has_sig":false,"md5_digest":"2a9ffbe4d6cc9077c5c77a3963a3282d","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16143,"upload_time":"2023-10-09T05:21:57","upload_time_iso_8601":"2023-10-09T05:21:57.933483Z","url":"https://files.pythonhosted.org/packages/29/0d/aaa2444a98484086f9743ffc9de3056c65549e707d92dfe993c1d1972979/llmlingua-0.1.1.dev4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1.dev5":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.1.dev5/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.1.dev5","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"e6487e165b827e455bd37fb4ecccc3202680a51ee8b683c86325dd4f0d8ffc23","md5":"b8678b3afce78fa63420530ad631ce48","sha256":"f6457bea6c2134ba8e976fc6fa41b0aa9ae8a043f45bdca1a01aaf299c4281ba"},"downloads":-1,"filename":"llmlingua-0.1.1.dev5-py3-none-any.whl","has_sig":false,"md5_digest":"b8678b3afce78fa63420530ad631ce48","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":13360,"upload_time":"2023-10-09T13:30:39","upload_time_iso_8601":"2023-10-09T13:30:39.005096Z","url":"https://files.pythonhosted.org/packages/e6/48/7e165b827e455bd37fb4ecccc3202680a51ee8b683c86325dd4f0d8ffc23/llmlingua-0.1.1.dev5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"b26fd81b6acf84ccf91c1dd67662136d0c2e4e2f278a0d243c064cb9d791fec6","md5":"8ac93c5fa3effc0f3ef26be6df3c1cac","sha256":"dca02a2a3cb6b24ea1ae815198bf2083d4bc7bf6c07c341829e8e97f05504dd7"},"downloads":-1,"filename":"llmlingua-0.1.1.dev5.tar.gz","has_sig":false,"md5_digest":"8ac93c5fa3effc0f3ef26be6df3c1cac","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16504,"upload_time":"2023-10-09T13:30:40","upload_time_iso_8601":"2023-10-09T13:30:40.534044Z","url":"https://files.pythonhosted.org/packages/b2/6f/d81b6acf84ccf91c1dd67662136d0c2e4e2f278a0d243c064cb9d791fec6/llmlingua-0.1.1.dev5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.2":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.2/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.2","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"e47109a18cf48489a2122c5e4a549c8ce3cf082f17d6905ef88d232ab66810f0","md5":"820b88e002f154e5fda0e80304e162b9","sha256":"0d731183f70d62b313a5190599a5f61beafdd9ad3383a5a54c2fad528b4a78c9"},"downloads":-1,"filename":"llmlingua-0.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"820b88e002f154e5fda0e80304e162b9","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":13299,"upload_time":"2023-10-09T14:17:30","upload_time_iso_8601":"2023-10-09T14:17:30.309800Z","url":"https://files.pythonhosted.org/packages/e4/71/09a18cf48489a2122c5e4a549c8ce3cf082f17d6905ef88d232ab66810f0/llmlingua-0.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"095c65b43ab1e0022058a43e8409b7635b5ea0a8431c2da926ae10a2a836c8e4","md5":"7b357a7c2fb831f50a0a37a1a71b2958","sha256":"b8a535e6aba67bf3bf466b61b55e63003cfa5c472f4afcd18f818f037f99bd3b"},"downloads":-1,"filename":"llmlingua-0.1.2.tar.gz","has_sig":false,"md5_digest":"7b357a7c2fb831f50a0a37a1a71b2958","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":16537,"upload_time":"2023-10-09T14:17:31","upload_time_iso_8601":"2023-10-09T14:17:31.935901Z","url":"https://files.pythonhosted.org/packages/09/5c/65b43ab1e0022058a43e8409b7635b5ea0a8431c2da926ae10a2a836c8e4/llmlingua-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.3/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"edab49bb1745f3653209bbe22280e9c05c93ad64dc20105ecc587cfd8c1cd91a","md5":"1c3cf39c48970dd514af0c850134d7f4","sha256":"0da3bf9be84dc090ea2dd5a3758bdb684ae7d400b38a10082a1b5b9405c7efb9"},"downloads":-1,"filename":"llmlingua-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"1c3cf39c48970dd514af0c850134d7f4","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":15122,"upload_time":"2023-11-15T04:55:19","upload_time_iso_8601":"2023-11-15T04:55:19.225418Z","url":"https://files.pythonhosted.org/packages/ed/ab/49bb1745f3653209bbe22280e9c05c93ad64dc20105ecc587cfd8c1cd91a/llmlingua-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9daad15cd2e98f5d7c5dca41fe818df260601960c62dca00b3c971510274e7e0","md5":"ef35503c52b57ebd89fc1a58eb02b5d3","sha256":"2f6a774529e45a6d1c0c7887a5bc31743883cf1fa45f2bb0e8b780ff9a88e5d9"},"downloads":-1,"filename":"llmlingua-0.1.3.tar.gz","has_sig":false,"md5_digest":"ef35503c52b57ebd89fc1a58eb02b5d3","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":19206,"upload_time":"2023-11-15T04:55:20","upload_time_iso_8601":"2023-11-15T04:55:20.946724Z","url":"https://files.pythonhosted.org/packages/9d/aa/d15cd2e98f5d7c5dca41fe818df260601960c62dca00b3c971510274e7e0/llmlingua-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.5/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"8122984a3a9ec81888135571d04e5aa60a6f2743a18a9cda15ea20fc8202cc9b","md5":"c1a570626a6d3fa4b4c0c639f13bb581","sha256":"12ada2040ae489d662748221dbfc0d611cfbf8d4d2f3ad02150bd808ddd91892"},"downloads":-1,"filename":"llmlingua-0.1.5-py3-none-any.whl","has_sig":false,"md5_digest":"c1a570626a6d3fa4b4c0c639f13bb581","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":15837,"upload_time":"2023-12-21T08:56:18","upload_time_iso_8601":"2023-12-21T08:56:18.256710Z","url":"https://files.pythonhosted.org/packages/81/22/984a3a9ec81888135571d04e5aa60a6f2743a18a9cda15ea20fc8202cc9b/llmlingua-0.1.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"77d4c7873270686ad0c794e3d82498ac4a126bcc9113231d1329a1b3dc91256b","md5":"c847cb6b862ddd6777451eaa24de01c6","sha256":"8bf9a8a26e2965d916cf5b2fdd175da85438e631e38a6f7ad7b20e2abc41fc9d"},"downloads":-1,"filename":"llmlingua-0.1.5.tar.gz","has_sig":false,"md5_digest":"c847cb6b862ddd6777451eaa24de01c6","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":20234,"upload_time":"2023-12-21T08:56:20","upload_time_iso_8601":"2023-12-21T08:56:20.098883Z","url":"https://files.pythonhosted.org/packages/77/d4/c7873270686ad0c794e3d82498ac4a126bcc9113231d1329a1b3dc91256b/llmlingua-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.6":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.1.6/","requires_dist":["nltk","numpy","tiktoken","torch","transformers (>=4.26.0)","black (==21.4b0) ; extra == 'dev'","flake8 (>=3.8.3) ; extra == 'dev'","isort (>=5.5.4) ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","tiktoken ; extra == 'dev'","torch ; extra == 'dev'","transformers (>=4.26.0) ; extra == 'dev'","black (==21.4b0) ; extra == 'quality'","flake8 (>=3.8.3) ; extra == 'quality'","isort (>=5.5.4) ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.1.6","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"e9e6ebc844b6865007cac838e36af15c142d77f6a5ed5135eca759962c08d553","md5":"00cdbe59e7e16ed6cde90e30868c0120","sha256":"cd6e7c32223ce0662df8c12eb4976beab33212fb207b2241a66733e427701ce0"},"downloads":-1,"filename":"llmlingua-0.1.6-py3-none-any.whl","has_sig":false,"md5_digest":"00cdbe59e7e16ed6cde90e30868c0120","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":17845,"upload_time":"2024-02-19T08:43:14","upload_time_iso_8601":"2024-02-19T08:43:14.590453Z","url":"https://files.pythonhosted.org/packages/e9/e6/ebc844b6865007cac838e36af15c142d77f6a5ed5135eca759962c08d553/llmlingua-0.1.6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"040b15a4a6c403d1bf5c4fe84b27171472ca22e4e579a9d84d2eb25dc0881392","md5":"cba46f665d507eb21bc172690bb4fae8","sha256":"97eeb0531bc730b8f8a4d92ed369e3856d154e47edd4e5ab15ec984795b6d348"},"downloads":-1,"filename":"llmlingua-0.1.6.tar.gz","has_sig":false,"md5_digest":"cba46f665d507eb21bc172690bb4fae8","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":22301,"upload_time":"2024-02-19T08:43:16","upload_time_iso_8601":"2024-02-19T08:43:16.414808Z","url":"https://files.pythonhosted.org/packages/04/0b/15a4a6c403d1bf5c4fe84b27171472ca22e4e579a9d84d2eb25dc0881392/llmlingua-0.1.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.0":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression,LLMs,Inference Acceleration,Black-box LLMs,Efficient LLMs","license":"MIT License","maintainer":"","maintainer_email":"","name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.2.0/","requires_dist":["transformers >=4.26.0","accelerate","torch","tiktoken","nltk","numpy","transformers >=4.26.0 ; extra == 'dev'","accelerate ; extra == 'dev'","torch ; extra == 'dev'","tiktoken ; extra == 'dev'","nltk ; extra == 'dev'","numpy ; extra == 'dev'","black ==21.4b0 ; extra == 'dev'","flake8 >=3.8.3 ; extra == 'dev'","isort >=5.5.4 ; extra == 'dev'","pre-commit ; extra == 'dev'","pytest ; extra == 'dev'","pytest-xdist ; extra == 'dev'","black ==21.4b0 ; extra == 'quality'","flake8 >=3.8.3 ; extra == 'quality'","isort >=5.5.4 ; extra == 'quality'","pre-commit ; extra == 'quality'","pytest ; extra == 'quality'","pytest-xdist ; extra == 'quality'"],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.2.0","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"4499a5988d4d0494b99d1606881dda9852849964723768b36496d55dc4391f87","md5":"865d3d39d02b64e0d34f3f82fec0d0d8","sha256":"f3578ec6a6ecbc1a418913f7cdc07321ff70df0a17d4471679a230c6f79f9f15"},"downloads":-1,"filename":"llmlingua-0.2.0-py3-none-any.whl","has_sig":false,"md5_digest":"865d3d39d02b64e0d34f3f82fec0d0d8","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":22410,"upload_time":"2024-03-13T04:09:54","upload_time_iso_8601":"2024-03-13T04:09:54.076665Z","url":"https://files.pythonhosted.org/packages/44/99/a5988d4d0494b99d1606881dda9852849964723768b36496d55dc4391f87/llmlingua-0.2.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"02eeb0893409123199ce06bd71fc44e93c4d9f223bdb03c639ffe8da35348579","md5":"30727154a513871d0adfedea81926349","sha256":"8d27b72679ebafe7ae73821b6e6a746f6b4c894134eaf0348ac3d4a39476f41c"},"downloads":-1,"filename":"llmlingua-0.2.0.tar.gz","has_sig":false,"md5_digest":"30727154a513871d0adfedea81926349","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":33611,"upload_time":"2024-03-13T04:09:55","upload_time_iso_8601":"2024-03-13T04:09:55.433824Z","url":"https://files.pythonhosted.org/packages/02/ee/b0893409123199ce06bd71fc44e93c4d9f223bdb03c639ffe8da35348579/llmlingua-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.1":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression, LLMs, Inference Acceleration, Black-box LLMs, Efficient LLMs","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.2.1/","requires_dist":["transformers>=4.26.0","accelerate","torch","tiktoken","nltk","numpy","transformers>=4.26.0; extra == \"dev\"","accelerate; extra == \"dev\"","torch; extra == \"dev\"","tiktoken; extra == \"dev\"","nltk; extra == \"dev\"","numpy; extra == \"dev\"","black==21.4b0; extra == \"dev\"","flake8>=3.8.3; extra == \"dev\"","isort>=5.5.4; extra == \"dev\"","pre-commit; extra == \"dev\"","pytest; extra == \"dev\"","pytest-xdist; extra == \"dev\"","black==21.4b0; extra == \"quality\"","flake8>=3.8.3; extra == \"quality\"","isort>=5.5.4; extra == \"quality\"","pre-commit; extra == \"quality\"","pytest; extra == \"quality\"","pytest-xdist; extra == \"quality\""],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.2.1","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"723ce6399ce6880cab61f2a8ad56b22afec9cfa645227dde05be3350ee80e039","md5":"bac088da78e67f16a6a7d61064378ab0","sha256":"e032200acce5de362cf1ef18f70d79c79c45e00e4bf28150c4dbd714d5f8a988"},"downloads":-1,"filename":"llmlingua-0.2.1-py3-none-any.whl","has_sig":false,"md5_digest":"bac088da78e67f16a6a7d61064378ab0","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":29201,"upload_time":"2024-03-20T03:43:55","upload_time_iso_8601":"2024-03-20T03:43:55.652888Z","url":"https://files.pythonhosted.org/packages/72/3c/e6399ce6880cab61f2a8ad56b22afec9cfa645227dde05be3350ee80e039/llmlingua-0.2.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"38ec841176df21f31e852493a727e430768c9f8443edc57b1e80cfdca6ddb181","md5":"6f08537da5cda026f27a46fce965750b","sha256":"a892093a945ea8ac5f06e84481ef63fa1eeabdbcddb268ad9a7d2524bdc84718"},"downloads":-1,"filename":"llmlingua-0.2.1.tar.gz","has_sig":false,"md5_digest":"6f08537da5cda026f27a46fce965750b","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":58402,"upload_time":"2024-03-20T03:43:57","upload_time_iso_8601":"2024-03-20T03:43:57.380817Z","url":"https://files.pythonhosted.org/packages/38/ec/841176df21f31e852493a727e430768c9f8443edc57b1e80cfdca6ddb181/llmlingua-0.2.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.2":{"info":{"author":"The LLMLingua team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/LLMLingua","keywords":"Prompt Compression, LLMs, Inference Acceleration, Black-box LLMs, Efficient LLMs","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"llmlingua","package_url":"https://pypi.org/project/llmlingua/","platform":null,"project_url":"https://pypi.org/project/llmlingua/","project_urls":{"Homepage":"https://github.com/microsoft/LLMLingua"},"provides_extra":null,"release_url":"https://pypi.org/project/llmlingua/0.2.2/","requires_dist":["transformers>=4.26.0","accelerate","torch","tiktoken","nltk","numpy","transformers>=4.26.0; extra == \"dev\"","accelerate; extra == \"dev\"","torch; extra == \"dev\"","tiktoken; extra == \"dev\"","nltk; extra == \"dev\"","numpy; extra == \"dev\"","black==21.4b0; extra == \"dev\"","flake8>=3.8.3; extra == \"dev\"","isort>=5.5.4; extra == \"dev\"","pre-commit; extra == \"dev\"","pytest; extra == \"dev\"","pytest-xdist; extra == \"dev\"","black==21.4b0; extra == \"quality\"","flake8>=3.8.3; extra == \"quality\"","isort>=5.5.4; extra == \"quality\"","pre-commit; extra == \"quality\"","pytest; extra == \"quality\"","pytest-xdist; extra == \"quality\""],"requires_python":">=3.8.0","summary":"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.","version":"0.2.2","yanked":false,"yanked_reason":null},"last_serial":22680190,"urls":[{"comment_text":"","digests":{"blake2b_256":"6e3e221fe46a3338f2babdb2082ee42df88fcaa8ea0e639e832cbb1b93c5923a","md5":"6c0efb180fc9d7f7cab51d073d419600","sha256":"da55137efe0db78063b3395396efe8a0dcfe4ae5a09aea0d503c34b7bf1d800c"},"downloads":-1,"filename":"llmlingua-0.2.2-py3-none-any.whl","has_sig":false,"md5_digest":"6c0efb180fc9d7f7cab51d073d419600","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8.0","size":30536,"upload_time":"2024-04-09T08:21:55","upload_time_iso_8601":"2024-04-09T08:21:55.428185Z","url":"https://files.pythonhosted.org/packages/6e/3e/221fe46a3338f2babdb2082ee42df88fcaa8ea0e639e832cbb1b93c5923a/llmlingua-0.2.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"fd027ed8ad9cec4b2f0a8a45c5b50a2435b559d2c764b5865e896d1c511080b1","md5":"131f7f6f7e1cd495dcf8cb7be2d057ee","sha256":"1a0caedd8d5a65512a85dadb6bfda6f5b3c4b45e5cb9e7b1c6009573f9058572"},"downloads":-1,"filename":"llmlingua-0.2.2.tar.gz","has_sig":false,"md5_digest":"131f7f6f7e1cd495dcf8cb7be2d057ee","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":59753,"upload_time":"2024-04-09T08:21:56","upload_time_iso_8601":"2024-04-09T08:21:56.880658Z","url":"https://files.pythonhosted.org/packages/fd/02/7ed8ad9cec4b2f0a8a45c5b50a2435b559d2c764b5865e896d1c511080b1/llmlingua-0.2.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}