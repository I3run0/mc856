{"0.0.1":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.1/","requires_dist":["colt5-attention","einops (>=0.6.1)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"257a7b9de75ec3aba0cdfbc4e9fb043f80c7a5d76720a56006a8bdf96225e99a","md5":"1c7385df2840f73fd8056ff5de46509c","sha256":"61b58edbec0d9bb3fbf0d79273402bf3be0aa4bff4f6147047eb3910197e4da6"},"downloads":-1,"filename":"mixture_of_attention-0.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"1c7385df2840f73fd8056ff5de46509c","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":5621,"upload_time":"2023-05-14T17:27:47","upload_time_iso_8601":"2023-05-14T17:27:47.440563Z","url":"https://files.pythonhosted.org/packages/25/7a/7b9de75ec3aba0cdfbc4e9fb043f80c7a5d76720a56006a8bdf96225e99a/mixture_of_attention-0.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"d898748cb8b84893b745eb0c1a292abd0a1c3c6a2a27f87b43001cb47760ac1b","md5":"b185e4337ab72281782fef6f29359f44","sha256":"b588d70f34dd092fd888208c5c8a93edcb284f6937c3ff5401434ca7fe6993e6"},"downloads":-1,"filename":"mixture-of-attention-0.0.1.tar.gz","has_sig":false,"md5_digest":"b185e4337ab72281782fef6f29359f44","packagetype":"sdist","python_version":"source","requires_python":null,"size":5429,"upload_time":"2023-05-14T17:27:49","upload_time_iso_8601":"2023-05-14T17:27:49.169174Z","url":"https://files.pythonhosted.org/packages/d8/98/748cb8b84893b745eb0c1a292abd0a1c3c6a2a27f87b43001cb47760ac1b/mixture-of-attention-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.10":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.10/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.10","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"d9f5f428527c9babe408edc4c47baf6647a62dd2809503fe1fc6d10225bfaa5f","md5":"22c44d253b2db2ff2137439142926e7a","sha256":"d96198451353d588a12ecc3a5940a10d31b5d6049f70fe07d804095dc5a55035"},"downloads":-1,"filename":"mixture_of_attention-0.0.10-py3-none-any.whl","has_sig":false,"md5_digest":"22c44d253b2db2ff2137439142926e7a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6996,"upload_time":"2023-05-17T00:48:24","upload_time_iso_8601":"2023-05-17T00:48:24.388973Z","url":"https://files.pythonhosted.org/packages/d9/f5/f428527c9babe408edc4c47baf6647a62dd2809503fe1fc6d10225bfaa5f/mixture_of_attention-0.0.10-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5f8bbff7f05ff1ad488666b541837607ee7e3d8ba722953e4c799d55e05cb376","md5":"904559879108a070e9f92af17d2114e0","sha256":"1091e8d175344b7b4226b3df8999d7d3628d3d36740c931d893a11e41523c509"},"downloads":-1,"filename":"mixture-of-attention-0.0.10.tar.gz","has_sig":false,"md5_digest":"904559879108a070e9f92af17d2114e0","packagetype":"sdist","python_version":"source","requires_python":null,"size":7366,"upload_time":"2023-05-17T00:48:25","upload_time_iso_8601":"2023-05-17T00:48:25.993729Z","url":"https://files.pythonhosted.org/packages/5f/8b/bff7f05ff1ad488666b541837607ee7e3d8ba722953e4c799d55e05cb376/mixture-of-attention-0.0.10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.11":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.11/","requires_dist":["colt5-attention (>=0.9.0)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.11","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"9ad77450d800f0602f4f5084dc33a10d71d717056b2914b80bbef06a2db613de","md5":"a3db9733ffa4b5c8f52106f65f4310e2","sha256":"91f2da0907c430310a7d224917c5aa52f0db0637caa85a05bae44bec5158292a"},"downloads":-1,"filename":"mixture_of_attention-0.0.11-py3-none-any.whl","has_sig":false,"md5_digest":"a3db9733ffa4b5c8f52106f65f4310e2","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":7000,"upload_time":"2023-05-22T16:59:06","upload_time_iso_8601":"2023-05-22T16:59:06.978302Z","url":"https://files.pythonhosted.org/packages/9a/d7/7450d800f0602f4f5084dc33a10d71d717056b2914b80bbef06a2db613de/mixture_of_attention-0.0.11-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"d9a7af88eeec93f46659639a7c1fa764f58a0a34cd81a336c5bd9bab930d36ad","md5":"80af7d6049de55a2952e0e22a280d7e8","sha256":"115e74bcb7a2cede37810df8bc1a43b0585eefd470250d7abe48e7d8e141c58b"},"downloads":-1,"filename":"mixture-of-attention-0.0.11.tar.gz","has_sig":false,"md5_digest":"80af7d6049de55a2952e0e22a280d7e8","packagetype":"sdist","python_version":"source","requires_python":null,"size":7505,"upload_time":"2023-05-22T16:59:08","upload_time_iso_8601":"2023-05-22T16:59:08.555692Z","url":"https://files.pythonhosted.org/packages/d9/a7/af88eeec93f46659639a7c1fa764f58a0a34cd81a336c5bd9bab930d36ad/mixture-of-attention-0.0.11.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.12":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.12/","requires_dist":["colt5-attention (>=0.10.2)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.12","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"54f72a033bb72e6c482304d8bce19b03b8f120cb5e495452d54a8f5e3951efc4","md5":"5a9c0a8399b21b344034e6eda7f98ac3","sha256":"54b9f3468506fc3abe9df8456561c835ddf369746b4e0af3748fb5ce1143e353"},"downloads":-1,"filename":"mixture_of_attention-0.0.12-py3-none-any.whl","has_sig":false,"md5_digest":"5a9c0a8399b21b344034e6eda7f98ac3","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":7036,"upload_time":"2023-05-24T18:09:47","upload_time_iso_8601":"2023-05-24T18:09:47.863797Z","url":"https://files.pythonhosted.org/packages/54/f7/2a033bb72e6c482304d8bce19b03b8f120cb5e495452d54a8f5e3951efc4/mixture_of_attention-0.0.12-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ca434950d9d875f018741935cb35c6d940e7731429547678748d2fbf1ad0c4f1","md5":"eb1a196620569ff54cd93e5c37972540","sha256":"eb334e24ea79e7e9aa76740b177dd6a3803dfb2c753b2587ccf04f1e02860612"},"downloads":-1,"filename":"mixture-of-attention-0.0.12.tar.gz","has_sig":false,"md5_digest":"eb1a196620569ff54cd93e5c37972540","packagetype":"sdist","python_version":"source","requires_python":null,"size":7547,"upload_time":"2023-05-24T18:09:49","upload_time_iso_8601":"2023-05-24T18:09:49.416249Z","url":"https://files.pythonhosted.org/packages/ca/43/4950d9d875f018741935cb35c6d940e7731429547678748d2fbf1ad0c4f1/mixture-of-attention-0.0.12.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.14":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.14/","requires_dist":["colt5-attention (>=0.10.2)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.14","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"ae2c431f91f30265968f8d92ba2ec228cc1bc5027e28e67b5cdbfc8d7a1bf246","md5":"12526198c5ca6e86f3ab5951b0853bff","sha256":"e2493cf78a1ab2c37303f3561bb8f90e42b2267d71e30c95150716b18f1eadb0"},"downloads":-1,"filename":"mixture_of_attention-0.0.14-py3-none-any.whl","has_sig":false,"md5_digest":"12526198c5ca6e86f3ab5951b0853bff","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":7870,"upload_time":"2023-05-24T21:10:10","upload_time_iso_8601":"2023-05-24T21:10:10.952134Z","url":"https://files.pythonhosted.org/packages/ae/2c/431f91f30265968f8d92ba2ec228cc1bc5027e28e67b5cdbfc8d7a1bf246/mixture_of_attention-0.0.14-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"e1a1784c0bb771692887b6f65d27d75e0d1baae6ca3e32262af610db7f3ddd67","md5":"7068e451d0d68b142934fddb3d75d6c5","sha256":"75b45181a0bbf5d42f9ee123e3b5d16005e2727ea952322155ab60d1bc30a7cd"},"downloads":-1,"filename":"mixture-of-attention-0.0.14.tar.gz","has_sig":false,"md5_digest":"7068e451d0d68b142934fddb3d75d6c5","packagetype":"sdist","python_version":"source","requires_python":null,"size":8950,"upload_time":"2023-05-24T21:10:12","upload_time_iso_8601":"2023-05-24T21:10:12.392172Z","url":"https://files.pythonhosted.org/packages/e1/a1/784c0bb771692887b6f65d27d75e0d1baae6ca3e32262af610db7f3ddd67/mixture-of-attention-0.0.14.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.15":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.15/","requires_dist":["colt5-attention (>=0.10.3)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.15","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"bbf633c1a46cecb9af169e4167c530f5baf3b231b8e7e4075e9c5e8e92f31ddc","md5":"bd1b0906debc7d81436f2b36c6e27158","sha256":"9d18be8fd9b25839c7800031fd02f509543c21d26d33c3c612b94750e6f8203e"},"downloads":-1,"filename":"mixture_of_attention-0.0.15-py3-none-any.whl","has_sig":false,"md5_digest":"bd1b0906debc7d81436f2b36c6e27158","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":7870,"upload_time":"2023-05-24T23:05:29","upload_time_iso_8601":"2023-05-24T23:05:29.982892Z","url":"https://files.pythonhosted.org/packages/bb/f6/33c1a46cecb9af169e4167c530f5baf3b231b8e7e4075e9c5e8e92f31ddc/mixture_of_attention-0.0.15-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5e3734482cba32014089d717b278795b66f00c001b232b2fa39825f8e0e4f9b9","md5":"66252444aeb330a2a4f10fe77faefd65","sha256":"89ac0b0a8afadc4a6377727551c9b26d1786bddf628c8d38aa0c240490bfd086"},"downloads":-1,"filename":"mixture-of-attention-0.0.15.tar.gz","has_sig":false,"md5_digest":"66252444aeb330a2a4f10fe77faefd65","packagetype":"sdist","python_version":"source","requires_python":null,"size":8942,"upload_time":"2023-05-24T23:05:31","upload_time_iso_8601":"2023-05-24T23:05:31.705064Z","url":"https://files.pythonhosted.org/packages/5e/37/34482cba32014089d717b278795b66f00c001b232b2fa39825f8e0e4f9b9/mixture-of-attention-0.0.15.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.16":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.16/","requires_dist":["colt5-attention (>=0.10.3)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.16","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"d3004c64da234dcea97c2716c5bca50bf7162c58b4bd9268ddbda3753d737dfc","md5":"b9637efeb09abdb1e8d110de076e8bfb","sha256":"f5e3fc47a2bf9a3c6c27245567780bf9a2c9293e051c81c144c6c267fd09f6b3"},"downloads":-1,"filename":"mixture_of_attention-0.0.16-py3-none-any.whl","has_sig":false,"md5_digest":"b9637efeb09abdb1e8d110de076e8bfb","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9758,"upload_time":"2023-05-24T23:06:30","upload_time_iso_8601":"2023-05-24T23:06:30.424132Z","url":"https://files.pythonhosted.org/packages/d3/00/4c64da234dcea97c2716c5bca50bf7162c58b4bd9268ddbda3753d737dfc/mixture_of_attention-0.0.16-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5962bc8d1c93e914cde99610fb91984883baa336a99002a3b6f9d981ebd6426d","md5":"6ea66f30a00f6a0056ea685e4912de70","sha256":"cfd4ca9db78d0c501c80b4f4aa56c3da9edeeb8c7da63052fcadb75b5abb1db2"},"downloads":-1,"filename":"mixture-of-attention-0.0.16.tar.gz","has_sig":false,"md5_digest":"6ea66f30a00f6a0056ea685e4912de70","packagetype":"sdist","python_version":"source","requires_python":null,"size":9990,"upload_time":"2023-05-24T23:06:32","upload_time_iso_8601":"2023-05-24T23:06:32.024128Z","url":"https://files.pythonhosted.org/packages/59/62/bc8d1c93e914cde99610fb91984883baa336a99002a3b6f9d981ebd6426d/mixture-of-attention-0.0.16.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.17":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.17/","requires_dist":["colt5-attention (>=0.10.4)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.17","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"7501b66ff6151b6732194fc895b1b1da213ac13b5eb92e9e9732eecaf0b75a04","md5":"88e6f1e1e5e427bbdf72561e07a3d949","sha256":"63780d20994a32672c0c35e65f8addabec0c0410fecafc4ad1f895a99b8b4cee"},"downloads":-1,"filename":"mixture_of_attention-0.0.17-py3-none-any.whl","has_sig":false,"md5_digest":"88e6f1e1e5e427bbdf72561e07a3d949","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9780,"upload_time":"2023-05-24T23:30:41","upload_time_iso_8601":"2023-05-24T23:30:41.964116Z","url":"https://files.pythonhosted.org/packages/75/01/b66ff6151b6732194fc895b1b1da213ac13b5eb92e9e9732eecaf0b75a04/mixture_of_attention-0.0.17-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"722123d99ec1279a7a1cd675d63eca0204d5aa908bca138ab652097105a33f24","md5":"219212b533926a1847d8d79c1db42345","sha256":"2a3b2cfd6337590b24dc4e92cbe1b4391004e96fb6d8da365385a8361c1eac99"},"downloads":-1,"filename":"mixture-of-attention-0.0.17.tar.gz","has_sig":false,"md5_digest":"219212b533926a1847d8d79c1db42345","packagetype":"sdist","python_version":"source","requires_python":null,"size":9996,"upload_time":"2023-05-24T23:30:43","upload_time_iso_8601":"2023-05-24T23:30:43.323253Z","url":"https://files.pythonhosted.org/packages/72/21/23d99ec1279a7a1cd675d63eca0204d5aa908bca138ab652097105a33f24/mixture-of-attention-0.0.17.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.18":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.18/","requires_dist":["colt5-attention (>=0.10.5)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.18","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"85538885d3ad037dcb0fb1574402b8c9d1a4db8efb4f27d643ed30ed50fc869f","md5":"1f08e93a58e6e93f6bacef6a28777206","sha256":"4857c56c1fe186a19b4350619aeb20b9b01adfad08cf09c412d864e610f96269"},"downloads":-1,"filename":"mixture_of_attention-0.0.18-py3-none-any.whl","has_sig":false,"md5_digest":"1f08e93a58e6e93f6bacef6a28777206","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9801,"upload_time":"2023-05-25T00:09:09","upload_time_iso_8601":"2023-05-25T00:09:09.545315Z","url":"https://files.pythonhosted.org/packages/85/53/8885d3ad037dcb0fb1574402b8c9d1a4db8efb4f27d643ed30ed50fc869f/mixture_of_attention-0.0.18-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"cc403e86c83c2a2e9b6476c15d35664be37bb7048c63b5cefebb9d2745df8b80","md5":"450e0b600b3f705be6fdd78424ddab01","sha256":"d3d42e79608e60326a743d61bc017584b5b578cb1d325c5afb4eb8a9f8b036a2"},"downloads":-1,"filename":"mixture-of-attention-0.0.18.tar.gz","has_sig":false,"md5_digest":"450e0b600b3f705be6fdd78424ddab01","packagetype":"sdist","python_version":"source","requires_python":null,"size":9996,"upload_time":"2023-05-25T00:09:10","upload_time_iso_8601":"2023-05-25T00:09:10.543078Z","url":"https://files.pythonhosted.org/packages/cc/40/3e86c83c2a2e9b6476c15d35664be37bb7048c63b5cefebb9d2745df8b80/mixture-of-attention-0.0.18.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.19":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.19/","requires_dist":["colt5-attention (>=0.10.6)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.19","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"3df22b0c866b9cbb8b51178fa86dc923819c40ef31c59d4874207a7a3257a6da","md5":"198906a9cf70f64edb1eda313dc4838c","sha256":"dcfaae98e0bfc80abedfe46ccf865b11a41fffef6ee6d30848169a794879dd2a"},"downloads":-1,"filename":"mixture_of_attention-0.0.19-py3-none-any.whl","has_sig":false,"md5_digest":"198906a9cf70f64edb1eda313dc4838c","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9802,"upload_time":"2023-05-25T00:27:51","upload_time_iso_8601":"2023-05-25T00:27:51.012624Z","url":"https://files.pythonhosted.org/packages/3d/f2/2b0c866b9cbb8b51178fa86dc923819c40ef31c59d4874207a7a3257a6da/mixture_of_attention-0.0.19-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"e1b9889008f975c5b0d7121f2714474028b3dc5f99a73df41ba8e402ea87a29f","md5":"0eb3b492ab0f055f429a4f57b3e1a1f2","sha256":"270b3e5cdae76c106052ff9a0251b2479c37139b475555f2b5c523778818240c"},"downloads":-1,"filename":"mixture-of-attention-0.0.19.tar.gz","has_sig":false,"md5_digest":"0eb3b492ab0f055f429a4f57b3e1a1f2","packagetype":"sdist","python_version":"source","requires_python":null,"size":9989,"upload_time":"2023-05-25T00:27:53","upload_time_iso_8601":"2023-05-25T00:27:53.204618Z","url":"https://files.pythonhosted.org/packages/e1/b9/889008f975c5b0d7121f2714474028b3dc5f99a73df41ba8e402ea87a29f/mixture-of-attention-0.0.19.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.2":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.2/","requires_dist":["colt5-attention (>=0.8.0)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.2","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"7ea7b5a700341c5f532a3b84556977cfc72c319a4dec877e761bc5315906c87b","md5":"73b59390b26cebed0d45458320d46f91","sha256":"cda8fb90f1185bc0ae7964f37faf6e83dff101391d5741dbc92bfe47a97ef1d2"},"downloads":-1,"filename":"mixture_of_attention-0.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"73b59390b26cebed0d45458320d46f91","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6338,"upload_time":"2023-05-15T18:51:47","upload_time_iso_8601":"2023-05-15T18:51:47.661766Z","url":"https://files.pythonhosted.org/packages/7e/a7/b5a700341c5f532a3b84556977cfc72c319a4dec877e761bc5315906c87b/mixture_of_attention-0.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"82964e775704e483d91c5df04071e14f761ba48ce41f3792ed3a525455619154","md5":"a9f78d0fe7e9c8f608877fdd050af39a","sha256":"5f2ff010086bba5142d4876632c615c38e093a1951eeb604260989e95785f924"},"downloads":-1,"filename":"mixture-of-attention-0.0.2.tar.gz","has_sig":false,"md5_digest":"a9f78d0fe7e9c8f608877fdd050af39a","packagetype":"sdist","python_version":"source","requires_python":null,"size":6607,"upload_time":"2023-05-15T18:51:48","upload_time_iso_8601":"2023-05-15T18:51:48.943193Z","url":"https://files.pythonhosted.org/packages/82/96/4e775704e483d91c5df04071e14f761ba48ce41f3792ed3a525455619154/mixture-of-attention-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.20":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.20/","requires_dist":["colt5-attention (>=0.10.6)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.20","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"20f3a96b137f2c09ffb83aeb3cda9a017f3da4411afe3aa343b1bc5b6b8d3ad7","md5":"7db01df7181cfb4518b37788860ad91d","sha256":"88698cf03ec52a64b99f61e5705eb819dd6e7d31b24b90ae0f118065d858d7b9"},"downloads":-1,"filename":"mixture_of_attention-0.0.20-py3-none-any.whl","has_sig":false,"md5_digest":"7db01df7181cfb4518b37788860ad91d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9866,"upload_time":"2023-05-25T01:52:40","upload_time_iso_8601":"2023-05-25T01:52:40.891296Z","url":"https://files.pythonhosted.org/packages/20/f3/a96b137f2c09ffb83aeb3cda9a017f3da4411afe3aa343b1bc5b6b8d3ad7/mixture_of_attention-0.0.20-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f01198f0f4ac3129bb35ef4d570a04c296522c8a9e4c21b1863fd46a6d436dfe","md5":"2e436eed0c3d1f67b91a4459e1b10541","sha256":"d7c38b90537da892cdbc8dbd47029cc67d563958e5d76583a5bab99c8c97d350"},"downloads":-1,"filename":"mixture-of-attention-0.0.20.tar.gz","has_sig":false,"md5_digest":"2e436eed0c3d1f67b91a4459e1b10541","packagetype":"sdist","python_version":"source","requires_python":null,"size":10050,"upload_time":"2023-05-25T01:52:42","upload_time_iso_8601":"2023-05-25T01:52:42.394569Z","url":"https://files.pythonhosted.org/packages/f0/11/98f0f4ac3129bb35ef4d570a04c296522c8a9e4c21b1863fd46a6d436dfe/mixture-of-attention-0.0.20.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.21":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.21/","requires_dist":["colt5-attention (>=0.10.6)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.21","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"ee40c6c8159b621e1a1c3cc31aac029a246f79116d8da0e9090c9d2262966827","md5":"e257c483ccc364db2a2b2a884091ecdc","sha256":"4ec399d54736b3d8ffcc28e3e33ecb4430a8b3db2a375a600aa45bc9f6d91e32"},"downloads":-1,"filename":"mixture_of_attention-0.0.21-py3-none-any.whl","has_sig":false,"md5_digest":"e257c483ccc364db2a2b2a884091ecdc","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":9891,"upload_time":"2023-05-25T02:02:35","upload_time_iso_8601":"2023-05-25T02:02:35.045969Z","url":"https://files.pythonhosted.org/packages/ee/40/c6c8159b621e1a1c3cc31aac029a246f79116d8da0e9090c9d2262966827/mixture_of_attention-0.0.21-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"d5f9e414eb09d55055a94c8ab0d54a0b676320e537332f4d1000ac3fa4046b8a","md5":"9a9be42eab2e82f9fb819d4045998ed8","sha256":"22c450e9c82e6876c02af55421e7acd760a27c62856127b001893e818d5f6e60"},"downloads":-1,"filename":"mixture-of-attention-0.0.21.tar.gz","has_sig":false,"md5_digest":"9a9be42eab2e82f9fb819d4045998ed8","packagetype":"sdist","python_version":"source","requires_python":null,"size":10074,"upload_time":"2023-05-25T02:02:36","upload_time_iso_8601":"2023-05-25T02:02:36.724539Z","url":"https://files.pythonhosted.org/packages/d5/f9/e414eb09d55055a94c8ab0d54a0b676320e537332f4d1000ac3fa4046b8a/mixture-of-attention-0.0.21.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.22":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.22/","requires_dist":["colt5-attention (>=0.10.6)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.22","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"9b883fadec01b82676ec2167a00510f47dccf94888642a2ba52b4d3c1eb5a0f3","md5":"a577adf49dcfc3647305c786a70ad5ae","sha256":"be881c5a94ce2f79eb30c380fd4013d8e13b071cbb07c547aa9c055ee5fe4cf7"},"downloads":-1,"filename":"mixture_of_attention-0.0.22-py3-none-any.whl","has_sig":false,"md5_digest":"a577adf49dcfc3647305c786a70ad5ae","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":10026,"upload_time":"2023-05-25T03:44:41","upload_time_iso_8601":"2023-05-25T03:44:41.262855Z","url":"https://files.pythonhosted.org/packages/9b/88/3fadec01b82676ec2167a00510f47dccf94888642a2ba52b4d3c1eb5a0f3/mixture_of_attention-0.0.22-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"8d4349e955de9d147ffa3b4f8923076bead6fb624d1bb90f33187740b766eeda","md5":"8c9450f85674180cd03a55ff36acfa6e","sha256":"840a2ea4ff9aedf80aaa551c4255937c05200e9ed89e135bd029535caaab0a50"},"downloads":-1,"filename":"mixture-of-attention-0.0.22.tar.gz","has_sig":false,"md5_digest":"8c9450f85674180cd03a55ff36acfa6e","packagetype":"sdist","python_version":"source","requires_python":null,"size":10212,"upload_time":"2023-05-25T03:44:42","upload_time_iso_8601":"2023-05-25T03:44:42.912997Z","url":"https://files.pythonhosted.org/packages/8d/43/49e955de9d147ffa3b4f8923076bead6fb624d1bb90f33187740b766eeda/mixture-of-attention-0.0.22.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.23":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.23/","requires_dist":["colt5-attention (>=0.10.6)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.23","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"70ef5f080d5e2eff3bd9604d266ef9c355d89695d62940839c8972e899a1c7ea","md5":"0f181fb4625ea1eb59e42e8be52de77d","sha256":"5278a9a32893a7b12397e15d8ec7c4acb20bea5d73e7ac3246cb4a15cdbed292"},"downloads":-1,"filename":"mixture_of_attention-0.0.23-py3-none-any.whl","has_sig":false,"md5_digest":"0f181fb4625ea1eb59e42e8be52de77d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":10481,"upload_time":"2023-06-23T22:01:04","upload_time_iso_8601":"2023-06-23T22:01:04.518697Z","url":"https://files.pythonhosted.org/packages/70/ef/5f080d5e2eff3bd9604d266ef9c355d89695d62940839c8972e899a1c7ea/mixture_of_attention-0.0.23-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1fe7161f3a9e590c2df63e0ba647cecec358fc4c6368ce4c7e93dfae2b781e53","md5":"879ddbe0b25aec3776ea8e0afca1bdb4","sha256":"2df010985db045f65a4cb20d9951da7e67d43a0d15a07a1a44bea787b12b5cec"},"downloads":-1,"filename":"mixture-of-attention-0.0.23.tar.gz","has_sig":false,"md5_digest":"879ddbe0b25aec3776ea8e0afca1bdb4","packagetype":"sdist","python_version":"source","requires_python":null,"size":10634,"upload_time":"2023-06-23T22:01:06","upload_time_iso_8601":"2023-06-23T22:01:06.159402Z","url":"https://files.pythonhosted.org/packages/1f/e7/161f3a9e590c2df63e0ba647cecec358fc4c6368ce4c7e93dfae2b781e53/mixture-of-attention-0.0.23.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.24":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.24/","requires_dist":["colt5-attention (>=0.10.14)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.24","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"9aa963d85c51f4dd1671e472553be877309b7950320544f231a096e504d11752","md5":"01ca59a39681614a1d294b210ed2594b","sha256":"efa65f9e2853fc19ff623dd3e90e5225d0c5c06e68a731644e44c42b3665e40b"},"downloads":-1,"filename":"mixture_of_attention-0.0.24-py3-none-any.whl","has_sig":false,"md5_digest":"01ca59a39681614a1d294b210ed2594b","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":10525,"upload_time":"2023-06-23T22:54:48","upload_time_iso_8601":"2023-06-23T22:54:48.376434Z","url":"https://files.pythonhosted.org/packages/9a/a9/63d85c51f4dd1671e472553be877309b7950320544f231a096e504d11752/mixture_of_attention-0.0.24-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"055a45a8523ba07e86df09136480b1c60599209192f57bfa270ad00867d26aee","md5":"f9ae8fe9f83b40739fd0f3f0d558ba63","sha256":"190c5f03d2c51fdcccfdf682025014aac27657735ccef2c2735dfd502dfbfc54"},"downloads":-1,"filename":"mixture-of-attention-0.0.24.tar.gz","has_sig":false,"md5_digest":"f9ae8fe9f83b40739fd0f3f0d558ba63","packagetype":"sdist","python_version":"source","requires_python":null,"size":10667,"upload_time":"2023-06-23T22:54:49","upload_time_iso_8601":"2023-06-23T22:54:49.322391Z","url":"https://files.pythonhosted.org/packages/05/5a/45a8523ba07e86df09136480b1c60599209192f57bfa270ad00867d26aee/mixture-of-attention-0.0.24.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.25":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence, deep learning, transformers, attention mechanism, mixture-of-experts, routed attention","license":"MIT","maintainer":null,"maintainer_email":null,"name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.25/","requires_dist":["colt5-attention>=0.10.14","einops>=0.6.1","local-attention>=1.8.6","torch>=1.6"],"requires_python":null,"summary":"Mixture of Attention","version":"0.0.25","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"f92c0a018dd49bbd16b92ae423a8b63bb88c7ac3fbf5cf239f1fff0d160624d6","md5":"4f89db9f3040343adfb3c770920da292","sha256":"93d10286ced5efc5da1adce98433af098691de8ac2378a62cf383b9d41193090"},"downloads":-1,"filename":"mixture_of_attention-0.0.25-py3-none-any.whl","has_sig":false,"md5_digest":"4f89db9f3040343adfb3c770920da292","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":11135,"upload_time":"2024-10-17T12:04:48","upload_time_iso_8601":"2024-10-17T12:04:48.008438Z","url":"https://files.pythonhosted.org/packages/f9/2c/0a018dd49bbd16b92ae423a8b63bb88c7ac3fbf5cf239f1fff0d160624d6/mixture_of_attention-0.0.25-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"05c82e4567637872d86b6407cc720157ef30c6707b718f3418c9033a80a7308b","md5":"26f991ea77c86e69462111809753cf39","sha256":"4301b3c542345e4077155dbd396ffdd5611f961e98a14c22ff3fc074990c7625"},"downloads":-1,"filename":"mixture_of_attention-0.0.25.tar.gz","has_sig":false,"md5_digest":"26f991ea77c86e69462111809753cf39","packagetype":"sdist","python_version":"source","requires_python":null,"size":11113,"upload_time":"2024-10-17T12:04:49","upload_time_iso_8601":"2024-10-17T12:04:49.482781Z","url":"https://files.pythonhosted.org/packages/05/c8/2e4567637872d86b6407cc720157ef30c6707b718f3418c9033a80a7308b/mixture_of_attention-0.0.25.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.3":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.3/","requires_dist":["colt5-attention (>=0.8.0)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.3","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"456e1ba1a6c53ba8f7e13c3fd32e1aa02b319bbbc1702705f75f1990f1dfc2f1","md5":"1e605984287c7bd69de8e95536dc2f79","sha256":"0ae71e0edb3111fa1da4a212a6769793e8eb13e3686f6ddfa1de0036e880b3cf"},"downloads":-1,"filename":"mixture_of_attention-0.0.3-py3-none-any.whl","has_sig":false,"md5_digest":"1e605984287c7bd69de8e95536dc2f79","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6528,"upload_time":"2023-05-15T19:26:14","upload_time_iso_8601":"2023-05-15T19:26:14.046373Z","url":"https://files.pythonhosted.org/packages/45/6e/1ba1a6c53ba8f7e13c3fd32e1aa02b319bbbc1702705f75f1990f1dfc2f1/mixture_of_attention-0.0.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"827d6dbccf1427c1aa963c752c20cea9a18216394afc5684ac586d2025e12d41","md5":"4e1eaf99d0fd05dcca130b36cb6ec261","sha256":"d1c20201a514f3a9d522591b80f8b8aa49765672153d537b1752809b009b8331"},"downloads":-1,"filename":"mixture-of-attention-0.0.3.tar.gz","has_sig":false,"md5_digest":"4e1eaf99d0fd05dcca130b36cb6ec261","packagetype":"sdist","python_version":"source","requires_python":null,"size":6781,"upload_time":"2023-05-15T19:26:15","upload_time_iso_8601":"2023-05-15T19:26:15.464039Z","url":"https://files.pythonhosted.org/packages/82/7d/6dbccf1427c1aa963c752c20cea9a18216394afc5684ac586d2025e12d41/mixture-of-attention-0.0.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.4":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.4/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.4","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"3a345b34584e1a8b927487251b34545707fb6a823e5765ce59d75c46d4da5abd","md5":"e6ed8f22f3c9e1a885d5366c78bd8be2","sha256":"a2c4043553992d8348b6793797f2f9101667284cef3733f9068b94abc8f72ced"},"downloads":-1,"filename":"mixture_of_attention-0.0.4-py3-none-any.whl","has_sig":false,"md5_digest":"e6ed8f22f3c9e1a885d5366c78bd8be2","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6512,"upload_time":"2023-05-15T22:04:59","upload_time_iso_8601":"2023-05-15T22:04:59.577108Z","url":"https://files.pythonhosted.org/packages/3a/34/5b34584e1a8b927487251b34545707fb6a823e5765ce59d75c46d4da5abd/mixture_of_attention-0.0.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ed71989549901ca512f7b4ac7be40dd9d524656693b51a9447faf844a933baf3","md5":"bab7ce0ad9b3bafc9bf7fe376b01723f","sha256":"ea58417e8ec45d69efde2a5d4217e1f2fe680199ed2922785c09feb43f8203ed"},"downloads":-1,"filename":"mixture-of-attention-0.0.4.tar.gz","has_sig":false,"md5_digest":"bab7ce0ad9b3bafc9bf7fe376b01723f","packagetype":"sdist","python_version":"source","requires_python":null,"size":6804,"upload_time":"2023-05-15T22:05:01","upload_time_iso_8601":"2023-05-15T22:05:01.621308Z","url":"https://files.pythonhosted.org/packages/ed/71/989549901ca512f7b4ac7be40dd9d524656693b51a9447faf844a933baf3/mixture-of-attention-0.0.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.5":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.5/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.5","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"ec0c7f25acd6235f4f722f5d359094c93d040fb350215003c37d98dabd6d97a9","md5":"4e2b8a63c8a32f3691fdc88fca2b6963","sha256":"b3f940ebe7a3df53fb12b39efcbda81a68209a3add005dde20aaa3ec18840237"},"downloads":-1,"filename":"mixture_of_attention-0.0.5-py3-none-any.whl","has_sig":false,"md5_digest":"4e2b8a63c8a32f3691fdc88fca2b6963","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6539,"upload_time":"2023-05-16T18:27:09","upload_time_iso_8601":"2023-05-16T18:27:09.678113Z","url":"https://files.pythonhosted.org/packages/ec/0c/7f25acd6235f4f722f5d359094c93d040fb350215003c37d98dabd6d97a9/mixture_of_attention-0.0.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"fece19e775bfeae372fcd2c8a490818c273f7d681d086cf9c1e666503b4ca998","md5":"4ee4c4abd4efeff568c70b06a5b4cd3b","sha256":"101334394ff842e1d41a0a884fe07ec00f2b7827402911833f83a26ab509c2e4"},"downloads":-1,"filename":"mixture-of-attention-0.0.5.tar.gz","has_sig":false,"md5_digest":"4ee4c4abd4efeff568c70b06a5b4cd3b","packagetype":"sdist","python_version":"source","requires_python":null,"size":6929,"upload_time":"2023-05-16T18:27:10","upload_time_iso_8601":"2023-05-16T18:27:10.823464Z","url":"https://files.pythonhosted.org/packages/fe/ce/19e775bfeae372fcd2c8a490818c273f7d681d086cf9c1e666503b4ca998/mixture-of-attention-0.0.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.6":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.6/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.6","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"2d501292042410f36eaadf065abc72f59d6a1fee9b3f43d58d72617541f2659b","md5":"2019d4e907e29ff37e0b39508b3dd8e0","sha256":"afc6e6a67e0d76ff85c412bb14778af880b21f6cb56c208ec87c75b17872dba1"},"downloads":-1,"filename":"mixture_of_attention-0.0.6-py3-none-any.whl","has_sig":false,"md5_digest":"2019d4e907e29ff37e0b39508b3dd8e0","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6553,"upload_time":"2023-05-16T18:31:23","upload_time_iso_8601":"2023-05-16T18:31:23.080085Z","url":"https://files.pythonhosted.org/packages/2d/50/1292042410f36eaadf065abc72f59d6a1fee9b3f43d58d72617541f2659b/mixture_of_attention-0.0.6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9ac681a574df3b705a2b363df8716811ca2c8bac5f83d0d18bef88fe8e2588bc","md5":"ae4db218adde81714682baa0c5701805","sha256":"4d11ca19d84c5fee4eef1697fe37520ff058c1621d348b4762ebfbe82b22f791"},"downloads":-1,"filename":"mixture-of-attention-0.0.6.tar.gz","has_sig":false,"md5_digest":"ae4db218adde81714682baa0c5701805","packagetype":"sdist","python_version":"source","requires_python":null,"size":6925,"upload_time":"2023-05-16T18:31:24","upload_time_iso_8601":"2023-05-16T18:31:24.252098Z","url":"https://files.pythonhosted.org/packages/9a/c6/81a574df3b705a2b363df8716811ca2c8bac5f83d0d18bef88fe8e2588bc/mixture-of-attention-0.0.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.8":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.8/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.8","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"0632209ed62d825c481c45e851433f2f94237f58c65452c55ee2fde507ed3cb3","md5":"6b51c8336439a5aa20b66c20b9bf3a7d","sha256":"e55eb8aa64ca1aff9e67b22e588c51f45fc0fb2591f1011c5dcb8e1dd8e2bd86"},"downloads":-1,"filename":"mixture_of_attention-0.0.8-py3-none-any.whl","has_sig":false,"md5_digest":"6b51c8336439a5aa20b66c20b9bf3a7d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6752,"upload_time":"2023-05-16T18:45:01","upload_time_iso_8601":"2023-05-16T18:45:01.017239Z","url":"https://files.pythonhosted.org/packages/06/32/209ed62d825c481c45e851433f2f94237f58c65452c55ee2fde507ed3cb3/mixture_of_attention-0.0.8-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"0d222dd6caf2d14841f29b91fbb951059a47b568dfb047bf621a4873a2d92924","md5":"963955d95c1624ae4e5852760d01c837","sha256":"7cc42820acb682d28502724117d852672eafaa0ec38d7aa5bc9e18937d7a6af3"},"downloads":-1,"filename":"mixture-of-attention-0.0.8.tar.gz","has_sig":false,"md5_digest":"963955d95c1624ae4e5852760d01c837","packagetype":"sdist","python_version":"source","requires_python":null,"size":7117,"upload_time":"2023-05-16T18:45:03","upload_time_iso_8601":"2023-05-16T18:45:03.375880Z","url":"https://files.pythonhosted.org/packages/0d/22/2dd6caf2d14841f29b91fbb951059a47b568dfb047bf621a4873a2d92924/mixture-of-attention-0.0.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.9":{"info":{"author":"Phil Wang","author_email":"lucidrains@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3.6","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/lucidrains/mixture-of-attention","keywords":"artificial intelligence,deep learning,transformers,attention mechanism,mixture-of-experts,routed attention","license":"MIT","maintainer":"","maintainer_email":"","name":"mixture-of-attention","package_url":"https://pypi.org/project/mixture-of-attention/","platform":null,"project_url":"https://pypi.org/project/mixture-of-attention/","project_urls":{"Homepage":"https://github.com/lucidrains/mixture-of-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/mixture-of-attention/0.0.9/","requires_dist":["colt5-attention (>=0.8.1)","einops (>=0.6.1)","local-attention (>=1.8.6)","torch (>=1.6)"],"requires_python":"","summary":"Mixture of Attention","version":"0.0.9","yanked":false,"yanked_reason":null},"last_serial":25526552,"urls":[{"comment_text":"","digests":{"blake2b_256":"51e19ea29ea5aa1a5c9f55379e9b84678298ed9754418d9f1ebabac293214740","md5":"d6545e806220f9e94ba5e9e3ab999110","sha256":"4b0cb71e3f727e519b8a7be5b86387b7f7b2329ca2343dcdfe4212e39c9f5302"},"downloads":-1,"filename":"mixture_of_attention-0.0.9-py3-none-any.whl","has_sig":false,"md5_digest":"d6545e806220f9e94ba5e9e3ab999110","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":6975,"upload_time":"2023-05-16T20:08:38","upload_time_iso_8601":"2023-05-16T20:08:38.759163Z","url":"https://files.pythonhosted.org/packages/51/e1/9ea29ea5aa1a5c9f55379e9b84678298ed9754418d9f1ebabac293214740/mixture_of_attention-0.0.9-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"67a6a1d065797692d42fe3810427aa234b035b69869ee0f257f19c58020fa8e3","md5":"7bb74698c9243c1e96029d7128a92b3f","sha256":"81cede3960d968c09be50736f04f4e8b796ffc61bfed489e0e0f5e27bb38ee87"},"downloads":-1,"filename":"mixture-of-attention-0.0.9.tar.gz","has_sig":false,"md5_digest":"7bb74698c9243c1e96029d7128a92b3f","packagetype":"sdist","python_version":"source","requires_python":null,"size":7330,"upload_time":"2023-05-16T20:08:40","upload_time_iso_8601":"2023-05-16T20:08:40.264904Z","url":"https://files.pythonhosted.org/packages/67/a6/a1d065797692d42fe3810427aa234b035b69869ee0f257f19c58020fa8e3/mixture-of-attention-0.0.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}