{"0.1.4":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.4/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"5287def10e67fd56a70bdc618e4a66b41b44a13c76d8c67be021d22ab275b740","md5":"24256cca83b89898019b56782781d8ce","sha256":"78acb03f3dafd72d8d0991c484ee7f7d0a218f04c226399e8b206bf8c04eed2a"},"downloads":-1,"filename":"minference-0.1.4.tar.gz","has_sig":false,"md5_digest":"24256cca83b89898019b56782781d8ce","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":112585,"upload_time":"2024-07-05T08:22:43","upload_time_iso_8601":"2024-07-05T08:22:43.018898Z","url":"https://files.pythonhosted.org/packages/52/87/def10e67fd56a70bdc618e4a66b41b44a13c76d8c67be021d22ab275b740/minference-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4.post1":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.4.post1/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.4.post1","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"550be1f306afdacf0563b9fbb39db90b9d70de3e3eaf32dd278eddcaefc100bc","md5":"2f79ff8709f3685555c66ffbf831e672","sha256":"345c2d1de04a0046a2935205a25eb1ef312a9dadde95ff66e88be99dbbd6571b"},"downloads":-1,"filename":"minference-0.1.4.post1.tar.gz","has_sig":false,"md5_digest":"2f79ff8709f3685555c66ffbf831e672","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":113379,"upload_time":"2024-07-07T08:14:52","upload_time_iso_8601":"2024-07-07T08:14:52.739271Z","url":"https://files.pythonhosted.org/packages/55/0b/e1f306afdacf0563b9fbb39db90b9d70de3e3eaf32dd278eddcaefc100bc/minference-0.1.4.post1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4.post2":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.4.post2/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.4.post2","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"12f82f1c209d82201b123b8c82de614716f9bae4dfb5f5fb0b777e0f40b15646","md5":"464b4711c31d3cc42883d55b30a0824c","sha256":"955595d972c6b08b376b50826a2bba81c20be7b85085ee5a8d136eb85ee58e5f"},"downloads":-1,"filename":"minference-0.1.4.post2.tar.gz","has_sig":false,"md5_digest":"464b4711c31d3cc42883d55b30a0824c","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":114089,"upload_time":"2024-07-12T08:02:33","upload_time_iso_8601":"2024-07-12T08:02:33.841566Z","url":"https://files.pythonhosted.org/packages/12/f8/2f1c209d82201b123b8c82de614716f9bae4dfb5f5fb0b777e0f40b15646/minference-0.1.4.post2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4.post3":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.4.post3/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.4.post3","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"82a4e702e18a2a5936db30fa98018aa9dc7d81b614d98ae878bf0fe9a4de26f0","md5":"a23285df6070a5ba169fc2ec061e03e8","sha256":"332b133e2efce6f8ff51d6633b1b3ffa63edda4877628cacf60d747f8dba4a9f"},"downloads":-1,"filename":"minference-0.1.4.post3.tar.gz","has_sig":false,"md5_digest":"a23285df6070a5ba169fc2ec061e03e8","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":116673,"upload_time":"2024-07-15T06:11:38","upload_time_iso_8601":"2024-07-15T06:11:38.857083Z","url":"https://files.pythonhosted.org/packages/82/a4/e702e18a2a5936db30fa98018aa9dc7d81b614d98ae878bf0fe9a4de26f0/minference-0.1.4.post3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4.post4":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.4.post4/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.4.post4","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"2e13ebe9d61a2c8ca1f8f055006757998d396d289bf31dd1cfcd56ab155821b4","md5":"ea4e0a50907d8695a32d30143bf0f2d7","sha256":"40695f4e2340de145403926d36aa70edaef59eeda75e7d2c9ae71153e9a7b9b0"},"downloads":-1,"filename":"minference-0.1.4.post4.tar.gz","has_sig":false,"md5_digest":"ea4e0a50907d8695a32d30143bf0f2d7","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":116820,"upload_time":"2024-07-16T08:03:55","upload_time_iso_8601":"2024-07-16T08:03:55.520644Z","url":"https://files.pythonhosted.org/packages/2e/13/ebe9d61a2c8ca1f8f055006757998d396d289bf31dd1cfcd56ab155821b4/minference-0.1.4.post4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.5/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"420907ae4e76413e288e7f60e3799cf44ab3e272cf0d3ba51a185f7994ddde08","md5":"8d094bbd2d88d840900b33f5a03af1f6","sha256":"61c661bd22aaf364573f9d22ab9bf73812aed82d8236fadee53d3c9b8e92df2e"},"downloads":-1,"filename":"minference-0.1.5.tar.gz","has_sig":false,"md5_digest":"8d094bbd2d88d840900b33f5a03af1f6","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":126949,"upload_time":"2024-07-24T11:53:11","upload_time_iso_8601":"2024-07-24T11:53:11.288487Z","url":"https://files.pythonhosted.org/packages/42/09/07ae4e76413e288e7f60e3799cf44ab3e272cf0d3ba51a185f7994ddde08/minference-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5.post1":{"info":{"author":"The MInference team","author_email":"hjiang@microsoft.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Science/Research","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/MInference","keywords":"LLMs Inference, Long-Context LLMs, Dynamic Sparse Attention, Efficient Inference","license":"MIT License","maintainer":null,"maintainer_email":null,"name":"minference","package_url":"https://pypi.org/project/minference/","platform":null,"project_url":"https://pypi.org/project/minference/","project_urls":{"Homepage":"https://github.com/microsoft/MInference"},"provides_extra":["dev","quality"],"release_url":"https://pypi.org/project/minference/0.1.5.post1/","requires_dist":null,"requires_python":">=3.8.0","summary":"To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.","version":"0.1.5.post1","yanked":false,"yanked_reason":null},"last_serial":24547953,"urls":[{"comment_text":"","digests":{"blake2b_256":"2cd120230a3364304c56311044d91339c394dc0912d77227fb42bfb8ca8501b8","md5":"87697dfedd78efa517081db364cf0e89","sha256":"03bae0197e7380c3c1d8eb16df6a02656e383929da726266d5cb719a55ba8b8c"},"downloads":-1,"filename":"minference-0.1.5.post1.tar.gz","has_sig":false,"md5_digest":"87697dfedd78efa517081db364cf0e89","packagetype":"sdist","python_version":"source","requires_python":">=3.8.0","size":181966,"upload_time":"2024-08-13T09:39:09","upload_time_iso_8601":"2024-08-13T09:39:09.411849Z","url":"https://files.pythonhosted.org/packages/2c/d1/20230a3364304c56311044d91339c394dc0912d77227fb42bfb8ca8501b8/minference-0.1.5.post1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}