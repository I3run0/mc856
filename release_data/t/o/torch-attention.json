{"1.0.0":{"info":{"author":"changzy00","author_email":"changzy@pku.org.cn","bugtrack_url":null,"classifiers":["Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/changzy00/pytorch-attention","keywords":"AttentionCNNsMLPsViTs","license":"Apache","maintainer":"","maintainer_email":"","name":"torch-attention","package_url":"https://pypi.org/project/torch-attention/","platform":null,"project_url":"https://pypi.org/project/torch-attention/","project_urls":{"Homepage":"https://github.com/changzy00/pytorch-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/torch-attention/1.0.0/","requires_dist":null,"requires_python":">=3.7.0","summary":"Pytorch implementation of popular Attention Mechanisms, Vision Transformers, MLP-Like models and CNNs.","version":"1.0.0","yanked":false,"yanked_reason":null},"last_serial":18542452,"urls":[{"comment_text":"","digests":{"blake2b_256":"809d527a370aa3ce2037a9fe4e7a3e3d3d2705318e00eaf3aaf7ee1ce0ca6e03","md5":"4054f7b6e6580068df23e2f864e00651","sha256":"1e1b6850a73c3e80b7d7f09266554862e4816de8c075deecfaca27be86966492"},"downloads":-1,"filename":"torch-attention-1.0.0.tar.gz","has_sig":false,"md5_digest":"4054f7b6e6580068df23e2f864e00651","packagetype":"sdist","python_version":"source","requires_python":">=3.7.0","size":3354,"upload_time":"2023-06-17T10:56:19","upload_time_iso_8601":"2023-06-17T10:56:19.322657Z","url":"https://files.pythonhosted.org/packages/80/9d/527a370aa3ce2037a9fe4e7a3e3d3d2705318e00eaf3aaf7ee1ce0ca6e03/torch-attention-1.0.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}