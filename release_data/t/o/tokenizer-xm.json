{"0.1":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Topic :: Software Development :: Build Tools"],"description_content_type":"","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_01.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_01.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/0.1/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to exclude contractions, lemmatize and stem.","version":"0.1","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"552c4d19ddee2423395aeb70dca5413716e4028665b354c5b489df460fb7da53","md5":"a0edfa44bf4e72e7c86162d61cf8727d","sha256":"ede096ef449268c8bf955f48e495430ca1e4616f5f58a6d7761a75cc19a157aa"},"downloads":-1,"filename":"tokenizer_xm-0.1.tar.gz","has_sig":false,"md5_digest":"a0edfa44bf4e72e7c86162d61cf8727d","packagetype":"sdist","python_version":"source","requires_python":null,"size":2994,"upload_time":"2019-09-20T19:44:48","upload_time_iso_8601":"2019-09-20T19:44:48.717767Z","url":"https://files.pythonhosted.org/packages/55/2c/4d19ddee2423395aeb70dca5413716e4028665b354c5b489df460fb7da53/tokenizer_xm-0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Topic :: Software Development :: Build Tools"],"description_content_type":"","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_02.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_02.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/0.2/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to exclude contractions, lemmatize and stem.","version":"0.2","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"b7c88b2ce0816e126c9b3661ba921ff3616d10e080ad007b83b775a98d383601","md5":"7d6e59a9959d14539d308d4c53c1099e","sha256":"e6cb18dceaa547eef85fe7d472b82536e3a024d77737b629ed223953c91ec454"},"downloads":-1,"filename":"tokenizer_xm-0.2.tar.gz","has_sig":false,"md5_digest":"7d6e59a9959d14539d308d4c53c1099e","packagetype":"sdist","python_version":"source","requires_python":null,"size":3013,"upload_time":"2019-09-20T20:12:08","upload_time_iso_8601":"2019-09-20T20:12:08.952991Z","url":"https://files.pythonhosted.org/packages/b7/c8/8b2ce0816e126c9b3661ba921ff3616d10e080ad007b83b775a98d383601/tokenizer_xm-0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.4":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Topic :: Software Development :: Build Tools"],"description_content_type":"","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_04.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_04.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/0.4/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to exclude contractions, lemmatize and stem.","version":"0.4","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"5196f6a80fa7cabbc2c2c1f7b3bdfa50a201cf7015c09ebff93ea5c5787906a2","md5":"d37ecab1f64f52f86679afca0f6af758","sha256":"d2200dee67cb2fbafcf1bbf7eb7c806db2ec812db538f0bae8dfa8f96e6d3bed"},"downloads":-1,"filename":"tokenizer_xm-0.4.tar.gz","has_sig":false,"md5_digest":"d37ecab1f64f52f86679afca0f6af758","packagetype":"sdist","python_version":"source","requires_python":null,"size":3006,"upload_time":"2019-11-19T00:05:44","upload_time_iso_8601":"2019-11-19T00:05:44.774289Z","url":"https://files.pythonhosted.org/packages/51/96/f6a80fa7cabbc2c2c1f7b3bdfa50a201cf7015c09ebff93ea5c5787906a2/tokenizer_xm-0.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.5":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Topic :: Software Development :: Build Tools"],"description_content_type":"text/markdown","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_05.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_05.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/0.5/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to include contractions, lemmatize and stem.","version":"0.5","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"85f5ba28eb6fe7c743f1cf6afd7a62467d2916efedbbcab1698e0e03c712249f","md5":"09051bb2c83ccc373e1819d9806544ea","sha256":"9175769ae7df5e914c083229a95776b10db0de566fbff9fd245b3187f69cf25d"},"downloads":-1,"filename":"tokenizer_xm-0.5.tar.gz","has_sig":false,"md5_digest":"09051bb2c83ccc373e1819d9806544ea","packagetype":"sdist","python_version":"source","requires_python":null,"size":4945,"upload_time":"2019-12-09T00:23:28","upload_time_iso_8601":"2019-12-09T00:23:28.977935Z","url":"https://files.pythonhosted.org/packages/85/f5/ba28eb6fe7c743f1cf6afd7a62467d2916efedbbcab1698e0e03c712249f/tokenizer_xm-0.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Programming Language :: Python :: 3.7","Topic :: Software Development :: Build Tools"],"description_content_type":"text/markdown","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_10.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_10.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/1.0/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to include contractions, lemmatize and stem.","version":"1.0","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"fba5e36b53c0db4faa918f1c4dbaa1aff66315c9efd2118229dac08d73afeceb","md5":"903616871eff3be6e4b294d5ebc81f43","sha256":"a85c2fb6fe1c7620fa9d658f22dde53969c30dd8cd506c11ca8adb403debfe96"},"downloads":-1,"filename":"tokenizer_xm-1.0.tar.gz","has_sig":false,"md5_digest":"903616871eff3be6e4b294d5ebc81f43","packagetype":"sdist","python_version":"source","requires_python":null,"size":4742,"upload_time":"2021-08-31T22:52:05","upload_time_iso_8601":"2021-08-31T22:52:05.980253Z","url":"https://files.pythonhosted.org/packages/fb/a5/e36b53c0db4faa918f1c4dbaa1aff66315c9efd2118229dac08d73afeceb/tokenizer_xm-1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.1":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Programming Language :: Python :: 3.7","Topic :: Software Development :: Build Tools"],"description_content_type":"text/markdown","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_101.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_101.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/1.0.1/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to include contractions, lemmatize and stem.","version":"1.0.1","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"4498e171e7bc81814a7f659d1dad8bc96a2a607380a90669aed27a2f2829560e","md5":"22a0d48e950ff01513d56badd0a8b162","sha256":"9fa7de7cd9c121ce8e816a8bac3862c01aacbe01ea4449101017c80d554e433f"},"downloads":-1,"filename":"tokenizer_xm-1.0.1.tar.gz","has_sig":false,"md5_digest":"22a0d48e950ff01513d56badd0a8b162","packagetype":"sdist","python_version":"source","requires_python":null,"size":4756,"upload_time":"2021-08-31T23:06:56","upload_time_iso_8601":"2021-08-31T23:06:56.367143Z","url":"https://files.pythonhosted.org/packages/44/98/e171e7bc81814a7f659d1dad8bc96a2a607380a90669aed27a2f2829560e/tokenizer_xm-1.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.2":{"info":{"author":"Xiao Ma","author_email":"Marshalma0923@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","License :: OSI Approved :: MIT License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.4","Programming Language :: Python :: 3.5","Programming Language :: Python :: 3.6","Programming Language :: Python :: 3.7","Topic :: Software Development :: Build Tools"],"description_content_type":"text/markdown","docs_url":null,"download_url":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_102.tar.gz","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ALaughingHorse/tokenizer_xm","keywords":"text preprocessing,tokenize,NLP","license":"MIT","maintainer":"","maintainer_email":"","name":"tokenizer-xm","package_url":"https://pypi.org/project/tokenizer-xm/","platform":"","project_url":"https://pypi.org/project/tokenizer-xm/","project_urls":{"Download":"https://github.com/ALaughingHorse/tokenizer_xm/archive/v_102.tar.gz","Homepage":"https://github.com/ALaughingHorse/tokenizer_xm"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizer-xm/1.0.2/","requires_dist":null,"requires_python":"","summary":"Tokenizing with options to include contractions, lemmatize and stem.","version":"1.0.2","yanked":false,"yanked_reason":null},"last_serial":11330511,"urls":[{"comment_text":"","digests":{"blake2b_256":"037300577ab0c92f2db25e10741db9c7912b5f6b6f5716286aa0442706eb1a2f","md5":"7b14d8ad040b8b4fe6a19fdd81b805ce","sha256":"ea1bfd7bdd31ecd15b8973eb22e08b34a5d481616e58fd49406278531852f118"},"downloads":-1,"filename":"tokenizer_xm-1.0.2.tar.gz","has_sig":false,"md5_digest":"7b14d8ad040b8b4fe6a19fdd81b805ce","packagetype":"sdist","python_version":"source","requires_python":null,"size":4751,"upload_time":"2021-08-31T23:15:56","upload_time_iso_8601":"2021-08-31T23:15:56.388224Z","url":"https://files.pythonhosted.org/packages/03/73/00577ab0c92f2db25e10741db9c7912b5f6b6f5716286aa0442706eb1a2f/tokenizer_xm-1.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}