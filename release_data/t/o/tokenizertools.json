{"0.1a":{"info":{"author":"David B. Curtis","author_email":"davecurtis@sonic.net","bugtrack_url":null,"classifiers":[],"description_content_type":null,"docs_url":null,"download_url":"UNKNOWN","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"http://github.com/dbc/tokenizertools","keywords":null,"license":"BSD","maintainer":null,"maintainer_email":null,"name":"tokenizertools","package_url":"https://pypi.org/project/tokenizertools/","platform":"any","project_url":"https://pypi.org/project/tokenizertools/","project_urls":{"Download":"UNKNOWN","Homepage":"http://github.com/dbc/tokenizertools"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizertools/0.1a/","requires_dist":null,"requires_python":null,"summary":"Implements lexical analyzers as iterators yielding tokens.","version":"0.1a","yanked":false,"yanked_reason":null},"last_serial":2346676,"urls":[{"comment_text":"","digests":{"blake2b_256":"41b8a06793517fc55f0865cb47f2e6d93c62bb0f980684a5f807eca59608a32a","md5":"cdae97ab40f82c6c818e86a498413443","sha256":"bd0238545f7275b670d71ce3fd899c8b5a81febb0faefcf2d4052ad20d140301"},"downloads":-1,"filename":"tokenizertools-0.1a.tar.gz","has_sig":false,"md5_digest":"cdae97ab40f82c6c818e86a498413443","packagetype":"sdist","python_version":"source","requires_python":null,"size":10139,"upload_time":"2014-06-17T04:44:16","upload_time_iso_8601":"2014-06-17T04:44:16.580188Z","url":"https://files.pythonhosted.org/packages/41/b8/a06793517fc55f0865cb47f2e6d93c62bb0f980684a5f807eca59608a32a/tokenizertools-0.1a.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0":{"info":{"author":"David B. Curtis","author_email":"davecurtis@sonic.net","bugtrack_url":null,"classifiers":[],"description_content_type":null,"docs_url":null,"download_url":"UNKNOWN","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"http://github.com/dbc/tokenizertools","keywords":null,"license":"BSD","maintainer":null,"maintainer_email":null,"name":"tokenizertools","package_url":"https://pypi.org/project/tokenizertools/","platform":"any","project_url":"https://pypi.org/project/tokenizertools/","project_urls":{"Download":"UNKNOWN","Homepage":"http://github.com/dbc/tokenizertools"},"provides_extra":null,"release_url":"https://pypi.org/project/tokenizertools/1.0/","requires_dist":null,"requires_python":null,"summary":"Implements lexical analyzers as iterators yielding tokens.","version":"1.0","yanked":false,"yanked_reason":null},"last_serial":2346676,"urls":[{"comment_text":"","digests":{"blake2b_256":"42813efe92537748299e3cd7cbfe519819855d17a6aecd92bb652d2310496bb7","md5":"944878faa5c1859e3db1b1301592cfdb","sha256":"44074a442b456c48918ee99cb09ff9b7fccc3708f94e13d13d0ff637daf3f7f7"},"downloads":-1,"filename":"tokenizertools-1.0.tar.gz","has_sig":false,"md5_digest":"944878faa5c1859e3db1b1301592cfdb","packagetype":"sdist","python_version":"source","requires_python":null,"size":10858,"upload_time":"2016-09-16T17:13:16","upload_time_iso_8601":"2016-09-16T17:13:16.550541Z","url":"https://files.pythonhosted.org/packages/42/81/3efe92537748299e3cd7cbfe519819855d17a6aecd92bb652d2310496bb7/tokenizertools-1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}