{"0.1.0":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.0/","requires_dist":null,"requires_python":">=3.9","summary":"An old fashioned rule-based tokenizer for Turkish","version":"0.1.0","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"f8f1dba329b76defb025cde0ea36647430116889c8ceaf320890501c62a3b0e8","md5":"949939a387b7fbc9be383c3b288e55e1","sha256":"eb7eebcafb5fa1d7f1db4026f8620a42fb5545dd6077c76cb2dcecdf67f4ce64"},"downloads":-1,"filename":"ts_tokenizer-0.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"949939a387b7fbc9be383c3b288e55e1","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9387793,"upload_time":"2024-05-21T23:37:43","upload_time_iso_8601":"2024-05-21T23:37:43.281634Z","url":"https://files.pythonhosted.org/packages/f8/f1/dba329b76defb025cde0ea36647430116889c8ceaf320890501c62a3b0e8/ts_tokenizer-0.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"8516a79696de58ec2dc3c6c537d4d58a9356519afc884b5bd9d89e5dca8d3ed1","md5":"5c9ec55cad027378d72cfeffcf0968e2","sha256":"a6ddccf8664581916132746ad15b4ad18c9f58276aeec0badd8f85f918866584"},"downloads":-1,"filename":"ts_tokenizer-0.1.0.tar.gz","has_sig":false,"md5_digest":"5c9ec55cad027378d72cfeffcf0968e2","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":9439156,"upload_time":"2024-05-21T23:37:49","upload_time_iso_8601":"2024-05-21T23:37:49.501267Z","url":"https://files.pythonhosted.org/packages/85/16/a79696de58ec2dc3c6c537d4d58a9356519afc884b5bd9d89e5dca8d3ed1/ts_tokenizer-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.1/","requires_dist":null,"requires_python":">=3.9","summary":"TS Tokenizer is a Turkish Tokenizer.","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"280fe4ce497993e9cacc49129b28eeedb3004defa84b5c22b71a9faa5be33ab8","md5":"6a8ec32de4773c4430c5e84f5cdeae38","sha256":"941a18326d894d02e3e1f483ea656ac086ac6e78eef6906e87ed6a1ee1adbaf6"},"downloads":-1,"filename":"ts_tokenizer-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"6a8ec32de4773c4430c5e84f5cdeae38","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9396419,"upload_time":"2024-08-31T22:33:30","upload_time_iso_8601":"2024-08-31T22:33:30.511013Z","url":"https://files.pythonhosted.org/packages/28/0f/e4ce497993e9cacc49129b28eeedb3004defa84b5c22b71a9faa5be33ab8/ts_tokenizer-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"12c4225178ea538cf190b50b3840caccfbefb0d4b8fb0056ec7301f9925ae134","md5":"3d89855eb1702a5edc4321bc91e40f77","sha256":"4b1659ccf463872186024b81b7b73b6ae0dfdbb1d20627dc4648fe291ded85df"},"downloads":-1,"filename":"ts_tokenizer-0.1.1.tar.gz","has_sig":false,"md5_digest":"3d89855eb1702a5edc4321bc91e40f77","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":9448354,"upload_time":"2024-08-31T22:33:36","upload_time_iso_8601":"2024-08-31T22:33:36.132850Z","url":"https://files.pythonhosted.org/packages/12/c4/225178ea538cf190b50b3840caccfbefb0d4b8fb0056ec7301f9925ae134/ts_tokenizer-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.10":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.10/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a hybrid (lexicon-based and rule-based) tokenizer designed for Turkish text.","version":"0.1.10","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"5cb9f3ede79bb871509116d19e6fe04a8d81eb1e61760cf1966362d9c980e0f8","md5":"a2d5be0aca94b3d71ef2b2983baad68f","sha256":"88c5c9ad232aa55bc40ec11863e5087fc9256e9a7875f5249cf97c3fbbdf0875"},"downloads":-1,"filename":"ts_tokenizer-0.1.10-py3-none-any.whl","has_sig":false,"md5_digest":"a2d5be0aca94b3d71ef2b2983baad68f","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9995991,"upload_time":"2024-10-17T14:03:21","upload_time_iso_8601":"2024-10-17T14:03:21.839227Z","url":"https://files.pythonhosted.org/packages/5c/b9/f3ede79bb871509116d19e6fe04a8d81eb1e61760cf1966362d9c980e0f8/ts_tokenizer-0.1.10-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bebe18291df10175c71e2f77ccf1b01dfc15588de6234310fbf108f3af7ae620","md5":"3dd43c09e54b20a7c31d8050384c2c8a","sha256":"e573e437c3557bb84f6762ed52b74187e4e08ccc06f48a0390c6b820252d69f4"},"downloads":-1,"filename":"ts_tokenizer-0.1.10.tar.gz","has_sig":false,"md5_digest":"3dd43c09e54b20a7c31d8050384c2c8a","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10051751,"upload_time":"2024-10-17T14:03:32","upload_time_iso_8601":"2024-10-17T14:03:32.225545Z","url":"https://files.pythonhosted.org/packages/be/be/18291df10175c71e2f77ccf1b01dfc15588de6234310fbf108f3af7ae620/ts_tokenizer-0.1.10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.11":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.11/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a hybrid (lexicon-based and rule-based) tokenizer designed for Turkish text.","version":"0.1.11","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"3d15005e425b8cfe48841381ddfc822659c9a50ea844081cbb7eb7b0cee4d244","md5":"e68566af0573790154c38584fbee160c","sha256":"0d190cad78bcfbf7a45945d4ae58fbf8152429313c8c2f7fc8d1a04b8d9b6253"},"downloads":-1,"filename":"ts_tokenizer-0.1.11-py3-none-any.whl","has_sig":false,"md5_digest":"e68566af0573790154c38584fbee160c","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9995944,"upload_time":"2024-10-17T14:52:25","upload_time_iso_8601":"2024-10-17T14:52:25.605870Z","url":"https://files.pythonhosted.org/packages/3d/15/005e425b8cfe48841381ddfc822659c9a50ea844081cbb7eb7b0cee4d244/ts_tokenizer-0.1.11-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"fda6567a42d3c983a6253c7a57b4a0a39880a8192e3a52685e664e6ee74c8ad2","md5":"be58144abe4208b5ffa577fa31c83c46","sha256":"dccd6337af539955da8b4ecd1b35109f789cccade29975b453839b7065fc96f4"},"downloads":-1,"filename":"ts_tokenizer-0.1.11.tar.gz","has_sig":false,"md5_digest":"be58144abe4208b5ffa577fa31c83c46","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10051741,"upload_time":"2024-10-17T14:52:31","upload_time_iso_8601":"2024-10-17T14:52:31.106981Z","url":"https://files.pythonhosted.org/packages/fd/a6/567a42d3c983a6253c7a57b4a0a39880a8192e3a52685e664e6ee74c8ad2/ts_tokenizer-0.1.11.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.12":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.12/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a hybrid (lexicon-based and rule-based) tokenizer designed for Turkish text.","version":"0.1.12","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"addf774732b2b278e5cc227d07be85b789b9252390630d216da8f2f38c3dbc29","md5":"a90423e72b2e0a831367c8fe6811d066","sha256":"f523f3c0267a858fee6a95d15a055101d806c9fcda1faaf36a932b3f8839127b"},"downloads":-1,"filename":"ts_tokenizer-0.1.12-py3-none-any.whl","has_sig":false,"md5_digest":"a90423e72b2e0a831367c8fe6811d066","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9996250,"upload_time":"2024-10-18T06:47:56","upload_time_iso_8601":"2024-10-18T06:47:56.083153Z","url":"https://files.pythonhosted.org/packages/ad/df/774732b2b278e5cc227d07be85b789b9252390630d216da8f2f38c3dbc29/ts_tokenizer-0.1.12-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"78e03c22f4ae03be68289c042deecd5fd014b0ac322ea97e29c6fad57b8c51e3","md5":"fc0d6d6acdd4238be3d8f190b873c2ab","sha256":"1d69b40fb156ada34670d661164c9687dfcbe9850d7e32bbd897d4aa8503523d"},"downloads":-1,"filename":"ts_tokenizer-0.1.12.tar.gz","has_sig":false,"md5_digest":"fc0d6d6acdd4238be3d8f190b873c2ab","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10052010,"upload_time":"2024-10-18T06:48:00","upload_time_iso_8601":"2024-10-18T06:48:00.054621Z","url":"https://files.pythonhosted.org/packages/78/e0/3c22f4ae03be68289c042deecd5fd014b0ac322ea97e29c6fad57b8c51e3/ts_tokenizer-0.1.12.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.2":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.2/","requires_dist":null,"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.2","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"f7d31bd73e4c4adcfeb0f161372810041ee71e41db68d43e4e54b5a11d0eaad1","md5":"26857516f8a25b2223ce4b5b00a630a1","sha256":"c70b2068faa310a5f83f5afcce35ca514a73798d048f3ae3f8fa06e778c8b1b0"},"downloads":-1,"filename":"ts_tokenizer-0.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"26857516f8a25b2223ce4b5b00a630a1","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9397239,"upload_time":"2024-08-31T23:44:42","upload_time_iso_8601":"2024-08-31T23:44:42.182839Z","url":"https://files.pythonhosted.org/packages/f7/d3/1bd73e4c4adcfeb0f161372810041ee71e41db68d43e4e54b5a11d0eaad1/ts_tokenizer-0.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6be732f4c3f0d8fe08956c549ac61750af9a75c96d1f42e2c4cea2dff69d154c","md5":"454ad1d162c5775f5b9bd87b6433bb02","sha256":"da82c87fd92c4466772679ba8fd1d517821722f2e1f0933c4ac5eac2de7a5fd8"},"downloads":-1,"filename":"ts_tokenizer-0.1.2.tar.gz","has_sig":false,"md5_digest":"454ad1d162c5775f5b9bd87b6433bb02","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":9449234,"upload_time":"2024-08-31T23:44:48","upload_time_iso_8601":"2024-08-31T23:44:48.055718Z","url":"https://files.pythonhosted.org/packages/6b/e7/32f4c3f0d8fe08956c549ac61750af9a75c96d1f42e2c4cea2dff69d154c/ts_tokenizer-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.3/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"18ab1bde1565ab3d259975a5c9cbfd50c2eef02ba08c26f00f7af7a9c051ae80","md5":"5fca357f9e135ad49d440dfb286705d5","sha256":"fcae3fb7bff391430dee9e5d46083883d22b40f4aee5cbdf86a4f408bb84954f"},"downloads":-1,"filename":"ts_tokenizer-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"5fca357f9e135ad49d440dfb286705d5","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9397782,"upload_time":"2024-09-04T11:18:00","upload_time_iso_8601":"2024-09-04T11:18:00.067304Z","url":"https://files.pythonhosted.org/packages/18/ab/1bde1565ab3d259975a5c9cbfd50c2eef02ba08c26f00f7af7a9c051ae80/ts_tokenizer-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"43d0c97574424d5bee50a2cb0ee1d191ea693231b17eb6ab0b51c54d0df2a3c6","md5":"69d11b01ac176849bf6ddd2e5f1b35df","sha256":"77540c79c1d1af914eec1bf5a06939bd96d57951f523336c937603416d766def"},"downloads":-1,"filename":"ts_tokenizer-0.1.3.tar.gz","has_sig":false,"md5_digest":"69d11b01ac176849bf6ddd2e5f1b35df","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":9449782,"upload_time":"2024-09-04T11:18:05","upload_time_iso_8601":"2024-09-04T11:18:05.139155Z","url":"https://files.pythonhosted.org/packages/43/d0/c97574424d5bee50a2cb0ee1d191ea693231b17eb6ab0b51c54d0df2a3c6/ts_tokenizer-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.4/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"708724e12e121dc98a32d7c35c3a674cc528c6fea6748aa1575c9858e2a053b6","md5":"9d8ee803dab874c764dc8e04a3ca491f","sha256":"2487aaa45195feb33cad17efb1d00be4d9260e7321e60842b3601a59371525c9"},"downloads":-1,"filename":"ts_tokenizer-0.1.4-py3-none-any.whl","has_sig":false,"md5_digest":"9d8ee803dab874c764dc8e04a3ca491f","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9994472,"upload_time":"2024-10-17T10:25:43","upload_time_iso_8601":"2024-10-17T10:25:43.451617Z","url":"https://files.pythonhosted.org/packages/70/87/24e12e121dc98a32d7c35c3a674cc528c6fea6748aa1575c9858e2a053b6/ts_tokenizer-0.1.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"b24899384571eacdd853f1a2e8a9114775cf5fb57e051889c7dcb0199b446107","md5":"523cdd11653c4c4b4d8c6fc41c59bf82","sha256":"692fb8fc5cfc60680d43daef26c3b79251a7c9832fbeb6697b3b2fcc5eacae62"},"downloads":-1,"filename":"ts_tokenizer-0.1.4.tar.gz","has_sig":false,"md5_digest":"523cdd11653c4c4b4d8c6fc41c59bf82","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10049302,"upload_time":"2024-10-17T10:25:55","upload_time_iso_8601":"2024-10-17T10:25:55.720115Z","url":"https://files.pythonhosted.org/packages/b2/48/99384571eacdd853f1a2e8a9114775cf5fb57e051889c7dcb0199b446107/ts_tokenizer-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.5":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.5/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.5","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"20d4a0670cb6cbbeac0827444f111fe5507748dbedd2bcbb3a42e1086e42e690","md5":"3ddb4e927a084c58f7b4dc6dc5b50bc0","sha256":"31d4ca867e12fadbab51a26b3b06740ee47244dc417f110dd2c7bfcd4369ff2b"},"downloads":-1,"filename":"ts_tokenizer-0.1.5-py3-none-any.whl","has_sig":false,"md5_digest":"3ddb4e927a084c58f7b4dc6dc5b50bc0","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9995747,"upload_time":"2024-10-17T12:35:13","upload_time_iso_8601":"2024-10-17T12:35:13.501777Z","url":"https://files.pythonhosted.org/packages/20/d4/a0670cb6cbbeac0827444f111fe5507748dbedd2bcbb3a42e1086e42e690/ts_tokenizer-0.1.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"7801cdbf98281a2ea10cea3c049aad9f072d5ffb65bba694d260ab63a5633555","md5":"ce8da875f18f9588e733c1ad8234ff5f","sha256":"3e983db9d6c0a2b0183b3b99a65503e9862194ab45a4428a380264a560dab955"},"downloads":-1,"filename":"ts_tokenizer-0.1.5.tar.gz","has_sig":false,"md5_digest":"ce8da875f18f9588e733c1ad8234ff5f","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10051744,"upload_time":"2024-10-17T12:36:00","upload_time_iso_8601":"2024-10-17T12:36:00.712643Z","url":"https://files.pythonhosted.org/packages/78/01/cdbf98281a2ea10cea3c049aad9f072d5ffb65bba694d260ab63a5633555/ts_tokenizer-0.1.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.6":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.6/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.6","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"a2345338f458326bfa257ee700d7ac68a6eebe0f11e5c3d16d5b7565208be619","md5":"7366837fe6af34c78efae759a95024c4","sha256":"b9ba337bbfd322654421af41b16b84186de3ef1a5d9163d651bb894282182b69"},"downloads":-1,"filename":"ts_tokenizer-0.1.6-py3-none-any.whl","has_sig":false,"md5_digest":"7366837fe6af34c78efae759a95024c4","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9996103,"upload_time":"2024-10-17T12:45:35","upload_time_iso_8601":"2024-10-17T12:45:35.532347Z","url":"https://files.pythonhosted.org/packages/a2/34/5338f458326bfa257ee700d7ac68a6eebe0f11e5c3d16d5b7565208be619/ts_tokenizer-0.1.6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"476c5d6f3a7e877e1165a51fbee98b510e12bba7c71b730b026a9f35c100770e","md5":"f955e70bca10d44672145d223b349ec0","sha256":"88fe760b62b90cd4555df7d4ee6ab591a501df4e6f841591599eefe623097cfb"},"downloads":-1,"filename":"ts_tokenizer-0.1.6.tar.gz","has_sig":false,"md5_digest":"f955e70bca10d44672145d223b349ec0","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10052176,"upload_time":"2024-10-17T12:45:53","upload_time_iso_8601":"2024-10-17T12:45:53.618392Z","url":"https://files.pythonhosted.org/packages/47/6c/5d6f3a7e877e1165a51fbee98b510e12bba7c71b730b026a9f35c100770e/ts_tokenizer-0.1.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.7":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.7/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.7","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"aa3791a19bbbd604d4a2cad12377f23d2f3a78aff4c2bba40b30e86da9131cce","md5":"a358b098c05da412550f0d353a0676b7","sha256":"6dbe97533ab10c8f61f18a139f97d5be7f2723eb6be149231d37de5bea9a74c4"},"downloads":-1,"filename":"ts_tokenizer-0.1.7-py3-none-any.whl","has_sig":false,"md5_digest":"a358b098c05da412550f0d353a0676b7","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9996093,"upload_time":"2024-10-17T12:54:36","upload_time_iso_8601":"2024-10-17T12:54:36.588108Z","url":"https://files.pythonhosted.org/packages/aa/37/91a19bbbd604d4a2cad12377f23d2f3a78aff4c2bba40b30e86da9131cce/ts_tokenizer-0.1.7-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9f259f289f6a39c9889e24729c474f254ef6610050f63a4e73433bd89febf42e","md5":"0ddf78244c00be2bcdb4949ddacf7ddf","sha256":"0e3f285e3c3a51ad18c9862da654d10a49a65df005ca07c2baa40e11b086f1f7"},"downloads":-1,"filename":"ts_tokenizer-0.1.7.tar.gz","has_sig":false,"md5_digest":"0ddf78244c00be2bcdb4949ddacf7ddf","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10052089,"upload_time":"2024-10-17T12:54:47","upload_time_iso_8601":"2024-10-17T12:54:47.533786Z","url":"https://files.pythonhosted.org/packages/9f/25/9f289f6a39c9889e24729c474f254ef6610050f63a4e73433bd89febf42e/ts_tokenizer-0.1.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.8":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.8/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.8","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"fa20256177d9e44abfdbcb87ac91a342daebaad26e04ebaaa04b51250a6cce8d","md5":"57e9751cb7da74c0a9064ad1b69bdfcd","sha256":"02728363743ebeb39b2de54b9ce0587f0f08a664776953879d12ba7a59d3d1cb"},"downloads":-1,"filename":"ts_tokenizer-0.1.8-py3-none-any.whl","has_sig":false,"md5_digest":"57e9751cb7da74c0a9064ad1b69bdfcd","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9996068,"upload_time":"2024-10-17T13:34:36","upload_time_iso_8601":"2024-10-17T13:34:36.614621Z","url":"https://files.pythonhosted.org/packages/fa/20/256177d9e44abfdbcb87ac91a342daebaad26e04ebaaa04b51250a6cce8d/ts_tokenizer-0.1.8-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"7cb2e8d0e00cd2e902e70a9895e5d86fd745732f5098f4760dfa9aabe8146cbd","md5":"08296a19e123138b135d7586caca50ff","sha256":"a60400e9498ccc9df998ae5ef4eb878d0ed72e014b5f723a048baa7f73fdd4f0"},"downloads":-1,"filename":"ts_tokenizer-0.1.8.tar.gz","has_sig":false,"md5_digest":"08296a19e123138b135d7586caca50ff","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10051920,"upload_time":"2024-10-17T13:34:56","upload_time_iso_8601":"2024-10-17T13:34:56.203494Z","url":"https://files.pythonhosted.org/packages/7c/b2/e8d0e00cd2e902e70a9895e5d86fd745732f5098f4760dfa9aabe8146cbd/ts_tokenizer-0.1.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.9":{"info":{"author":"Taner Sezer","author_email":"tanersezerr@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/tanerim/ts_tokenizer","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"ts-tokenizer","package_url":"https://pypi.org/project/ts-tokenizer/","platform":null,"project_url":"https://pypi.org/project/ts-tokenizer/","project_urls":{"Homepage":"https://github.com/tanerim/ts_tokenizer"},"provides_extra":null,"release_url":"https://pypi.org/project/ts-tokenizer/0.1.9/","requires_dist":["tqdm~=4.66.4"],"requires_python":">=3.9","summary":"TS Tokenizer is a rule-based tokenizerspecifically designed for processing Turkish text.It provides functionalities to split text into tokensfollowing the grammatical and linguistic rules of the Turkish language.","version":"0.1.9","yanked":false,"yanked_reason":null},"last_serial":25541518,"urls":[{"comment_text":"","digests":{"blake2b_256":"adf4f80db84f85c4a6843af6ae7ccf78a24ae35df24908220faec851c9f8bc7b","md5":"42283269af9b7b8563a62819cf442978","sha256":"be8e75bd336820449905b78f9a8e331d458ac950e7e7aa7e182380727f149f44"},"downloads":-1,"filename":"ts_tokenizer-0.1.9-py3-none-any.whl","has_sig":false,"md5_digest":"42283269af9b7b8563a62819cf442978","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":9995887,"upload_time":"2024-10-17T13:38:53","upload_time_iso_8601":"2024-10-17T13:38:53.084020Z","url":"https://files.pythonhosted.org/packages/ad/f4/f80db84f85c4a6843af6ae7ccf78a24ae35df24908220faec851c9f8bc7b/ts_tokenizer-0.1.9-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"15fc6f2c43ec0ac7c6c43a686f7b5a0e98c93dd45fb844f799cbd68e3ee0d4a3","md5":"2938b1dc7f40ffda10fc89fc7eaf4c27","sha256":"90e2e1c0e7843bf8d8ab724d634b2bc74158fd63f2c16a17d88c01ba2344f1b5"},"downloads":-1,"filename":"ts_tokenizer-0.1.9.tar.gz","has_sig":false,"md5_digest":"2938b1dc7f40ffda10fc89fc7eaf4c27","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":10051769,"upload_time":"2024-10-17T13:39:02","upload_time_iso_8601":"2024-10-17T13:39:02.835083Z","url":"https://files.pythonhosted.org/packages/15/fc/6f2c43ec0ac7c6c43a686f7b5a0e98c93dd45fb844f799cbd68e3ee0d4a3/ts_tokenizer-0.1.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}