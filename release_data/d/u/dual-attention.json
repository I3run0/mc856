{"0.0.1":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.1/","requires_dist":["torch","einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"931f227f4da757a8963ceb28d5f1bf2a1efc7f32b779af10aff75d078b820089","md5":"85479649a9805916ca852fbb2f02fae6","sha256":"fb1f66489323478124f0644605e23bc3fe2b2e05ad56d4b2b40f522ae72e77f9"},"downloads":-1,"filename":"dual_attention-0.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"85479649a9805916ca852fbb2f02fae6","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":37430,"upload_time":"2024-08-16T22:14:55","upload_time_iso_8601":"2024-08-16T22:14:55.199157Z","url":"https://files.pythonhosted.org/packages/93/1f/227f4da757a8963ceb28d5f1bf2a1efc7f32b779af10aff75d078b820089/dual_attention-0.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.2":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.2/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.2","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"dbd86a74e42651a306171abe70c7970b7d88304ead74e9d5ce0a6fc5d99f57c6","md5":"8e2f9437152ef75f8e1ebf858410662a","sha256":"522d22ce8fe81ee9d640ec0c05cd503f34f45dbcc0e0d9a619a4a22edf9570b5"},"downloads":-1,"filename":"dual_attention-0.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"8e2f9437152ef75f8e1ebf858410662a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":50753,"upload_time":"2024-08-18T02:15:34","upload_time_iso_8601":"2024-08-18T02:15:34.296028Z","url":"https://files.pythonhosted.org/packages/db/d8/6a74e42651a306171abe70c7970b7d88304ead74e9d5ce0a6fc5d99f57c6/dual_attention-0.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.3":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Documentation":"https://dual-attention-transformer.readthedocs.io/","Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.3/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.3","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"927b53bbb5ba0d4fe8f62944e8c207e194f80c5942bf70ba937eb7aad0fd91fd","md5":"f381251e8ccea09e18936e24af6f5deb","sha256":"e0fbe698d4d4f33a1b0e823bd3724a6c121637f74ee8d4696c59603a1bd7bc48"},"downloads":-1,"filename":"dual_attention-0.0.3-py3-none-any.whl","has_sig":false,"md5_digest":"f381251e8ccea09e18936e24af6f5deb","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":50855,"upload_time":"2024-08-18T02:38:46","upload_time_iso_8601":"2024-08-18T02:38:46.587044Z","url":"https://files.pythonhosted.org/packages/92/7b/53bbb5ba0d4fe8f62944e8c207e194f80c5942bf70ba937eb7aad0fd91fd/dual_attention-0.0.3-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.4":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Documentation":"https://dual-attention-transformer.readthedocs.io/","Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.4/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.4","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"98ca232da41ae13b020cb941589544504a905aafb75c6a890a076a649ca756eb","md5":"ac3f045e643bd57cf4cf72826fe974ce","sha256":"d58a87d58a110e650cf3d0ef30981322fc609319fb518889cd06a76c6dba536e"},"downloads":-1,"filename":"dual_attention-0.0.4-py3-none-any.whl","has_sig":false,"md5_digest":"ac3f045e643bd57cf4cf72826fe974ce","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":52468,"upload_time":"2024-08-19T00:00:40","upload_time_iso_8601":"2024-08-19T00:00:40.415267Z","url":"https://files.pythonhosted.org/packages/98/ca/232da41ae13b020cb941589544504a905aafb75c6a890a076a649ca756eb/dual_attention-0.0.4-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.5":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Documentation":"https://dual-attention-transformer.readthedocs.io/","Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.5/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.5","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"8021520ce3658ed58843dffb5abf73a358775d913689f8d75ca9c8d10db8f234","md5":"13e388e582364dd3c9396f44f5bf818d","sha256":"9a2583fd908474d67be658168ee6be916e2381157b658b898b205c3dc70b732d"},"downloads":-1,"filename":"dual_attention-0.0.5-py3-none-any.whl","has_sig":false,"md5_digest":"13e388e582364dd3c9396f44f5bf818d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":46332,"upload_time":"2024-09-11T21:47:39","upload_time_iso_8601":"2024-09-11T21:47:39.423263Z","url":"https://files.pythonhosted.org/packages/80/21/520ce3658ed58843dffb5abf73a358775d913689f8d75ca9c8d10db8f234/dual_attention-0.0.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"aaa0755b35f5722910ee22bb795568b230bfd08ce0de8af2facecf4609883442","md5":"c64734bf1632725465713ac69985d9d6","sha256":"589436ffd385fcb4a7b98383cd47edd5a09a1cebebee8376b1851429f92e8e8b"},"downloads":-1,"filename":"dual_attention-0.0.5.tar.gz","has_sig":false,"md5_digest":"c64734bf1632725465713ac69985d9d6","packagetype":"sdist","python_version":"source","requires_python":null,"size":40050,"upload_time":"2024-09-11T21:47:40","upload_time_iso_8601":"2024-09-11T21:47:40.870380Z","url":"https://files.pythonhosted.org/packages/aa/a0/755b35f5722910ee22bb795568b230bfd08ce0de8af2facecf4609883442/dual_attention-0.0.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.6":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Documentation":"https://dual-attention-transformer.readthedocs.io/","Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.6/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.6","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"094325ec7d6932c9a51581d9978936ad623efef87f495f2645ac55738ffea57e","md5":"e522df236734156e89c8e08ec436bc74","sha256":"53e0d45259668a31c9883d8f5f77c7a9d1d26166a63c11e1863471cff2b3ffff"},"downloads":-1,"filename":"dual_attention-0.0.6-py3-none-any.whl","has_sig":false,"md5_digest":"e522df236734156e89c8e08ec436bc74","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":46335,"upload_time":"2024-09-18T20:48:30","upload_time_iso_8601":"2024-09-18T20:48:30.459463Z","url":"https://files.pythonhosted.org/packages/09/43/25ec7d6932c9a51581d9978936ad623efef87f495f2645ac55738ffea57e/dual_attention-0.0.6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c27ff93996faf954435064fe373bd673fe78002da4a12e9dc7e6f95922ba0ed8","md5":"803bc95e759b8946e14c1f5027c8e845","sha256":"c884d2470a9487cb96e000b4573e4f6e6b3ee404a72678a83b311c3730e46bd8"},"downloads":-1,"filename":"dual_attention-0.0.6.tar.gz","has_sig":false,"md5_digest":"803bc95e759b8946e14c1f5027c8e845","packagetype":"sdist","python_version":"source","requires_python":null,"size":40067,"upload_time":"2024-09-18T20:48:32","upload_time_iso_8601":"2024-09-18T20:48:32.975010Z","url":"https://files.pythonhosted.org/packages/c2/7f/f93996faf954435064fe373bd673fe78002da4a12e9dc7e6f95922ba0ed8/dual_attention-0.0.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.7":{"info":{"author":"Awni Altabaa","author_email":"awni.altabaa@yale.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":null,"keywords":null,"license":"MIT","maintainer":null,"maintainer_email":null,"name":"dual-attention","package_url":"https://pypi.org/project/dual-attention/","platform":null,"project_url":"https://pypi.org/project/dual-attention/","project_urls":{"Documentation":"https://dual-attention-transformer.readthedocs.io/","Source":"https://github.com/Awni00/dual-attention","Tracker":"https://github.com/Awni00/dual-attention/issues"},"provides_extra":null,"release_url":"https://pypi.org/project/dual-attention/0.0.7/","requires_dist":["einops"],"requires_python":null,"summary":"Python package implementing the Dual Attention Transformer (DAT), as proposed by the paper \"Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\" by Awni Altabaa, John Lafferty.","version":"0.0.7","yanked":false,"yanked_reason":null},"last_serial":25177726,"urls":[{"comment_text":"","digests":{"blake2b_256":"1a38bb59d0689145dca08f000f296d9b2b775c6393d02ff51f4890618d8d2531","md5":"008a23f0b4fa5bd6ea25d0b923db235d","sha256":"8984d5afff45daa0e9799643b5e8223b08f893b214693ca0ee72cec3bdae7043"},"downloads":-1,"filename":"dual_attention-0.0.7-py3-none-any.whl","has_sig":false,"md5_digest":"008a23f0b4fa5bd6ea25d0b923db235d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":46764,"upload_time":"2024-09-24T20:25:48","upload_time_iso_8601":"2024-09-24T20:25:48.849172Z","url":"https://files.pythonhosted.org/packages/1a/38/bb59d0689145dca08f000f296d9b2b775c6393d02ff51f4890618d8d2531/dual_attention-0.0.7-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"85f2e60534e28d5a74975bcba312bd6ea1f2e5cb2f10f3acfd62505872fcac16","md5":"e82e63b3d0a648569a8a6d5c74ae1203","sha256":"f9eec6e0d1eb56758558e1043421fa5106062a276d68ec86e8ad8d2e01055a3d"},"downloads":-1,"filename":"dual_attention-0.0.7.tar.gz","has_sig":false,"md5_digest":"e82e63b3d0a648569a8a6d5c74ae1203","packagetype":"sdist","python_version":"source","requires_python":null,"size":40417,"upload_time":"2024-09-24T20:25:49","upload_time_iso_8601":"2024-09-24T20:25:49.998652Z","url":"https://files.pythonhosted.org/packages/85/f2/e60534e28d5a74975bcba312bd6ea1f2e5cb2f10f3acfd62505872fcac16/dual_attention-0.0.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}