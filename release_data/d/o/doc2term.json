{"0.1":{"info":{"author":"","author_email":"","bugtrack_url":null,"classifiers":["License :: OSI Approved :: Apache Software License","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/callforpapers-source/doc2term","keywords":"tokenizer,NLP,punctuation,standarization,duplicate-detection,text-processing,text-tokenizing,doc2term","license":"Apache License, Version 2.0","maintainer":"Saeed Dehqan","maintainer_email":"saeed.dehghan@owasp.org","name":"doc2term","package_url":"https://pypi.org/project/doc2term/","platform":"","project_url":"https://pypi.org/project/doc2term/","project_urls":{"Homepage":"https://github.com/callforpapers-source/doc2term"},"provides_extra":null,"release_url":"https://pypi.org/project/doc2term/0.1/","requires_dist":null,"requires_python":">=3.7","summary":"A fast NLP tokenizer that detects tokens and remove duplications and punctuations","version":"0.1","yanked":false,"yanked_reason":null},"last_serial":10367917,"urls":[{"comment_text":"","digests":{"blake2b_256":"ab40088c628f964db9636e947ab8fe5686f0b47865ff44edfd5af81f9638e68d","md5":"240a7a0fa821a2c959dbd1047a2589fb","sha256":"1b684765faecccd53c1c1509e09e476b4c5bbd2698520dbad2cac25bc540e654"},"downloads":-1,"filename":"doc2term-0.1.tar.gz","has_sig":false,"md5_digest":"240a7a0fa821a2c959dbd1047a2589fb","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":8298,"upload_time":"2021-05-16T01:11:40","upload_time_iso_8601":"2021-05-16T01:11:40.236526Z","url":"https://files.pythonhosted.org/packages/ab/40/088c628f964db9636e947ab8fe5686f0b47865ff44edfd5af81f9638e68d/doc2term-0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}