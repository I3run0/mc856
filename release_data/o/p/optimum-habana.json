{"1.0.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/habana","keywords":"transformers,quantization,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.0.1/","requires_dist":["transformers (==4.18.0)","optimum","datasets","tokenizers","torch","sentencepiece","scipy","pillow","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.","version":"1.0.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"cedd1fc0859462a585791fe842af7c324cb4745a45b34702f76ef265df2935d8","md5":"1eccf19f5eea20d6282fc551936db30a","sha256":"93626c83d2c45d5358a74755d1bbf10a38dffa8fb191a6cac3227cc009324459"},"downloads":-1,"filename":"optimum_habana-1.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"1eccf19f5eea20d6282fc551936db30a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":31825,"upload_time":"2022-04-26T10:15:32","upload_time_iso_8601":"2022-04-26T10:15:32.307260Z","url":"https://files.pythonhosted.org/packages/ce/dd/1fc0859462a585791fe842af7c324cb4745a45b34702f76ef265df2935d8/optimum_habana-1.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a8441bf236a7659ac24867ebcc2beeac8fc140249c1e7a104188646db813a43a","md5":"91b71a6ec5e0d99847e4517f213a645f","sha256":"0919380b0a2e2dc1544e6c30a5f9719415de5f9b885892bf116dbe05756ea313"},"downloads":-1,"filename":"optimum-habana-1.0.1.tar.gz","has_sig":false,"md5_digest":"91b71a6ec5e0d99847e4517f213a645f","packagetype":"sdist","python_version":"source","requires_python":null,"size":31092,"upload_time":"2022-04-26T10:15:33","upload_time_iso_8601":"2022-04-26T10:15:33.820426Z","url":"https://files.pythonhosted.org/packages/a8/44/1bf236a7659ac24867ebcc2beeac8fc140249c1e7a104188646db813a43a/optimum-habana-1.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.1.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.1.0/","requires_dist":["transformers (>=4.20.0)","optimum","datasets","tokenizers","torch","sentencepiece","scipy","pillow","dill (<0.3.5)","multiprocess (<0.70.13)","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.","version":"1.1.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"7d5d77b3cf3ea7b5be9a7594b5b15b47067a5ee84305d3ea971bd74006e93f38","md5":"ec07adab67effb90efa4175941a4b4c2","sha256":"c5be69af2c6466cc8ce4d3e86fcd1e19d4ba41b8183ce4ecc86f68a30bc6139f"},"downloads":-1,"filename":"optimum_habana-1.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"ec07adab67effb90efa4175941a4b4c2","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":58662,"upload_time":"2022-07-15T10:30:17","upload_time_iso_8601":"2022-07-15T10:30:17.809179Z","url":"https://files.pythonhosted.org/packages/7d/5d/77b3cf3ea7b5be9a7594b5b15b47067a5ee84305d3ea971bd74006e93f38/optimum_habana-1.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"dce6b29cdd5ef242239239fef927e56aec34757835dfda125eb075a255cdf9de","md5":"63a52d39cd55a372fe315a3f9105cafa","sha256":"42d2ab34c1b5fd2802881b896b1c4824df91e80ed22ba67e85b424a3449ea6ca"},"downloads":-1,"filename":"optimum-habana-1.1.0.tar.gz","has_sig":false,"md5_digest":"63a52d39cd55a372fe315a3f9105cafa","packagetype":"sdist","python_version":"source","requires_python":null,"size":53150,"upload_time":"2022-07-15T10:30:19","upload_time_iso_8601":"2022-07-15T10:30:19.530352Z","url":"https://files.pythonhosted.org/packages/dc/e6/b29cdd5ef242239239fef927e56aec34757835dfda125eb075a255cdf9de/optimum-habana-1.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.1.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.1.1/","requires_dist":["transformers (>=4.20.0)","optimum","datasets","tokenizers","torch","sentencepiece","scipy","pillow","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.","version":"1.1.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"32a7aeca1e7298b5b8a58722fc8d11bee648478440c793bf3770c38825a2a302","md5":"f41ad71f40e89ea405dabfc3613f9c6d","sha256":"7fc4bb7f7d1b8ec21b789811edc0f7d6041cae03077dc97e365aa8c3772f8a4c"},"downloads":-1,"filename":"optimum_habana-1.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"f41ad71f40e89ea405dabfc3613f9c6d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":61002,"upload_time":"2022-08-02T07:40:37","upload_time_iso_8601":"2022-08-02T07:40:37.539780Z","url":"https://files.pythonhosted.org/packages/32/a7/aeca1e7298b5b8a58722fc8d11bee648478440c793bf3770c38825a2a302/optimum_habana-1.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"0c8ceb1c44b48be1a8cdeceb18b0429fd1ebf6ceb95d6bbb6d4eab31448f9eba","md5":"2beca795d7d91dca1bbf4c0d275d0cdf","sha256":"dd5c93dad5b035bb08f52a0ae70c194be67a5db29cd6ae4c27100321e8dc6120"},"downloads":-1,"filename":"optimum-habana-1.1.1.tar.gz","has_sig":false,"md5_digest":"2beca795d7d91dca1bbf4c0d275d0cdf","packagetype":"sdist","python_version":"source","requires_python":null,"size":55175,"upload_time":"2022-08-02T07:40:39","upload_time_iso_8601":"2022-08-02T07:40:39.210110Z","url":"https://files.pythonhosted.org/packages/0c/8c/eb1c44b48be1a8cdeceb18b0429fd1ebf6ceb95d6bbb6d4eab31448f9eba/optimum-habana-1.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.1.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.1.2/","requires_dist":["transformers (>=4.20.0)","optimum","datasets","tokenizers","torch","sentencepiece","scipy","pillow","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.","version":"1.1.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"98a39b9227eea98d0094ec0c8e84d5473fa1e515f8cefab9248fec55a0260afa","md5":"6d6ace656ab7d84a156fb119d4b2f65b","sha256":"15b6bbb00162923f86fecc55587c1972e79ef8506493f0dae5238c08816dbf8e"},"downloads":-1,"filename":"optimum_habana-1.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"6d6ace656ab7d84a156fb119d4b2f65b","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":74065,"upload_time":"2022-08-12T07:46:52","upload_time_iso_8601":"2022-08-12T07:46:52.000070Z","url":"https://files.pythonhosted.org/packages/98/a3/9b9227eea98d0094ec0c8e84d5473fa1e515f8cefab9248fec55a0260afa/optimum_habana-1.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6c135b5e623688df5e63bce0b23e54484b138bb9f90c85c84edf2eb7608060b9","md5":"724fc14307fb73e033e959a5869d2909","sha256":"bfe02df164de0250c0e75336d9bb1f92e674e9724e03750c94f0d1e508d64dc8"},"downloads":-1,"filename":"optimum-habana-1.1.2.tar.gz","has_sig":false,"md5_digest":"724fc14307fb73e033e959a5869d2909","packagetype":"sdist","python_version":"source","requires_python":null,"size":67762,"upload_time":"2022-08-12T07:46:54","upload_time_iso_8601":"2022-08-12T07:46:54.095672Z","url":"https://files.pythonhosted.org/packages/6c/13/5b5e623688df5e63bce0b23e54484b138bb9f90c85c84edf2eb7608060b9/optimum-habana-1.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.10.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.10.0/","requires_dist":["transformers <4.35.0,>=4.34.0","optimum","torch","accelerate >=0.23.0","diffusers <0.24.0,>=0.18.0","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.10.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"7d9fb0c7a91b31f1d7a08e0cb6c963813f96e96bcbe5a2ce0ae8cad47000c9e7","md5":"6ffc844e13be25ebcda8303c99162257","sha256":"a2d56765ecdaa3bda7ac0d6fdeb64a897a67bf0790eea9b419d6c74f95556ed4"},"downloads":-1,"filename":"optimum_habana-1.10.0-py3-none-any.whl","has_sig":false,"md5_digest":"6ffc844e13be25ebcda8303c99162257","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":277325,"upload_time":"2024-01-30T21:41:06","upload_time_iso_8601":"2024-01-30T21:41:06.611716Z","url":"https://files.pythonhosted.org/packages/7d/9f/b0c7a91b31f1d7a08e0cb6c963813f96e96bcbe5a2ce0ae8cad47000c9e7/optimum_habana-1.10.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5d64dd52720fc7d2213a8456be7936d664a0e95cf12b15dfba1723602ae560b3","md5":"8d44f304cfc5e95fc33a1f82182296a6","sha256":"a02f596e0b9b3c1cfae596d12469942bea1a3e243d352ea074b47c031041fdb6"},"downloads":-1,"filename":"optimum-habana-1.10.0.tar.gz","has_sig":false,"md5_digest":"8d44f304cfc5e95fc33a1f82182296a6","packagetype":"sdist","python_version":"source","requires_python":null,"size":245027,"upload_time":"2024-01-30T21:41:08","upload_time_iso_8601":"2024-01-30T21:41:08.727614Z","url":"https://files.pythonhosted.org/packages/5d/64/dd52720fc7d2213a8456be7936d664a0e95cf12b15dfba1723602ae560b3/optimum-habana-1.10.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.10.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.10.2/","requires_dist":["transformers <4.38.0,>=4.37.0","optimum","torch","accelerate <0.28.0","diffusers <0.27.0,>=0.26.0","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.10.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"635fccc6f4f84e5a2ee09e1d516a70b2b0ac09521fcd6153a1f3590365b94d84","md5":"de645c23ae0acfee1613e92f2b3a3741","sha256":"28bc44a8ca66cb32a3ea46df38913c133889b8a75330fb7fe0c6b92f496b44d0"},"downloads":-1,"filename":"optimum_habana-1.10.2-py3-none-any.whl","has_sig":false,"md5_digest":"de645c23ae0acfee1613e92f2b3a3741","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":294819,"upload_time":"2024-02-18T01:57:53","upload_time_iso_8601":"2024-02-18T01:57:53.257741Z","url":"https://files.pythonhosted.org/packages/63/5f/ccc6f4f84e5a2ee09e1d516a70b2b0ac09521fcd6153a1f3590365b94d84/optimum_habana-1.10.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"79ce68e29db5a2582ade2cb77205a2031d2482e8db5be0692c6fb3c775c696d3","md5":"0177c9c837c49c9cb4180f148a5ff053","sha256":"328e76e608afeda81f27552cfba919a243b5b473936f9d14e97c72c9113406e8"},"downloads":-1,"filename":"optimum-habana-1.10.2.tar.gz","has_sig":false,"md5_digest":"0177c9c837c49c9cb4180f148a5ff053","packagetype":"sdist","python_version":"source","requires_python":null,"size":265022,"upload_time":"2024-02-18T01:57:55","upload_time_iso_8601":"2024-02-18T01:57:55.579321Z","url":"https://files.pythonhosted.org/packages/79/ce/68e29db5a2582ade2cb77205a2031d2482e8db5be0692c6fb3c775c696d3/optimum-habana-1.10.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.10.4":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.10.4/","requires_dist":["transformers <4.38.0,>=4.37.0","optimum","torch","accelerate <0.28.0","diffusers <0.27.0,>=0.26.0","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.10.4","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"391bfd69dc4c6a9b2272136bfcc743597e0522b4f201dafa6bcad064d22e186f","md5":"06dab19986104c8171424bfea1195510","sha256":"ef1fe4b2df975b052a75a280925dfb1fba29cdaf37aaca576c6056605866f5e3"},"downloads":-1,"filename":"optimum_habana-1.10.4-py3-none-any.whl","has_sig":false,"md5_digest":"06dab19986104c8171424bfea1195510","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":294825,"upload_time":"2024-02-23T03:20:17","upload_time_iso_8601":"2024-02-23T03:20:17.572591Z","url":"https://files.pythonhosted.org/packages/39/1b/fd69dc4c6a9b2272136bfcc743597e0522b4f201dafa6bcad064d22e186f/optimum_habana-1.10.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"3defcbcab8b0115ab241c5bc95bc2d4bbb851011950b843fba868f2a61f5c57c","md5":"526e7862e4fa8bdfbdf07318c1602ba1","sha256":"f1165bbbb834806d66c9beefb307bdc0497035ba5ed7575c5004bbbf1b2216e3"},"downloads":-1,"filename":"optimum-habana-1.10.4.tar.gz","has_sig":false,"md5_digest":"526e7862e4fa8bdfbdf07318c1602ba1","packagetype":"sdist","python_version":"source","requires_python":null,"size":265091,"upload_time":"2024-02-23T03:20:20","upload_time_iso_8601":"2024-02-23T03:20:20.791163Z","url":"https://files.pythonhosted.org/packages/3d/ef/cbcab8b0115ab241c5bc95bc2d4bbb851011950b843fba868f2a61f5c57c/optimum-habana-1.10.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.11.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.11.0/","requires_dist":["transformers<4.39.0,>=4.38.0","optimum","torch","accelerate<0.28.0","diffusers<0.27.0,>=0.26.0","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","pytest<8.0.0; extra == \"tests\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","safetensors; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.11.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"2865946142d26b9dda15626f24f9e9507ba8ec29597242fd6dbd0baa66fa8b0b","md5":"3fc3fd3bda1487b73a9b6d4687e8c0f9","sha256":"7e94b40d339ac42f754d6e0c9f7df513ebd1afd63307ec2f05e677adfbe347c4"},"downloads":-1,"filename":"optimum_habana-1.11.0-py3-none-any.whl","has_sig":false,"md5_digest":"3fc3fd3bda1487b73a9b6d4687e8c0f9","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":347159,"upload_time":"2024-04-04T13:34:04","upload_time_iso_8601":"2024-04-04T13:34:04.426166Z","url":"https://files.pythonhosted.org/packages/28/65/946142d26b9dda15626f24f9e9507ba8ec29597242fd6dbd0baa66fa8b0b/optimum_habana-1.11.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"db8d7afcba1e76990f248976f1927afa7c267e1691a26cc9f61a1dad2a1b8909","md5":"d681109ba6fa76811abbd8a6900c7385","sha256":"d7c7caea5daf17c0d520c52797d8a663432de6aa9ee8b35f6304d1646f225826"},"downloads":-1,"filename":"optimum-habana-1.11.0.tar.gz","has_sig":false,"md5_digest":"d681109ba6fa76811abbd8a6900c7385","packagetype":"sdist","python_version":"source","requires_python":null,"size":313174,"upload_time":"2024-04-04T13:34:06","upload_time_iso_8601":"2024-04-04T13:34:06.999065Z","url":"https://files.pythonhosted.org/packages/db/8d/7afcba1e76990f248976f1927afa7c267e1691a26cc9f61a1dad2a1b8909/optimum-habana-1.11.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.11.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.11.1/","requires_dist":["transformers<4.39.0,>=4.38.0","optimum","torch","accelerate<0.28.0","diffusers<0.27.0,>=0.26.0","pytest<8.0.0","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","safetensors; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.11.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"0f4c754da8454d30839e9143d803e41d31604bd8196a0c6ba49ee306195d1266","md5":"44aa7539f789d035a7ca334e0a8a3383","sha256":"2b4f2a801ebd7f235bde91f3e351893efcc586afef3b0c867eb3c4c8c4c26bec"},"downloads":-1,"filename":"optimum_habana-1.11.1-py3-none-any.whl","has_sig":false,"md5_digest":"44aa7539f789d035a7ca334e0a8a3383","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":347456,"upload_time":"2024-04-20T05:15:47","upload_time_iso_8601":"2024-04-20T05:15:47.144509Z","url":"https://files.pythonhosted.org/packages/0f/4c/754da8454d30839e9143d803e41d31604bd8196a0c6ba49ee306195d1266/optimum_habana-1.11.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"21a8f5fa85975b602ef568d9869f3fb594d282e722f6213e42f86e082f0fb963","md5":"2b4efea56a54e903d42f51d7669c4ca3","sha256":"c70dbd6d69ce11c8bb78292913304616600d051d94237c8310c3bdbf73ca7017"},"downloads":-1,"filename":"optimum_habana-1.11.1.tar.gz","has_sig":false,"md5_digest":"2b4efea56a54e903d42f51d7669c4ca3","packagetype":"sdist","python_version":"source","requires_python":null,"size":313449,"upload_time":"2024-04-20T05:15:49","upload_time_iso_8601":"2024-04-20T05:15:49.642202Z","url":"https://files.pythonhosted.org/packages/21/a8/f5fa85975b602ef568d9869f3fb594d282e722f6213e42f86e082f0fb963/optimum_habana-1.11.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.12.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.12.0/","requires_dist":["transformers<4.41.0,>=4.40.0","optimum","torch","accelerate<0.28.0","diffusers<0.27.0,>=0.26.0","huggingface-hub<0.23.0","datasets<2.20.0","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.12.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"ba1edb0bfdf8e9bd0f5b4db386d57d742c24e5753474ecd4fe564630498eb326","md5":"51657c606885f441d1f9755edb87f680","sha256":"a8f8c74802d110460abc15dae0be98d20c916f2adc22de23bc259c740cbeb19a"},"downloads":-1,"filename":"optimum_habana-1.12.0-py3-none-any.whl","has_sig":false,"md5_digest":"51657c606885f441d1f9755edb87f680","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":444247,"upload_time":"2024-06-22T18:24:47","upload_time_iso_8601":"2024-06-22T18:24:47.734012Z","url":"https://files.pythonhosted.org/packages/ba/1e/db0bfdf8e9bd0f5b4db386d57d742c24e5753474ecd4fe564630498eb326/optimum_habana-1.12.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"31fdda84ba58a2994cda119506dcafdc1668181f973e709bcd486259c5e49684","md5":"f88ca82112284d2fd1a5119321dfdb6b","sha256":"6e04bc5dc4223db1ef719b84f8d6ec3680b54cfbba8a741363b8e268a6b06a97"},"downloads":-1,"filename":"optimum-habana-1.12.0.tar.gz","has_sig":false,"md5_digest":"f88ca82112284d2fd1a5119321dfdb6b","packagetype":"sdist","python_version":"source","requires_python":null,"size":401142,"upload_time":"2024-06-22T18:24:50","upload_time_iso_8601":"2024-06-22T18:24:50.429623Z","url":"https://files.pythonhosted.org/packages/31/fd/da84ba58a2994cda119506dcafdc1668181f973e709bcd486259c5e49684/optimum-habana-1.12.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.12.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.12.1/","requires_dist":["transformers<4.41.0,>=4.40.0","optimum","torch","accelerate<0.28.0","diffusers<0.27.0,>=0.26.0","huggingface-hub<0.23.0","datasets<2.20.0","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.12.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"b89070cf1e0dd90dd85327acab46da3614dbcc4d7207c31c1c7d709a850981de","md5":"35e30d4f08c059338649a7b4a6a5db6f","sha256":"623121138e02c32bba622178f684eff67961fe7ed51bc47ba13db68f9b5b920b"},"downloads":-1,"filename":"optimum_habana-1.12.1-py3-none-any.whl","has_sig":false,"md5_digest":"35e30d4f08c059338649a7b4a6a5db6f","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":444237,"upload_time":"2024-07-11T13:41:58","upload_time_iso_8601":"2024-07-11T13:41:58.122397Z","url":"https://files.pythonhosted.org/packages/b8/90/70cf1e0dd90dd85327acab46da3614dbcc4d7207c31c1c7d709a850981de/optimum_habana-1.12.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"57e427edf509abd0b0c99e3ff1350316f3da26ae4c6caf19065391c9d4e8ed10","md5":"fa7a59dc30b5545e1f14b10e1b03f424","sha256":"e6ecd11f0da292b0f4d8a4f9a9f7a63207cbc10eb6a2b6039f6be96305b95418"},"downloads":-1,"filename":"optimum_habana-1.12.1.tar.gz","has_sig":false,"md5_digest":"fa7a59dc30b5545e1f14b10e1b03f424","packagetype":"sdist","python_version":"source","requires_python":null,"size":402065,"upload_time":"2024-07-11T13:42:00","upload_time_iso_8601":"2024-07-11T13:42:00.329286Z","url":"https://files.pythonhosted.org/packages/57/e4/27edf509abd0b0c99e3ff1350316f3da26ae4c6caf19065391c9d4e8ed10/optimum_habana-1.12.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.13.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.13.0/","requires_dist":["transformers<4.44.0,>=4.43.0","optimum","torch","accelerate<0.34.0,>=0.33.0","diffusers==0.29.2","huggingface-hub>=0.23.2","sentence-transformers[train]==3.0.1","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","timm; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\"","scipy; extra == \"tests\"","torchsde; extra == \"tests\"","peft; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.13.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"8adf325d8efa18e7b42ae9f09a34744e59ae5e215231bfc7601cab1e228f69f7","md5":"4b3aae93104299a5fe91ce5cb4fb39c9","sha256":"288102caf2174db686d1a99cadeef1d251e419be6a33503dc2dce29211bf7f3b"},"downloads":-1,"filename":"optimum_habana-1.13.0-py3-none-any.whl","has_sig":false,"md5_digest":"4b3aae93104299a5fe91ce5cb4fb39c9","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":564302,"upload_time":"2024-08-16T12:58:24","upload_time_iso_8601":"2024-08-16T12:58:24.323159Z","url":"https://files.pythonhosted.org/packages/8a/df/325d8efa18e7b42ae9f09a34744e59ae5e215231bfc7601cab1e228f69f7/optimum_habana-1.13.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f0f32f9db0dadf82175b129c086c66e469a0324d622addd77428ebb36b52959b","md5":"f05d90121c8a3992e4414cb80ba9fb7f","sha256":"39162fd91e3aa2a4b607a68213e2ee5711d92d97aafb23d12b8e562fa94b2861"},"downloads":-1,"filename":"optimum_habana-1.13.0.tar.gz","has_sig":false,"md5_digest":"f05d90121c8a3992e4414cb80ba9fb7f","packagetype":"sdist","python_version":"source","requires_python":null,"size":527521,"upload_time":"2024-08-16T12:58:26","upload_time_iso_8601":"2024-08-16T12:58:26.294386Z","url":"https://files.pythonhosted.org/packages/f0/f3/2f9db0dadf82175b129c086c66e469a0324d622addd77428ebb36b52959b/optimum_habana-1.13.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.13.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.13.1/","requires_dist":["transformers<4.44.0,>=4.43.0","optimum","torch","accelerate<0.34.0,>=0.33.0","diffusers==0.29.2","huggingface-hub>=0.23.2","sentence-transformers[train]==3.0.1","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","timm; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\"","scipy; extra == \"tests\"","torchsde; extra == \"tests\"","peft; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.13.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"283d3bea6b17dcd4bede20b76144dc766683cdb6552343058501eb8a62f2a2f1","md5":"6b85ae4f58d336ccc0632bf47f05c56b","sha256":"4b0ed3cf494e2f42cbda6df229c3aaf573e19b37035d19d6d9d2fa02a4f2af00"},"downloads":-1,"filename":"optimum_habana-1.13.1-py3-none-any.whl","has_sig":false,"md5_digest":"6b85ae4f58d336ccc0632bf47f05c56b","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":565695,"upload_time":"2024-08-25T13:29:27","upload_time_iso_8601":"2024-08-25T13:29:27.498200Z","url":"https://files.pythonhosted.org/packages/28/3d/3bea6b17dcd4bede20b76144dc766683cdb6552343058501eb8a62f2a2f1/optimum_habana-1.13.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9497d0b7f893a85e3bbc4e927321c645b90d9f3982d0d169b602db09a901cda2","md5":"df0161743829e62adc0f12e73f3dc069","sha256":"c85ce2e02464d40cb81c7596972d3203cd60c00ea14909363727e1033649c6ef"},"downloads":-1,"filename":"optimum_habana-1.13.1.tar.gz","has_sig":false,"md5_digest":"df0161743829e62adc0f12e73f3dc069","packagetype":"sdist","python_version":"source","requires_python":null,"size":528909,"upload_time":"2024-08-25T13:29:29","upload_time_iso_8601":"2024-08-25T13:29:29.859513Z","url":"https://files.pythonhosted.org/packages/94/97/d0b7f893a85e3bbc4e927321c645b90d9f3982d0d169b602db09a901cda2/optimum_habana-1.13.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.13.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.13.2/","requires_dist":["transformers<4.44.0,>=4.43.0","optimum","torch","accelerate<0.34.0,>=0.33.0","diffusers==0.29.2","huggingface-hub>=0.23.2","sentence-transformers[train]==3.0.1","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","timm; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\"","scipy; extra == \"tests\"","torchsde; extra == \"tests\"","peft; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.13.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"8d9e0d1e29d5f3782064a87c65a0286eeafa116cef0ed87600663bdc3c8e883f","md5":"03665f11f9d30784de931aa594abd5b4","sha256":"fbec406db58e6e24b9d4cfdbc72ac88423eeef3edaf81645dbd502141a29d052"},"downloads":-1,"filename":"optimum_habana-1.13.2-py3-none-any.whl","has_sig":false,"md5_digest":"03665f11f9d30784de931aa594abd5b4","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":565818,"upload_time":"2024-09-06T20:11:05","upload_time_iso_8601":"2024-09-06T20:11:05.023794Z","url":"https://files.pythonhosted.org/packages/8d/9e/0d1e29d5f3782064a87c65a0286eeafa116cef0ed87600663bdc3c8e883f/optimum_habana-1.13.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f4485fbe9ef958258adb3b360973eec0057477c3c095a8128baf91307ce65d73","md5":"a508f839bfa61ced036385d0f4e2c7b6","sha256":"6e88c93860e33c8186f6dbd55a61ed287ed3cba241f289fcf4a4001d6e2a41f0"},"downloads":-1,"filename":"optimum_habana-1.13.2.tar.gz","has_sig":false,"md5_digest":"a508f839bfa61ced036385d0f4e2c7b6","packagetype":"sdist","python_version":"source","requires_python":null,"size":529040,"upload_time":"2024-09-06T20:11:07","upload_time_iso_8601":"2024-09-06T20:11:07.029120Z","url":"https://files.pythonhosted.org/packages/f4/48/5fbe9ef958258adb3b360973eec0057477c3c095a8128baf91307ce65d73/optimum_habana-1.13.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.14.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.14.0/","requires_dist":["transformers<4.46.0,>=4.45.2","optimum","torch","accelerate<0.34.0,>=0.33.0","diffusers==0.29.2","huggingface-hub>=0.24.7","sentence-transformers[train]==3.0.1","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","timm; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\"","scipy; extra == \"tests\"","torchsde; extra == \"tests\"","peft; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.14.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"7512059033cc571b860b8382e44bb7577b073cbbfeedd1fa2fc841d8caff3d72","md5":"56017149021ee102e3051beb4051b44f","sha256":"9b94ea8a14ef85c21f68b350295d29d28015a48712f9b0691ff589b112b92c1f"},"downloads":-1,"filename":"optimum_habana-1.14.0-py3-none-any.whl","has_sig":false,"md5_digest":"56017149021ee102e3051beb4051b44f","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":656044,"upload_time":"2024-10-22T14:41:30","upload_time_iso_8601":"2024-10-22T14:41:30.041540Z","url":"https://files.pythonhosted.org/packages/75/12/059033cc571b860b8382e44bb7577b073cbbfeedd1fa2fc841d8caff3d72/optimum_habana-1.14.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bf1a8bf57fa7243eded78068a890a605f6e305bba3cc315464812e25f022b9ce","md5":"b4503fb51e886bec38ee8470d969ef10","sha256":"8ede74eed1b803af9bc9d3518630ef252f21cb414e7cb451614d68328f186795"},"downloads":-1,"filename":"optimum_habana-1.14.0.tar.gz","has_sig":false,"md5_digest":"b4503fb51e886bec38ee8470d969ef10","packagetype":"sdist","python_version":"source","requires_python":null,"size":609001,"upload_time":"2024-10-22T14:41:32","upload_time_iso_8601":"2024-10-22T14:41:32.068324Z","url":"https://files.pythonhosted.org/packages/bf/1a/8bf57fa7243eded78068a890a605f6e305bba3cc315464812e25f022b9ce/optimum_habana-1.14.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.14.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers, diffusers, mixed-precision training, fine-tuning, gaudi, hpu","license":"Apache","maintainer":null,"maintainer_email":null,"name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":["quality","tests"],"release_url":"https://pypi.org/project/optimum-habana/1.14.1/","requires_dist":["transformers<4.46.0,>=4.45.2","optimum","torch","accelerate<0.34.0,>=0.33.0","diffusers==0.29.2","huggingface-hub>=0.24.7","sentence-transformers[train]==3.0.1","ruff; extra == \"quality\"","hf-doc-builder; extra == \"quality\"","psutil; extra == \"tests\"","parameterized; extra == \"tests\"","GitPython; extra == \"tests\"","optuna; extra == \"tests\"","sentencepiece; extra == \"tests\"","datasets; extra == \"tests\"","timm; extra == \"tests\"","safetensors; extra == \"tests\"","pytest<8.0.0; extra == \"tests\"","scipy; extra == \"tests\"","torchsde; extra == \"tests\"","peft; extra == \"tests\""],"requires_python":null,"summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.14.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"61c749ad525999fe58123987b8c4b49dc9480ce0e57938671aec77fb99a66883","md5":"df869a07b4530a517fa813e49d93e525","sha256":"557dd06a5fa2597b1487384a354c3a1618390c35e6b088f5d7a80c0611fe2bfd"},"downloads":-1,"filename":"optimum_habana-1.14.1-py3-none-any.whl","has_sig":false,"md5_digest":"df869a07b4530a517fa813e49d93e525","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":656047,"upload_time":"2024-10-29T17:09:38","upload_time_iso_8601":"2024-10-29T17:09:38.865499Z","url":"https://files.pythonhosted.org/packages/61/c7/49ad525999fe58123987b8c4b49dc9480ce0e57938671aec77fb99a66883/optimum_habana-1.14.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ce492faf3a097d62d792d4504a6691a149bbb4a02afb09f18c55c96b9e97ba5a","md5":"52c89aca30f526e57ea1a468e818e9d9","sha256":"627626860b82452b75f73087df3e14004278ed30dacf767c100f1324675c8120"},"downloads":-1,"filename":"optimum_habana-1.14.1.tar.gz","has_sig":false,"md5_digest":"52c89aca30f526e57ea1a468e818e9d9","packagetype":"sdist","python_version":"source","requires_python":null,"size":609104,"upload_time":"2024-10-29T17:09:40","upload_time_iso_8601":"2024-10-29T17:09:40.705073Z","url":"https://files.pythonhosted.org/packages/ce/49/2faf3a097d62d792d4504a6691a149bbb4a02afb09f18c55c96b9e97ba5a/optimum_habana-1.14.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.2.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.2.0/","requires_dist":null,"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.2.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"b5a010401f2bc7f2e1c9e033bfc03d773b740fd70c7037e9439ad1041ac47392","md5":"2206be4c2b52b916f8a16368f178508e","sha256":"654bc062e8ddfec79c848199d37b24711710806bf47ed78e0794a3e4700b199b"},"downloads":-1,"filename":"optimum-habana-1.2.0.tar.gz","has_sig":false,"md5_digest":"2206be4c2b52b916f8a16368f178508e","packagetype":"sdist","python_version":"source","requires_python":null,"size":62509,"upload_time":"2022-09-12T08:44:19","upload_time_iso_8601":"2022-09-12T08:44:19.501730Z","url":"https://files.pythonhosted.org/packages/b5/a0/10401f2bc7f2e1c9e033bfc03d773b740fd70c7037e9439ad1041ac47392/optimum-habana-1.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.2.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.2.1/","requires_dist":["transformers (<4.22,>=4.21)","optimum","datasets","tokenizers","torch","sentencepiece","scipy","pillow","accelerate","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.2.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"263b477dd3a9cec83d3967e86cec4816537d1781dd8608b8f83de7dd70ced950","md5":"b100704cdfac8784f1b2f8517ddc65fa","sha256":"2ad642b710b3365b4c19ef09e8c0495ae20d13be35bd6726b125f753a72d9df9"},"downloads":-1,"filename":"optimum_habana-1.2.1-py3-none-any.whl","has_sig":false,"md5_digest":"b100704cdfac8784f1b2f8517ddc65fa","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":68828,"upload_time":"2022-09-12T08:56:24","upload_time_iso_8601":"2022-09-12T08:56:24.902378Z","url":"https://files.pythonhosted.org/packages/26/3b/477dd3a9cec83d3967e86cec4816537d1781dd8608b8f83de7dd70ced950/optimum_habana-1.2.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"55d941502eb1c4aa28046895872bad7dc8b84a4ac9c50bf03f1b6f7f5a005361","md5":"e1f50643fa153ddf1cc3e0a05946d235","sha256":"7db41a131e80297c49a928b22ee37a1179356c2f478162bba08eeed34124c780"},"downloads":-1,"filename":"optimum-habana-1.2.1.tar.gz","has_sig":false,"md5_digest":"e1f50643fa153ddf1cc3e0a05946d235","packagetype":"sdist","python_version":"source","requires_python":null,"size":62369,"upload_time":"2022-09-12T08:56:26","upload_time_iso_8601":"2022-09-12T08:56:26.845178Z","url":"https://files.pythonhosted.org/packages/55/d9/41502eb1c4aa28046895872bad7dc8b84a4ac9c50bf03f1b6f7f5a005361/optimum-habana-1.2.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.2.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.2.2/","requires_dist":["transformers (<4.23,>=4.22)","optimum","torch","accelerate","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.2.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"87991a7d714f2f3ca038c3bef62a72be832862110aba5332da13b664007c6299","md5":"32dd234cf205982fcd64e2e3242f1fad","sha256":"f8eaa762b4e6ead2e3c51319855d5731afb213091c8d082a3e4b3d4cd31a64eb"},"downloads":-1,"filename":"optimum_habana-1.2.2-py3-none-any.whl","has_sig":false,"md5_digest":"32dd234cf205982fcd64e2e3242f1fad","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":69440,"upload_time":"2022-10-02T20:09:19","upload_time_iso_8601":"2022-10-02T20:09:19.986764Z","url":"https://files.pythonhosted.org/packages/87/99/1a7d714f2f3ca038c3bef62a72be832862110aba5332da13b664007c6299/optimum_habana-1.2.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"394205475557fea98b96b618e1a740e04ef787a5495c91264f7dfb62dc4122d6","md5":"fc77357e38f66836785d70d02ea9d512","sha256":"736ea38b461095ccf3cf1ca9a76b5aab619a3ea11ad1517631ef76a1d93d0016"},"downloads":-1,"filename":"optimum-habana-1.2.2.tar.gz","has_sig":false,"md5_digest":"fc77357e38f66836785d70d02ea9d512","packagetype":"sdist","python_version":"source","requires_python":null,"size":62911,"upload_time":"2022-10-02T20:09:21","upload_time_iso_8601":"2022-10-02T20:09:21.968313Z","url":"https://files.pythonhosted.org/packages/39/42/05475557fea98b96b618e1a740e04ef787a5495c91264f7dfb62dc4122d6/optimum-habana-1.2.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.2.3":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.2.3/","requires_dist":["transformers (<4.24,>=4.23)","optimum","torch","accelerate","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.2.3","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"56e72a977547959256bcfa9986d1cccb781585c0c97e6e8b0a24b43a8e26adad","md5":"1ba15fe0b24d2545eff79da309b73d53","sha256":"2b579a067f5ab20dceba6cfeba7d250e94e945defe0c63a71f83f813653defd4"},"downloads":-1,"filename":"optimum_habana-1.2.3-py3-none-any.whl","has_sig":false,"md5_digest":"1ba15fe0b24d2545eff79da309b73d53","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":69973,"upload_time":"2022-10-13T08:04:38","upload_time_iso_8601":"2022-10-13T08:04:38.135456Z","url":"https://files.pythonhosted.org/packages/56/e7/2a977547959256bcfa9986d1cccb781585c0c97e6e8b0a24b43a8e26adad/optimum_habana-1.2.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6d0c6e2ae53fe7280a4fa4009abbabe97c1b8e59dd4f5f2ac22e5a801b017188","md5":"61d4b5432cf9e693c44188e81d296934","sha256":"b4ff6a89cbd58939b0db9b365e1aad58cb6a914963e83413231c16ca8a9afa99"},"downloads":-1,"filename":"optimum-habana-1.2.3.tar.gz","has_sig":false,"md5_digest":"61d4b5432cf9e693c44188e81d296934","packagetype":"sdist","python_version":"source","requires_python":null,"size":63471,"upload_time":"2022-10-13T08:04:39","upload_time_iso_8601":"2022-10-13T08:04:39.874180Z","url":"https://files.pythonhosted.org/packages/6d/0c/6e2ae53fe7280a4fa4009abbabe97c1b8e59dd4f5f2ac22e5a801b017188/optimum-habana-1.2.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.3.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.3.0/","requires_dist":["transformers (>=4.23.0)","optimum","torch","accelerate","diffusers (>=0.9.0)","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.3.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"a572d7d170963778697fcbbb1c42f5bfce702110ae6ed19fa757c3c7c4b9d838","md5":"e88fddace6169c7b00cb1aa0ed8322ff","sha256":"5dfac7a287f305553266ff443bfd03d21bcd6a69ee9f7f31c52d3fb7b4c63283"},"downloads":-1,"filename":"optimum_habana-1.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"e88fddace6169c7b00cb1aa0ed8322ff","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":95420,"upload_time":"2022-12-01T10:31:17","upload_time_iso_8601":"2022-12-01T10:31:17.635228Z","url":"https://files.pythonhosted.org/packages/a5/72/d7d170963778697fcbbb1c42f5bfce702110ae6ed19fa757c3c7c4b9d838/optimum_habana-1.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"3261d56a2dd3716edb439b04edf5ec0fdcdf478eedde2332493c7474769468f7","md5":"8c1022b33e91a59440654d604d7315b6","sha256":"7c6fb67f065c290f86c106b61eda2acecc0222f2694037bbb6fa85cb7c23a7e7"},"downloads":-1,"filename":"optimum-habana-1.3.0.tar.gz","has_sig":false,"md5_digest":"8c1022b33e91a59440654d604d7315b6","packagetype":"sdist","python_version":"source","requires_python":null,"size":84571,"upload_time":"2022-12-01T10:31:20","upload_time_iso_8601":"2022-12-01T10:31:20.494232Z","url":"https://files.pythonhosted.org/packages/32/61/d56a2dd3716edb439b04edf5ec0fdcdf478eedde2332493c7474769468f7/optimum-habana-1.3.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.3.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.3.1/","requires_dist":["transformers (>=4.25.0)","optimum","torch","accelerate","diffusers (>=0.9.0)","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.3.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"a6f3a2537e50d87fcf7da301b906b731fee947ee036c17c6f200045c42735135","md5":"8de722dcb933d4bc9457d2dcedc802a2","sha256":"c6ce407cc7496dca3ec5723686d636fefd4935410c7db54cdaa7486e19304662"},"downloads":-1,"filename":"optimum_habana-1.3.1-py3-none-any.whl","has_sig":false,"md5_digest":"8de722dcb933d4bc9457d2dcedc802a2","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":95934,"upload_time":"2022-12-02T10:37:10","upload_time_iso_8601":"2022-12-02T10:37:10.883204Z","url":"https://files.pythonhosted.org/packages/a6/f3/a2537e50d87fcf7da301b906b731fee947ee036c17c6f200045c42735135/optimum_habana-1.3.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a104edb3f00b840bc1a08b298c8e83a3ed3bc259417c988a499658307a63b86a","md5":"8568db34aa8b37d2d4a62e2e5d4fe9f3","sha256":"47e766ca39a2b6421402d957350f2c18161006e0bad407ff453b42b9b5bf30fe"},"downloads":-1,"filename":"optimum-habana-1.3.1.tar.gz","has_sig":false,"md5_digest":"8568db34aa8b37d2d4a62e2e5d4fe9f3","packagetype":"sdist","python_version":"source","requires_python":null,"size":84937,"upload_time":"2022-12-02T10:37:12","upload_time_iso_8601":"2022-12-02T10:37:12.925340Z","url":"https://files.pythonhosted.org/packages/a1/04/edb3f00b840bc1a08b298c8e83a3ed3bc259417c988a499658307a63b86a/optimum-habana-1.3.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.3.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.3.2/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.9.0)","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.3.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"c89f3b628cb71d339e347cf33cc535f41cc84741200ac38c24e8aa14175b13eb","md5":"cfcc7c7d189901d5aaf66a3a6d3b4f72","sha256":"47df1d0becc727f0563136459224deeeb328c3456ec1a980e58c8298d2278000"},"downloads":-1,"filename":"optimum_habana-1.3.2-py3-none-any.whl","has_sig":false,"md5_digest":"cfcc7c7d189901d5aaf66a3a6d3b4f72","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":92924,"upload_time":"2023-01-24T23:08:52","upload_time_iso_8601":"2023-01-24T23:08:52.457820Z","url":"https://files.pythonhosted.org/packages/c8/9f/3b628cb71d339e347cf33cc535f41cc84741200ac38c24e8aa14175b13eb/optimum_habana-1.3.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"87e3ec73eccfd6485c1ede65df26e2ae1d16619b62c50f55a35a85b20f4fe54d","md5":"77f925190b0c31f35209e6b0fcd260b8","sha256":"b1750a36e8f9f01b2310acd463343c7ea72ecc4c222eea16ad9efb52509898d2"},"downloads":-1,"filename":"optimum-habana-1.3.2.tar.gz","has_sig":false,"md5_digest":"77f925190b0c31f35209e6b0fcd260b8","packagetype":"sdist","python_version":"source","requires_python":null,"size":81955,"upload_time":"2023-01-24T23:08:54","upload_time_iso_8601":"2023-01-24T23:08:54.119620Z","url":"https://files.pythonhosted.org/packages/87/e3/ec73eccfd6485c1ede65df26e2ae1d16619b62c50f55a35a85b20f4fe54d/optimum-habana-1.3.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.3.3":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.3.3/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","isort ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.","version":"1.3.3","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"3abc4e7586fee24253adfd7b8c34cbc2284edd8dc22b063d6e28cec46010bb10","md5":"769757c364e5fb7876b6c58aef0928f5","sha256":"ba1431ed2bc983726942c277c245e98a8a751be4350b87926356ac0f840c385c"},"downloads":-1,"filename":"optimum_habana-1.3.3-py3-none-any.whl","has_sig":false,"md5_digest":"769757c364e5fb7876b6c58aef0928f5","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":93822,"upload_time":"2023-01-30T20:01:37","upload_time_iso_8601":"2023-01-30T20:01:37.469761Z","url":"https://files.pythonhosted.org/packages/3a/bc/4e7586fee24253adfd7b8c34cbc2284edd8dc22b063d6e28cec46010bb10/optimum_habana-1.3.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"cb36e32bd78dd592153af6d0ab2f7841813a80228e5b64310bf4923b0c87e13c","md5":"edd5538353bf7f6565957ddda63f1a02","sha256":"67b43309dec248b7e6d43ea2d13e95b74df874c025dd88148363643205a37f5a"},"downloads":-1,"filename":"optimum-habana-1.3.3.tar.gz","has_sig":false,"md5_digest":"edd5538353bf7f6565957ddda63f1a02","packagetype":"sdist","python_version":"source","requires_python":null,"size":82854,"upload_time":"2023-01-30T20:01:39","upload_time_iso_8601":"2023-01-30T20:01:39.545967Z","url":"https://files.pythonhosted.org/packages/cb/36/e32bd78dd592153af6d0ab2f7841813a80228e5b64310bf4923b0c87e13c/optimum-habana-1.3.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.4.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.4.0/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.4.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"1f0ad101e2c652c452c34c933279b9204eae68f94bd36bb3cf10ebbd00b8af06","md5":"837436878f651a013782f16a495a49bf","sha256":"f9a8f07be21fa2deb2128db2d48988d172910e6a2b38464001be0959cc92c87a"},"downloads":-1,"filename":"optimum_habana-1.4.0-py3-none-any.whl","has_sig":false,"md5_digest":"837436878f651a013782f16a495a49bf","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":95898,"upload_time":"2023-02-12T23:04:15","upload_time_iso_8601":"2023-02-12T23:04:15.466292Z","url":"https://files.pythonhosted.org/packages/1f/0a/d101e2c652c452c34c933279b9204eae68f94bd36bb3cf10ebbd00b8af06/optimum_habana-1.4.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"cc490d833845c07e0a56b46e1fd2801e66fb49c55bdfd1db64135d106567762c","md5":"33a9e53f0fee92726a58a38b7314862e","sha256":"491fd16b6d74241c7c3bdb83501b364c0bf340138a951a4994605042fbb55e0f"},"downloads":-1,"filename":"optimum-habana-1.4.0.tar.gz","has_sig":false,"md5_digest":"33a9e53f0fee92726a58a38b7314862e","packagetype":"sdist","python_version":"source","requires_python":null,"size":84217,"upload_time":"2023-02-12T23:04:17","upload_time_iso_8601":"2023-02-12T23:04:17.277307Z","url":"https://files.pythonhosted.org/packages/cc/49/0d833845c07e0a56b46e1fd2801e66fb49c55bdfd1db64135d106567762c/optimum-habana-1.4.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.4.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.4.1/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.4.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"3a0a21f52c99c6e3ec8966d1bf113e25f3b0e2cde36370149f1d2677b1bbb50b","md5":"8d3a79e0ba8d68d77341967a8ce726b4","sha256":"68af05cb26714d0fcf7d2a304936720ab3f8c737c39d65877c91b0918712015e"},"downloads":-1,"filename":"optimum_habana-1.4.1-py3-none-any.whl","has_sig":false,"md5_digest":"8d3a79e0ba8d68d77341967a8ce726b4","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":96021,"upload_time":"2023-02-13T13:50:47","upload_time_iso_8601":"2023-02-13T13:50:47.573188Z","url":"https://files.pythonhosted.org/packages/3a/0a/21f52c99c6e3ec8966d1bf113e25f3b0e2cde36370149f1d2677b1bbb50b/optimum_habana-1.4.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"20a256c09fc58dbcb5520211f9bfc0c084a95c0ef8f3968b40aecc3e16d40dbb","md5":"1cd7be53bce168a5acd3e7dc5f817d3e","sha256":"edd0018eafd50ae00520af6d83314eaf0fed0cf31d3da7bb6354e2ed3781acdd"},"downloads":-1,"filename":"optimum-habana-1.4.1.tar.gz","has_sig":false,"md5_digest":"1cd7be53bce168a5acd3e7dc5f817d3e","packagetype":"sdist","python_version":"source","requires_python":null,"size":84310,"upload_time":"2023-02-13T13:50:50","upload_time_iso_8601":"2023-02-13T13:50:50.121113Z","url":"https://files.pythonhosted.org/packages/20/a2/56c09fc58dbcb5520211f9bfc0c084a95c0ef8f3968b40aecc3e16d40dbb/optimum-habana-1.4.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.4.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.4.2/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.4.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"ecca28d2d1bb24a3cc6c3e81a71a9b015f4dd0119eecd8fa7dd07d0227dafb46","md5":"91d0dd924b32f6932f24e316c55abc24","sha256":"95d41c3ff58cba4b0fc3693c36ee7358759a5e976f0e017b1a2825684772d080"},"downloads":-1,"filename":"optimum_habana-1.4.2-py3-none-any.whl","has_sig":false,"md5_digest":"91d0dd924b32f6932f24e316c55abc24","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":99643,"upload_time":"2023-03-16T14:07:41","upload_time_iso_8601":"2023-03-16T14:07:41.551937Z","url":"https://files.pythonhosted.org/packages/ec/ca/28d2d1bb24a3cc6c3e81a71a9b015f4dd0119eecd8fa7dd07d0227dafb46/optimum_habana-1.4.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6d9b907b3da5bff5be5c53e9be876032a53ccd02e3ccf505a0bc6e409ece485a","md5":"b86df53783a920323826ec3bfed90e8c","sha256":"1f08f902bcc603405a77d7a1f0671af6b50bc818c0df1c50f9eb1ca7ce6f7af4"},"downloads":-1,"filename":"optimum-habana-1.4.2.tar.gz","has_sig":false,"md5_digest":"b86df53783a920323826ec3bfed90e8c","packagetype":"sdist","python_version":"source","requires_python":null,"size":87293,"upload_time":"2023-03-16T14:07:43","upload_time_iso_8601":"2023-03-16T14:07:43.906570Z","url":"https://files.pythonhosted.org/packages/6d/9b/907b3da5bff5be5c53e9be876032a53ccd02e3ccf505a0bc6e409ece485a/optimum-habana-1.4.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.5.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.5.0/","requires_dist":["transformers (>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.5.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"4e1e337fca8ccaa8811ef1687d8ffffde3739cee86d6858b75465e20b0ba223f","md5":"3b02d4ca66dd08fe39d447dfba676ce9","sha256":"c68c477bed7253d96927f7efb6d9a1b66cd9701fcad4988d42836a380a0e00d1"},"downloads":-1,"filename":"optimum_habana-1.5.0-py3-none-any.whl","has_sig":false,"md5_digest":"3b02d4ca66dd08fe39d447dfba676ce9","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":112263,"upload_time":"2023-04-17T17:46:19","upload_time_iso_8601":"2023-04-17T17:46:19.393123Z","url":"https://files.pythonhosted.org/packages/4e/1e/337fca8ccaa8811ef1687d8ffffde3739cee86d6858b75465e20b0ba223f/optimum_habana-1.5.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1d30b6196654cf144718656cee1546048f8214db8c33b55105badee00015dd3a","md5":"bfce9adde23f141db28eab58c9290df6","sha256":"2ac5c0c514e445f28589de75560f8b5334ceeab01c59393040704b5d6a800400"},"downloads":-1,"filename":"optimum-habana-1.5.0.tar.gz","has_sig":false,"md5_digest":"bfce9adde23f141db28eab58c9290df6","packagetype":"sdist","python_version":"source","requires_python":null,"size":122705,"upload_time":"2023-04-17T17:46:21","upload_time_iso_8601":"2023-04-17T17:46:21.507891Z","url":"https://files.pythonhosted.org/packages/1d/30/b6196654cf144718656cee1546048f8214db8c33b55105badee00015dd3a/optimum-habana-1.5.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.5.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.5.1/","requires_dist":["transformers (<4.29.0,>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.5.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"3b1dee94d5b0836dfe96afbd2e48892de5be226470d462aac3c28206a316610d","md5":"4d5983b3518b59854714c1b9327f958f","sha256":"0767dd84ffb9826a5102c9237422046eb897b1a22c993627114ad84092f7784b"},"downloads":-1,"filename":"optimum_habana-1.5.1-py3-none-any.whl","has_sig":false,"md5_digest":"4d5983b3518b59854714c1b9327f958f","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":112267,"upload_time":"2023-05-11T09:07:31","upload_time_iso_8601":"2023-05-11T09:07:31.532407Z","url":"https://files.pythonhosted.org/packages/3b/1d/ee94d5b0836dfe96afbd2e48892de5be226470d462aac3c28206a316610d/optimum_habana-1.5.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"aeb82525fe13eb023b3625be169ae269625ba578e3d2ec24c976f57add39e7d9","md5":"cf6682094c1222869ceb96ecb2598410","sha256":"b1fd96547b16bade6469551a89bf97f1925dc37ef9b9d48ad5a35430a970d577"},"downloads":-1,"filename":"optimum-habana-1.5.1.tar.gz","has_sig":false,"md5_digest":"cf6682094c1222869ceb96ecb2598410","packagetype":"sdist","python_version":"source","requires_python":null,"size":122728,"upload_time":"2023-05-11T09:07:33","upload_time_iso_8601":"2023-05-11T09:07:33.848132Z","url":"https://files.pythonhosted.org/packages/ae/b8/2525fe13eb023b3625be169ae269625ba578e3d2ec24c976f57add39e7d9/optimum-habana-1.5.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.6.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.6.0/","requires_dist":["transformers (<4.29.0,>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.12.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.6.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"bcea63b421fd1b26d3ccc2802b93416b2127a96d92be2f25b65daf51828853bb","md5":"1877597234e9c962d2c7fc37f1303c1a","sha256":"fa3a887e40177fc727a51a20865d100f73010943074ad1e8f561b370b4a8adf1"},"downloads":-1,"filename":"optimum_habana-1.6.0-py3-none-any.whl","has_sig":false,"md5_digest":"1877597234e9c962d2c7fc37f1303c1a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":149231,"upload_time":"2023-06-26T09:39:27","upload_time_iso_8601":"2023-06-26T09:39:27.241310Z","url":"https://files.pythonhosted.org/packages/bc/ea/63b421fd1b26d3ccc2802b93416b2127a96d92be2f25b65daf51828853bb/optimum_habana-1.6.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6825c9845c2c653da17d6812d61711083cfa3ec28f0edfa600a3e629b9dd11c9","md5":"51b4a481ae5ac37eddb94f186477fdbe","sha256":"e3dc6c99f36fcccf4886271562eee9ed789bf4136203e937e16c616a80406c9a"},"downloads":-1,"filename":"optimum-habana-1.6.0.tar.gz","has_sig":false,"md5_digest":"51b4a481ae5ac37eddb94f186477fdbe","packagetype":"sdist","python_version":"source","requires_python":null,"size":149016,"upload_time":"2023-06-26T09:39:29","upload_time_iso_8601":"2023-06-26T09:39:29.322776Z","url":"https://files.pythonhosted.org/packages/68/25/c9845c2c653da17d6812d61711083cfa3ec28f0edfa600a3e629b9dd11c9/optimum-habana-1.6.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.6.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.6.1/","requires_dist":["transformers (<4.29.0,>=4.26.0)","optimum","torch","accelerate","diffusers (>=0.18.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.6.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"a7daa3c63790844a4e0c081bc4430c2cf2ca930c8038ec332bc5830d356e0e5e","md5":"c15405dbfe7c723c5fd1c1de76069ba3","sha256":"454c5cc7e46316d56326274393e1236e261c7081fea50c0d5d3810e57c130dae"},"downloads":-1,"filename":"optimum_habana-1.6.1-py3-none-any.whl","has_sig":false,"md5_digest":"c15405dbfe7c723c5fd1c1de76069ba3","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":150415,"upload_time":"2023-07-07T07:52:22","upload_time_iso_8601":"2023-07-07T07:52:22.041692Z","url":"https://files.pythonhosted.org/packages/a7/da/a3c63790844a4e0c081bc4430c2cf2ca930c8038ec332bc5830d356e0e5e/optimum_habana-1.6.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c7a35ba563b59d726662d89ce6552c9a7370b6d8969b79dee5a64be6363adf4e","md5":"9c4be9d8fe5f3edca1e847338cf968bf","sha256":"0853dfd48675e6297a980409dfc8e0218e08cdc21eb92147abedddf7ff477683"},"downloads":-1,"filename":"optimum-habana-1.6.1.tar.gz","has_sig":false,"md5_digest":"9c4be9d8fe5f3edca1e847338cf968bf","packagetype":"sdist","python_version":"source","requires_python":null,"size":150187,"upload_time":"2023-07-07T07:52:23","upload_time_iso_8601":"2023-07-07T07:52:23.573541Z","url":"https://files.pythonhosted.org/packages/c7/a3/5ba563b59d726662d89ce6552c9a7370b6d8969b79dee5a64be6363adf4e/optimum-habana-1.6.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.0/","requires_dist":["transformers >=4.31.0","optimum","torch","accelerate >=0.21.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"6e70c3b30ae2c25709064857c49c8b5ef64c70ac8b348778001df1426bd03eb5","md5":"89890e54fc8bdd2ca4c80211f6610159","sha256":"7e0f99e2cfdb064536a461d511d21437d9198cf1d63d127bee877b0c3f8ffffe"},"downloads":-1,"filename":"optimum_habana-1.7.0-py3-none-any.whl","has_sig":false,"md5_digest":"89890e54fc8bdd2ca4c80211f6610159","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":204690,"upload_time":"2023-08-17T11:19:53","upload_time_iso_8601":"2023-08-17T11:19:53.543048Z","url":"https://files.pythonhosted.org/packages/6e/70/c3b30ae2c25709064857c49c8b5ef64c70ac8b348778001df1426bd03eb5/optimum_habana-1.7.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"32e7a71e1396376d3ffc9318fdc72c06c9a01f5ef7754e7e1f4bd6865fce3201","md5":"74c863a4ec90c3de7181726722f2b272","sha256":"75175036e33fdae355662dd82d01af9258287bf8127682e6abce6e5a7b0485cd"},"downloads":-1,"filename":"optimum-habana-1.7.0.tar.gz","has_sig":false,"md5_digest":"74c863a4ec90c3de7181726722f2b272","packagetype":"sdist","python_version":"source","requires_python":null,"size":191614,"upload_time":"2023-08-17T11:19:55","upload_time_iso_8601":"2023-08-17T11:19:55.593969Z","url":"https://files.pythonhosted.org/packages/32/e7/a71e1396376d3ffc9318fdc72c06c9a01f5ef7754e7e1f4bd6865fce3201/optimum-habana-1.7.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.1/","requires_dist":["transformers >=4.31.0","optimum","torch","accelerate >=0.21.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"08c9b3caa67c8f4cdfb70a92a76bb81d21de798d28f18b8bf3230f228935f0ba","md5":"b20d434e054979e0ebf6d9457ff9e37c","sha256":"e80a5bafdd408024bbd0e3562fbab9dce06a5d4d19d8cf7c9e63b1cb11837a8d"},"downloads":-1,"filename":"optimum_habana-1.7.1-py3-none-any.whl","has_sig":false,"md5_digest":"b20d434e054979e0ebf6d9457ff9e37c","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":204368,"upload_time":"2023-08-23T10:37:50","upload_time_iso_8601":"2023-08-23T10:37:50.850518Z","url":"https://files.pythonhosted.org/packages/08/c9/b3caa67c8f4cdfb70a92a76bb81d21de798d28f18b8bf3230f228935f0ba/optimum_habana-1.7.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"7c026affc4b3770f03f5c2fd8651de8574fffe5347247b5ea348bac30830b4c6","md5":"a03269228badc380a3c38ac3e14be508","sha256":"6edfd6638dff1e736fad4236b1735abd311e7e93c4a679bc5cfb001aeff5d79b"},"downloads":-1,"filename":"optimum-habana-1.7.1.tar.gz","has_sig":false,"md5_digest":"a03269228badc380a3c38ac3e14be508","packagetype":"sdist","python_version":"source","requires_python":null,"size":190723,"upload_time":"2023-08-23T10:37:52","upload_time_iso_8601":"2023-08-23T10:37:52.523442Z","url":"https://files.pythonhosted.org/packages/7c/02/6affc4b3770f03f5c2fd8651de8574fffe5347247b5ea348bac30830b4c6/optimum-habana-1.7.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.2/","requires_dist":["transformers >=4.31.0","optimum","torch","accelerate >=0.21.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"ab60dc2cf858e8134a18c036dea17ab671f3fee38919aaef34294f8a6190f3e7","md5":"84fb9c953aeb72b94fc2213e399f4311","sha256":"809a782b6e7783499da95d74950860ecbc3ce19547d88f2c2bef745e5836a16f"},"downloads":-1,"filename":"optimum_habana-1.7.2-py3-none-any.whl","has_sig":false,"md5_digest":"84fb9c953aeb72b94fc2213e399f4311","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":201427,"upload_time":"2023-08-24T20:19:58","upload_time_iso_8601":"2023-08-24T20:19:58.696075Z","url":"https://files.pythonhosted.org/packages/ab/60/dc2cf858e8134a18c036dea17ab671f3fee38919aaef34294f8a6190f3e7/optimum_habana-1.7.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"3f67bf4bd524903e04efff458edf7d08b6877c3fcac1d2a210c03d99e58ddf5b","md5":"2afc82d34385a5efbe9f26d7cb719ade","sha256":"25c52686b1fb15bfe4b6032d44f40f0674107fddc20cada0bc9b54ecbf635847"},"downloads":-1,"filename":"optimum-habana-1.7.2.tar.gz","has_sig":false,"md5_digest":"2afc82d34385a5efbe9f26d7cb719ade","packagetype":"sdist","python_version":"source","requires_python":null,"size":187858,"upload_time":"2023-08-24T20:20:01","upload_time_iso_8601":"2023-08-24T20:20:01.166788Z","url":"https://files.pythonhosted.org/packages/3f/67/bf4bd524903e04efff458edf7d08b6877c3fcac1d2a210c03d99e58ddf5b/optimum-habana-1.7.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.3":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.3/","requires_dist":["transformers >=4.32.0","optimum","torch","accelerate >=0.22.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.3","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"0c32b7a02a56bd578b557c2216415779ab62eaa0224c04a2e10a4c9692f444ad","md5":"80f4412cba3ff3479c7c103b08c4a326","sha256":"2643b52c5fd0231006d74693d951452882986159d49c0f5ca41d6ab92dbb83d1"},"downloads":-1,"filename":"optimum_habana-1.7.3-py3-none-any.whl","has_sig":false,"md5_digest":"80f4412cba3ff3479c7c103b08c4a326","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":206080,"upload_time":"2023-09-08T14:24:28","upload_time_iso_8601":"2023-09-08T14:24:28.475805Z","url":"https://files.pythonhosted.org/packages/0c/32/b7a02a56bd578b557c2216415779ab62eaa0224c04a2e10a4c9692f444ad/optimum_habana-1.7.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bb040cce49a90fdd5e40e7a9fa29879af5aedabebfc35d8d1b2bded5aa9727ed","md5":"6ce7bcd628a73725ece4f7d3f965eecc","sha256":"b2f18050cb9e6e5f830916ff3b269e21660f78b52822352be859234ccfbd80ad"},"downloads":-1,"filename":"optimum-habana-1.7.3.tar.gz","has_sig":false,"md5_digest":"6ce7bcd628a73725ece4f7d3f965eecc","packagetype":"sdist","python_version":"source","requires_python":null,"size":191599,"upload_time":"2023-09-08T14:24:30","upload_time_iso_8601":"2023-09-08T14:24:30.728048Z","url":"https://files.pythonhosted.org/packages/bb/04/0cce49a90fdd5e40e7a9fa29879af5aedabebfc35d8d1b2bded5aa9727ed/optimum-habana-1.7.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.4":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.4/","requires_dist":["transformers >=4.32.0","optimum","torch","accelerate >=0.22.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.4","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"44c3313dfd5a406a48a0cd21ce0404d7d894ee15beea7abf0b3d4d47fd269edc","md5":"233a70d2c3ae79a310dd1b4c8ed453d3","sha256":"3b59604a7875c1a5b62555d07dfb8fe83c36c09cf7dcb74ccbe0941473d4fe45"},"downloads":-1,"filename":"optimum_habana-1.7.4-py3-none-any.whl","has_sig":false,"md5_digest":"233a70d2c3ae79a310dd1b4c8ed453d3","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":207283,"upload_time":"2023-09-12T18:23:08","upload_time_iso_8601":"2023-09-12T18:23:08.945405Z","url":"https://files.pythonhosted.org/packages/44/c3/313dfd5a406a48a0cd21ce0404d7d894ee15beea7abf0b3d4d47fd269edc/optimum_habana-1.7.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"b4350c4180aa225a4ba48efa4c259df16dfd30828e120952ab113fdb8c86857a","md5":"ce7baf0bb8e09799da9c85d067f7749d","sha256":"2ac3425579c8b479b3623e3d5dedff8075f99cc22dd62c002d7f0d016c25d8e3"},"downloads":-1,"filename":"optimum-habana-1.7.4.tar.gz","has_sig":false,"md5_digest":"ce7baf0bb8e09799da9c85d067f7749d","packagetype":"sdist","python_version":"source","requires_python":null,"size":192730,"upload_time":"2023-09-12T18:23:10","upload_time_iso_8601":"2023-09-12T18:23:10.533946Z","url":"https://files.pythonhosted.org/packages/b4/35/0c4180aa225a4ba48efa4c259df16dfd30828e120952ab113fdb8c86857a/optimum-habana-1.7.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.7.5":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.7.5/","requires_dist":["transformers >=4.32.0","optimum","torch","accelerate >=0.22.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.7.5","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"44b1a9e608bd4ba5049cf9edfdb23fa5da0b75272f59cac1e200da591d8d8f4f","md5":"3739584fdf8c8b825ab2d710f6ab1472","sha256":"4ed37e967d289da8b56c67884d2e339c40928a9b8fb33d7210d085f03d2c4376"},"downloads":-1,"filename":"optimum_habana-1.7.5-py3-none-any.whl","has_sig":false,"md5_digest":"3739584fdf8c8b825ab2d710f6ab1472","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":207289,"upload_time":"2023-09-14T14:21:49","upload_time_iso_8601":"2023-09-14T14:21:49.455838Z","url":"https://files.pythonhosted.org/packages/44/b1/a9e608bd4ba5049cf9edfdb23fa5da0b75272f59cac1e200da591d8d8f4f/optimum_habana-1.7.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"80d04d865aa1c223b54962123afb3cbe33df720502f8e4f54ce8bccc7fe45845","md5":"9c9ee323112d56af439e5c73b16b1571","sha256":"2040b14b4e395d3948b5b3404ee3762fc524449bf647f9a7ea66dcf422866a28"},"downloads":-1,"filename":"optimum-habana-1.7.5.tar.gz","has_sig":false,"md5_digest":"9c9ee323112d56af439e5c73b16b1571","packagetype":"sdist","python_version":"source","requires_python":null,"size":192769,"upload_time":"2023-09-14T14:21:52","upload_time_iso_8601":"2023-09-14T14:21:52.947442Z","url":"https://files.pythonhosted.org/packages/80/d0/4d865aa1c223b54962123afb3cbe33df720502f8e4f54ce8bccc7fe45845/optimum-habana-1.7.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.8.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.8.0/","requires_dist":["optimum","transformers (>=4.33.0)","torch","accelerate (>=0.22.0)","diffusers (>=0.18.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.8.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"51af67122aa6d2a481f469da3a32275f111e8a0a76a096895ac72536c589cf95","md5":"392a5489c0e0c3843e6c8c2a54424bae","sha256":"0e5bc3f03122e65205ba292855cbc3067c091a0f898f73f49e5e754661bfd671"},"downloads":-1,"filename":"optimum_habana-1.8.0-py3-none-any.whl","has_sig":false,"md5_digest":"392a5489c0e0c3843e6c8c2a54424bae","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":232794,"upload_time":"2023-10-19T20:14:44","upload_time_iso_8601":"2023-10-19T20:14:44.809096Z","url":"https://files.pythonhosted.org/packages/51/af/67122aa6d2a481f469da3a32275f111e8a0a76a096895ac72536c589cf95/optimum_habana-1.8.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"389060ad948922defc884d558802149b04abed6b7029f6ce5a2ba8f8585ee9b4","md5":"9c4f05dce27cc5bd2f1a8248da55160a","sha256":"63dc1ecd0d5c8f41528bff374d2645887931aa7467b1512314a2cd92e295e838"},"downloads":-1,"filename":"optimum-habana-1.8.0.tar.gz","has_sig":false,"md5_digest":"9c4f05dce27cc5bd2f1a8248da55160a","packagetype":"sdist","python_version":"source","requires_python":null,"size":218368,"upload_time":"2023-10-19T20:14:46","upload_time_iso_8601":"2023-10-19T20:14:46.963998Z","url":"https://files.pythonhosted.org/packages/38/90/60ad948922defc884d558802149b04abed6b7029f6ce5a2ba8f8585ee9b4/optimum-habana-1.8.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.8.1":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.8.1/","requires_dist":["transformers <4.35.0,>=4.33.0","optimum","torch","accelerate >=0.23.0","diffusers >=0.18.0","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.8.1","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"ca8f4a8c89dbd0dbde33125415329bf6575f4903e156db9f79117f7c89598eaf","md5":"3aba6e4e6cb8e81e4b2bc41750d0d994","sha256":"52b2b0680ba076146e78a31249662ef48b99c6899d5d7537287dac7201633726"},"downloads":-1,"filename":"optimum_habana-1.8.1-py3-none-any.whl","has_sig":false,"md5_digest":"3aba6e4e6cb8e81e4b2bc41750d0d994","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":232795,"upload_time":"2023-11-02T10:27:03","upload_time_iso_8601":"2023-11-02T10:27:03.352131Z","url":"https://files.pythonhosted.org/packages/ca/8f/4a8c89dbd0dbde33125415329bf6575f4903e156db9f79117f7c89598eaf/optimum_habana-1.8.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"0b101d08c60cc98f0124251498b8fbafaea73863719dadd541e681ea253f77d7","md5":"d0ab3daee851e4fd816f2c3ed76db63b","sha256":"a5b3c8d82d780689de44b7e47c67f267c41786a17b44da3dead1022635977196"},"downloads":-1,"filename":"optimum-habana-1.8.1.tar.gz","has_sig":false,"md5_digest":"d0ab3daee851e4fd816f2c3ed76db63b","packagetype":"sdist","python_version":"source","requires_python":null,"size":218392,"upload_time":"2023-11-02T10:27:05","upload_time_iso_8601":"2023-11-02T10:27:05.347458Z","url":"https://files.pythonhosted.org/packages/0b/10/1d08c60cc98f0124251498b8fbafaea73863719dadd541e681ea253f77d7/optimum-habana-1.8.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.8.2":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.8.2/","requires_dist":["transformers (<4.35.0,>=4.33.0)","optimum","torch","accelerate (>=0.23.0)","diffusers (>=0.18.0)","black ; extra == 'quality'","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.8.2","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"93cb67a23be2d36647aa9640d8e45728bdbe495a4a08398c75d01d19ee29eac9","md5":"5da9c76e63e5a4cf12076bdde153cb21","sha256":"256caaa9827cc86a2ad32e6e06c2b2a2fd3cd7aafb075c35521e2f3ff2f79223"},"downloads":-1,"filename":"optimum_habana-1.8.2-py3-none-any.whl","has_sig":false,"md5_digest":"5da9c76e63e5a4cf12076bdde153cb21","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":232983,"upload_time":"2023-11-24T21:34:15","upload_time_iso_8601":"2023-11-24T21:34:15.992387Z","url":"https://files.pythonhosted.org/packages/93/cb/67a23be2d36647aa9640d8e45728bdbe495a4a08398c75d01d19ee29eac9/optimum_habana-1.8.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"95b113f3241af597220377258889e6be9f256a2d021f337835dc2d86a45e69ae","md5":"a37b092635d84e855e5b6d89a50ad1ca","sha256":"791e978844c4853dbcac3eebde010ced32d4d2c625a13ea82dde5ca2fd802e3b"},"downloads":-1,"filename":"optimum-habana-1.8.2.tar.gz","has_sig":false,"md5_digest":"a37b092635d84e855e5b6d89a50ad1ca","packagetype":"sdist","python_version":"source","requires_python":null,"size":218455,"upload_time":"2023-11-24T21:34:18","upload_time_iso_8601":"2023-11-24T21:34:18.206151Z","url":"https://files.pythonhosted.org/packages/95/b1/13f3241af597220377258889e6be9f256a2d021f337835dc2d86a45e69ae/optimum-habana-1.8.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.9.0":{"info":{"author":"HuggingFace Inc. Special Ops Team","author_email":"hardware@huggingface.co","bugtrack_url":null,"classifiers":["Development Status :: 5 - Production/Stable","Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://huggingface.co/hardware/habana","keywords":"transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu","license":"Apache","maintainer":"","maintainer_email":"","name":"optimum-habana","package_url":"https://pypi.org/project/optimum-habana/","platform":null,"project_url":"https://pypi.org/project/optimum-habana/","project_urls":{"Homepage":"https://huggingface.co/hardware/habana"},"provides_extra":null,"release_url":"https://pypi.org/project/optimum-habana/1.9.0/","requires_dist":["transformers (<4.35.0,>=4.34.0)","optimum","torch","accelerate (>=0.23.0)","diffusers (<0.24.0,>=0.18.0)","ruff ; extra == 'quality'","hf-doc-builder ; extra == 'quality'","pytest ; extra == 'tests'","psutil ; extra == 'tests'","parameterized ; extra == 'tests'","GitPython ; extra == 'tests'","optuna ; extra == 'tests'","sentencepiece ; extra == 'tests'","datasets ; extra == 'tests'","safetensors ; extra == 'tests'"],"requires_python":"","summary":"Optimum Habana is the interface between the Hugging Face Transformers and Diffusers libraries and Habana's Gaudi processor (HPU). It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.","version":"1.9.0","yanked":false,"yanked_reason":null},"last_serial":25729181,"urls":[{"comment_text":"","digests":{"blake2b_256":"f149ef307d4adc4fe8f6ee23ed314cac547bd39077bd503fd68b44adaada3965","md5":"73f7c8dd44aee4eed0bbc29fc1373a36","sha256":"04ef1abfdeee4ba9cfa76064e3ccfefa7bfc6b0ae7921bdd84db5a26014d119a"},"downloads":-1,"filename":"optimum_habana-1.9.0-py3-none-any.whl","has_sig":false,"md5_digest":"73f7c8dd44aee4eed0bbc29fc1373a36","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":250151,"upload_time":"2023-11-30T23:19:57","upload_time_iso_8601":"2023-11-30T23:19:57.768779Z","url":"https://files.pythonhosted.org/packages/f1/49/ef307d4adc4fe8f6ee23ed314cac547bd39077bd503fd68b44adaada3965/optimum_habana-1.9.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"abbfbc4d843a4563546395e33c52961673e6fde06268186fcc81318ba6bf009e","md5":"79b4968d46e0478b189226d5712a47fb","sha256":"8f5402d81e99fd0fd6bcd90a3927505d97783dffbc27177996f28b42e216e910"},"downloads":-1,"filename":"optimum-habana-1.9.0.tar.gz","has_sig":false,"md5_digest":"79b4968d46e0478b189226d5712a47fb","packagetype":"sdist","python_version":"source","requires_python":null,"size":234392,"upload_time":"2023-11-30T23:20:00","upload_time_iso_8601":"2023-11-30T23:20:00.195651Z","url":"https://files.pythonhosted.org/packages/ab/bf/bc4d843a4563546395e33c52961673e6fde06268186fcc81318ba6bf009e/optimum-habana-1.9.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}