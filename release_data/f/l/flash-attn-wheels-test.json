{"2.0.4":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.4/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.4","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"45be2c49b8f740e4b778844cd247f35f9095379d207b8152c3253d429cbb06ec","md5":"5a81b8d81eed2fde34d515b447908131","sha256":"c4de074a7cde2a8ac16ecb4682bdfda62aec0712c7897471c9b1fe396ca282a2"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.4.tar.gz","has_sig":false,"md5_digest":"5a81b8d81eed2fde34d515b447908131","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":56417,"upload_time":"2023-08-11T00:33:34","upload_time_iso_8601":"2023-08-11T00:33:34.573649Z","url":"https://files.pythonhosted.org/packages/45/be/2c49b8f740e4b778844cd247f35f9095379d207b8152c3253d429cbb06ec/flash_attn_wheels_test-2.0.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.5":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.5/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.5","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"edadc55d6899da98927a5b6db004e1b2ba7a6b1827f5ec6a11d413e524745bc2","md5":"8a2b04f43820188e6fd78c3dbb37d7a1","sha256":"924c1aa54ecc0cf9b01e6e438fb2111a88a0e9a2e7160e8cc2e50badfef48ad0"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.5.tar.gz","has_sig":false,"md5_digest":"8a2b04f43820188e6fd78c3dbb37d7a1","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":56409,"upload_time":"2023-08-11T04:46:45","upload_time_iso_8601":"2023-08-11T04:46:45.107710Z","url":"https://files.pythonhosted.org/packages/ed/ad/c55d6899da98927a5b6db004e1b2ba7a6b1827f5ec6a11d413e524745bc2/flash_attn_wheels_test-2.0.5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.6":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.6/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.6","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"7425ad30c84ae166d3f01495fdf7c4b91d1aca3f60a74d96cbbe70938c45a45e","md5":"dba51ca5c5b515c3cbfe62c9be102c35","sha256":"dfa19b825dea093ba1dd916db8a23063047fbf3f1068ac7a2133deff4150fb0f"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.6.tar.gz","has_sig":false,"md5_digest":"dba51ca5c5b515c3cbfe62c9be102c35","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1896971,"upload_time":"2023-08-11T19:19:56","upload_time_iso_8601":"2023-08-11T19:19:56.927197Z","url":"https://files.pythonhosted.org/packages/74/25/ad30c84ae166d3f01495fdf7c4b91d1aca3f60a74d96cbbe70938c45a45e/flash_attn_wheels_test-2.0.6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.6.post4":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.6.post4/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.6.post4","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"c76e1b379f09849ff41422c4fe4987ca767345dacb6ea0e2fe0f32aec98ead68","md5":"477b58f4b136e5a84189bba46aa61c8b","sha256":"077379394c15db6aba23ee29f82ab5e485e5940a639a235a486f2716983998e4"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.6.post4.tar.gz","has_sig":false,"md5_digest":"477b58f4b136e5a84189bba46aa61c8b","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":2017422,"upload_time":"2023-08-14T20:46:34","upload_time_iso_8601":"2023-08-14T20:46:34.175727Z","url":"https://files.pythonhosted.org/packages/c7/6e/1b379f09849ff41422c4fe4987ca767345dacb6ea0e2fe0f32aec98ead68/flash_attn_wheels_test-2.0.6.post4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.6.post8":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.6.post8/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.6.post8","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"b0a37c348bc01be9df32018a39cf8f09d53b06837aa8dfa57e4c6c3022e7e03f","md5":"e12182bdb6f90219832a4168974a5c29","sha256":"984b54f6b2a31094ef95f7689dc154afda2a0d145f695be16c79613432d32b83"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.6.post8.tar.gz","has_sig":false,"md5_digest":"e12182bdb6f90219832a4168974a5c29","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":2016977,"upload_time":"2023-08-17T02:57:57","upload_time_iso_8601":"2023-08-17T02:57:57.276679Z","url":"https://files.pythonhosted.org/packages/b0/a3/7c348bc01be9df32018a39cf8f09d53b06837aa8dfa57e4c6c3022e7e03f/flash_attn_wheels_test-2.0.6.post8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.7":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.7/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.7","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"dc92d7997ec4e173f42e4cb36ca8340a44367275970a712db1a1bc83ba85a0e5","md5":"ff8960a60682b166ea56aa4751beee07","sha256":"a8acf8f0df45af2c0cfa89638cd23d9b12daa031054114120c82c0222fccbeae"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.7.tar.gz","has_sig":false,"md5_digest":"ff8960a60682b166ea56aa4751beee07","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1896849,"upload_time":"2023-08-11T20:01:05","upload_time_iso_8601":"2023-08-11T20:01:05.245861Z","url":"https://files.pythonhosted.org/packages/dc/92/d7997ec4e173f42e4cb36ca8340a44367275970a712db1a1bc83ba85a0e5/flash_attn_wheels_test-2.0.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post1":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post1/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post1","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"c4e8017268a0ba14b5f76a68d908fe7d39daa3dc67c8f6176fe9a2c82c601b4a","md5":"9b6b6562324cc900dcedede2ebd46db7","sha256":"596a8b1af40489b667866a664d3af55955941e62ad3a7c01084d9607f2136ea1"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post1.tar.gz","has_sig":false,"md5_digest":"9b6b6562324cc900dcedede2ebd46db7","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898248,"upload_time":"2023-08-11T22:31:14","upload_time_iso_8601":"2023-08-11T22:31:14.563284Z","url":"https://files.pythonhosted.org/packages/c4/e8/017268a0ba14b5f76a68d908fe7d39daa3dc67c8f6176fe9a2c82c601b4a/flash_attn_wheels_test-2.0.8.post1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post10":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post10/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post10","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"71bd844b8472867475b22ff7a07a4989a02107ab03f0d60ad3861be4bf4018dc","md5":"51fc5e58dbac88df3336583be93cd572","sha256":"c0388a07d24df2908bdc3f364bd675a9cf1c654352c5105435b06fa58c7684a9"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post10.tar.gz","has_sig":false,"md5_digest":"51fc5e58dbac88df3336583be93cd572","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898914,"upload_time":"2023-08-12T20:03:32","upload_time_iso_8601":"2023-08-12T20:03:32.136141Z","url":"https://files.pythonhosted.org/packages/71/bd/844b8472867475b22ff7a07a4989a02107ab03f0d60ad3861be4bf4018dc/flash_attn_wheels_test-2.0.8.post10.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post11":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post11/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post11","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"06b8df1a1cbc42bec5bdb6bc210e53ae5d00defe34923cbaeff58cc95597e559","md5":"f43798a2b23c9aa65b94ee89c396a94c","sha256":"76291745e98a3468a0fc7ed3247a1f222aa3a87f332e0da6968682e55a432960"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post11.tar.gz","has_sig":false,"md5_digest":"f43798a2b23c9aa65b94ee89c396a94c","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898817,"upload_time":"2023-08-12T20:51:21","upload_time_iso_8601":"2023-08-12T20:51:21.724711Z","url":"https://files.pythonhosted.org/packages/06/b8/df1a1cbc42bec5bdb6bc210e53ae5d00defe34923cbaeff58cc95597e559/flash_attn_wheels_test-2.0.8.post11.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post17":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post17/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post17","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"44fde920608e4f115c21a340d66015dc0258b2e99e699c7cfc4fe8b433dda411","md5":"045a7d3474e5ca148d4f4aeb50d19621","sha256":"632018117f1acba417d574ca4d66a0a6ae51e107ed3ed2b897381af0d17ef3cc"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post17.tar.gz","has_sig":false,"md5_digest":"045a7d3474e5ca148d4f4aeb50d19621","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898451,"upload_time":"2023-08-13T21:27:09","upload_time_iso_8601":"2023-08-13T21:27:09.714676Z","url":"https://files.pythonhosted.org/packages/44/fd/e920608e4f115c21a340d66015dc0258b2e99e699c7cfc4fe8b433dda411/flash_attn_wheels_test-2.0.8.post17.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post2":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post2/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post2","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"dd109e9dfb74f2f5aa22fa7ecdcb5ea9a713995d3b83ee23191dc85ff29d17b6","md5":"04a02150067a057ea4396ac8bdb06547","sha256":"e51dd947b8ea7aa246aad3370868fbc4a9cbcc9dc12d22378f2f55aff36fd7a2"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post2.tar.gz","has_sig":false,"md5_digest":"04a02150067a057ea4396ac8bdb06547","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898222,"upload_time":"2023-08-11T23:13:19","upload_time_iso_8601":"2023-08-11T23:13:19.923822Z","url":"https://files.pythonhosted.org/packages/dd/10/9e9dfb74f2f5aa22fa7ecdcb5ea9a713995d3b83ee23191dc85ff29d17b6/flash_attn_wheels_test-2.0.8.post2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post5":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post5/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post5","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"c3cd8417d4469fd6b7911fc21fcca87eda0b12b7090c5761516cfddf115d00ac","md5":"acd3ac4d0e1b335a5746a9a13e313ca3","sha256":"f174e5ce3cf4e4a93b7823bc1f1177b30680843f0b0370e9e63e2b5d90d776e8"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post5.tar.gz","has_sig":false,"md5_digest":"acd3ac4d0e1b335a5746a9a13e313ca3","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898165,"upload_time":"2023-08-12T05:47:48","upload_time_iso_8601":"2023-08-12T05:47:48.802189Z","url":"https://files.pythonhosted.org/packages/c3/cd/8417d4469fd6b7911fc21fcca87eda0b12b7090c5761516cfddf115d00ac/flash_attn_wheels_test-2.0.8.post5.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post8":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post8/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post8","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"0b27f5a0f512e74d2985eff41758452e65761d8375f3fb0ef64c2bf2ab0c1627","md5":"884bd7baf2ec9276de04bff52528371e","sha256":"1b3a911109ae284dacb2997c7ce1a615fc9f2fac5b1ba3151ecee7e921f39636"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post8.tar.gz","has_sig":false,"md5_digest":"884bd7baf2ec9276de04bff52528371e","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898100,"upload_time":"2023-08-12T06:20:59","upload_time_iso_8601":"2023-08-12T06:20:59.621895Z","url":"https://files.pythonhosted.org/packages/0b/27/f5a0f512e74d2985eff41758452e65761d8375f3fb0ef64c2bf2ab0c1627/flash_attn_wheels_test-2.0.8.post8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"2.0.8.post9":{"info":{"author":"Tri Dao","author_email":"trid@cs.stanford.edu","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Dao-AILab/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels-test","package_url":"https://pypi.org/project/flash-attn-wheels-test/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels-test/","project_urls":{"Homepage":"https://github.com/Dao-AILab/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels-test/2.0.8.post9/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"2.0.8.post9","yanked":false,"yanked_reason":null},"last_serial":19339067,"urls":[{"comment_text":"","digests":{"blake2b_256":"c370091c93e9b19594370ffea49bd3927f1f41d52b57c9fc7e2c83465a7f6194","md5":"46f82986d7d017f7f394000b25a0686f","sha256":"64b6221dd5ecd761a268528f292a082eb6dd4741ca0d788286140b912f42f37d"},"downloads":-1,"filename":"flash_attn_wheels_test-2.0.8.post9.tar.gz","has_sig":false,"md5_digest":"46f82986d7d017f7f394000b25a0686f","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":1898050,"upload_time":"2023-08-12T07:11:11","upload_time_iso_8601":"2023-08-12T07:11:11.140207Z","url":"https://files.pythonhosted.org/packages/c3/70/091c93e9b19594370ffea49bd3927f1f41d52b57c9fc7e2c83465a7f6194/flash_attn_wheels_test-2.0.8.post9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}