{"1.0.7":{"info":{"author":"Pierce Freeman","author_email":"pierce@freeman.vc","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/piercefreeman/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels","package_url":"https://pypi.org/project/flash-attn-wheels/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels/","project_urls":{"Homepage":"https://github.com/piercefreeman/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels/1.0.7/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"1.0.7","yanked":false,"yanked_reason":null},"last_serial":18416488,"urls":[{"comment_text":"","digests":{"blake2b_256":"a70cc5fcd891f0e60b68cd2cfec76e100e03daf8b013806de3c8fa459bd327d6","md5":"237bbfcca664278f7747504ed1dbbecd","sha256":"429c8e9322c118a77fa01763f6607e4fd3c3f13fea9a63ce66b216c9b16a665b"},"downloads":-1,"filename":"flash_attn_wheels-1.0.7.tar.gz","has_sig":false,"md5_digest":"237bbfcca664278f7747504ed1dbbecd","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":216431,"upload_time":"2023-06-03T01:20:58","upload_time_iso_8601":"2023-06-03T01:20:58.145568Z","url":"https://files.pythonhosted.org/packages/a7/0c/c5fcd891f0e60b68cd2cfec76e100e03daf8b013806de3c8fa459bd327d6/flash_attn_wheels-1.0.7.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.8":{"info":{"author":"Pierce Freeman","author_email":"pierce@freeman.vc","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/piercefreeman/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels","package_url":"https://pypi.org/project/flash-attn-wheels/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels/","project_urls":{"Homepage":"https://github.com/piercefreeman/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels/1.0.8/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"1.0.8","yanked":false,"yanked_reason":null},"last_serial":18416488,"urls":[{"comment_text":"","digests":{"blake2b_256":"247ff477dbc5d6f4a6f032dcc59752cdc6db509ed4f7e1fe32e453f5989c43c7","md5":"f72fb7faaa0bd9733515a46856dc4ce4","sha256":"fdbbde80569c7d69a2f1db9d1bb58d2066feaf2b05a06ba1de45e2b43dbf4c2a"},"downloads":-1,"filename":"flash_attn_wheels-1.0.8.tar.gz","has_sig":false,"md5_digest":"f72fb7faaa0bd9733515a46856dc4ce4","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":209093,"upload_time":"2023-06-04T14:17:35","upload_time_iso_8601":"2023-06-04T14:17:35.946944Z","url":"https://files.pythonhosted.org/packages/24/7f/f477dbc5d6f4a6f032dcc59752cdc6db509ed4f7e1fe32e453f5989c43c7/flash_attn_wheels-1.0.8.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"1.0.9":{"info":{"author":"Pierce Freeman","author_email":"pierce@freeman.vc","bugtrack_url":null,"classifiers":["License :: OSI Approved :: BSD License","Operating System :: Unix","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/piercefreeman/flash-attention","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"flash-attn-wheels","package_url":"https://pypi.org/project/flash-attn-wheels/","platform":null,"project_url":"https://pypi.org/project/flash-attn-wheels/","project_urls":{"Homepage":"https://github.com/piercefreeman/flash-attention"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attn-wheels/1.0.9/","requires_dist":null,"requires_python":">=3.7","summary":"Flash Attention: Fast and Memory-Efficient Exact Attention","version":"1.0.9","yanked":false,"yanked_reason":null},"last_serial":18416488,"urls":[{"comment_text":"","digests":{"blake2b_256":"f92e0ff829365bcffb2a215f1d5d95016b073c2dc90e359333c4a3e3b60d6698","md5":"b54cdae2066074f1b362b0790e6f99f6","sha256":"6f4ed8e9c2ea8dedd085b50ce377e7a90310ef26bb0880afd789beece502598c"},"downloads":-1,"filename":"flash_attn_wheels-1.0.9.tar.gz","has_sig":false,"md5_digest":"b54cdae2066074f1b362b0790e6f99f6","packagetype":"sdist","python_version":"source","requires_python":">=3.7","size":210065,"upload_time":"2023-06-08T01:00:24","upload_time_iso_8601":"2023-06-08T01:00:24.751264Z","url":"https://files.pythonhosted.org/packages/f9/2e/0ff829365bcffb2a215f1d5d95016b073c2dc90e359333c4a3e3b60d6698/flash_attn_wheels-1.0.9.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}