{"0.0.1":{"info":{"author":"Xiaotian Han","author_email":"your.email@example.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/ahxt/flash-attention-x","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"flash-attention-x","package_url":"https://pypi.org/project/flash-attention-x/","platform":null,"project_url":"https://pypi.org/project/flash-attention-x/","project_urls":{"Homepage":"https://github.com/ahxt/flash-attention-x"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-x/0.0.1/","requires_dist":null,"requires_python":">=3.8","summary":"A flash attention(s) implementation in triton.","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":23992520,"urls":[{"comment_text":"","digests":{"blake2b_256":"c69e38f907af63cc1ca3ee67f6cbea8a40d44b62efbf6e0cb7a4c3c530e41874","md5":"df1f9ca384e86a8fcd8b6618162f10a8","sha256":"e83277a2bd3d7023d22cb5c8b6dd159d4bd99cabfce60dde9426f091b01f242a"},"downloads":-1,"filename":"flash_attention_x-0.0.1.tar.gz","has_sig":false,"md5_digest":"df1f9ca384e86a8fcd8b6618162f10a8","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":6721,"upload_time":"2024-07-04T23:53:14","upload_time_iso_8601":"2024-07-04T23:53:14.576148Z","url":"https://files.pythonhosted.org/packages/c6/9e/38f907af63cc1ca3ee67f6cbea8a40d44b62efbf6e0cb7a4c3c530e41874/flash_attention_x-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}