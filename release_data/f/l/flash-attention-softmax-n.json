{"0.1.0":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3 :: Only","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.0/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.0","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"03e452a47eaff9cce0e4c9d5dcf801a44fd9f2abee05dfba50f8a446eda6265b","md5":"9ad97518b78ec387c74e8c56e2e3fed8","sha256":"c8ad2259ff8ad18e341c089a6acbedee18a107245e21ad66275df0db967d75b2"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"9ad97518b78ec387c74e8c56e2e3fed8","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":21657,"upload_time":"2023-08-26T00:03:46","upload_time_iso_8601":"2023-08-26T00:03:46.687729Z","url":"https://files.pythonhosted.org/packages/03/e4/52a47eaff9cce0e4c9d5dcf801a44fd9f2abee05dfba50f8a446eda6265b/flash_attention_softmax_n-0.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"0e4fcc434c22d8d6306db3c5cb3d57bb81936e7e1af6a30c33d16935493ded5b","md5":"c221862e6915d91991221b983e36d243","sha256":"ea8aa79fb5b3e8d27d26ee1109d4e2e7691af4b95fe1e4fa50bf3ac56492b465"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.0.tar.gz","has_sig":false,"md5_digest":"c221862e6915d91991221b983e36d243","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":21804,"upload_time":"2023-08-26T00:03:47","upload_time_iso_8601":"2023-08-26T00:03:47.815442Z","url":"https://files.pythonhosted.org/packages/0e/4f/cc434c22d8d6306db3c5cb3d57bb81936e7e1af6a30c33d16935493ded5b/flash-attention-softmax-n-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.0rc6":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.0rc6/","requires_dist":["einops (>=0.6.1)","torch (>=2.0.0)","triton (>=2.0.0) ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.0rc6","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"102cf0ccd8d90a0ce40deff97d3ca681a46d37fcef33cddee004b37bd0e39c45","md5":"c1a679b1ab09eef6c7b32f39b5883c22","sha256":"73ac8fc2ad25a6bd61db6dd7494dcdc86709c431c39200860de95e06be90d958"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.0rc6-py3-none-any.whl","has_sig":false,"md5_digest":"c1a679b1ab09eef6c7b32f39b5883c22","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":21737,"upload_time":"2023-08-26T00:29:11","upload_time_iso_8601":"2023-08-26T00:29:11.962730Z","url":"https://files.pythonhosted.org/packages/10/2c/f0ccd8d90a0ce40deff97d3ca681a46d37fcef33cddee004b37bd0e39c45/flash_attention_softmax_n-0.1.0rc6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"252da1342b986bf86672e8d50958c2f52b2853f0782a3ae6edb6c2ad39ea6a1b","md5":"e70caff7efe70e7fb2f68fba0b1d2a0d","sha256":"c976a10f48c12f248fb1d3b36bfb1fc9ec33691ebfe7a4b713f6b5fc810ed253"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.0rc6.tar.gz","has_sig":false,"md5_digest":"e70caff7efe70e7fb2f68fba0b1d2a0d","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":21867,"upload_time":"2023-08-26T00:29:13","upload_time_iso_8601":"2023-08-26T00:29:13.359462Z","url":"https://files.pythonhosted.org/packages/25/2d/a1342b986bf86672e8d50958c2f52b2853f0782a3ae6edb6c2ad39ea6a1b/flash-attention-softmax-n-0.1.0rc6.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.1/","requires_dist":["einops (>=0.6.1)","torch (>=2.0.0)","triton (>=2.0.0) ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"f6b885081626f879c4d5be9fe8dbf3d7a17682779c7e7b6e3bc7160ad6246bff","md5":"872fff2b0a492bd3f7463cb9106387d3","sha256":"ad3c9299f86a2623c9786857fee686a009b817649d7efef3aec0d72e47f1bb63"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"872fff2b0a492bd3f7463cb9106387d3","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":21713,"upload_time":"2023-08-26T00:42:48","upload_time_iso_8601":"2023-08-26T00:42:48.549439Z","url":"https://files.pythonhosted.org/packages/f6/b8/85081626f879c4d5be9fe8dbf3d7a17682779c7e7b6e3bc7160ad6246bff/flash_attention_softmax_n-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"eb4063a4d7523c8f29a321b8937149345b9506418776c527ede979c309f45f1f","md5":"99120bfd1be848ba44af5a895ebb00d2","sha256":"d77e2d832512c569e9341b72fd41bcb904a4a7ae05a2059c904ea994d55511f0"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.1.tar.gz","has_sig":false,"md5_digest":"99120bfd1be848ba44af5a895ebb00d2","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":21876,"upload_time":"2023-08-26T00:42:49","upload_time_iso_8601":"2023-08-26T00:42:49.587637Z","url":"https://files.pythonhosted.org/packages/eb/40/63a4d7523c8f29a321b8937149345b9506418776c527ede979c309f45f1f/flash-attention-softmax-n-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.2":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.2/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.2","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"d7213139d7409eb50735b4c5b3d28526352e430a2a2ebd045ecc45bd33293f3e","md5":"9c12ea0f11f2cd3ffc35a3bc3dcc4a55","sha256":"4df6b062a0c1324d403289e9377518122e6769a370d23f97fc476dbf2db268a2"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"9c12ea0f11f2cd3ffc35a3bc3dcc4a55","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":21691,"upload_time":"2023-08-26T18:38:40","upload_time_iso_8601":"2023-08-26T18:38:40.159883Z","url":"https://files.pythonhosted.org/packages/d7/21/3139d7409eb50735b4c5b3d28526352e430a2a2ebd045ecc45bd33293f3e/flash_attention_softmax_n-0.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"d00989d6f11f17833d20af896173389adb89799f7350f508118a32ea7957959a","md5":"ada82ce48584bfe167aca989d441e020","sha256":"f27feebe1b02d410d4bc5a2f1736168a2bbf77b5359b34ed029b0bcd2d8a61ce"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.2.tar.gz","has_sig":false,"md5_digest":"ada82ce48584bfe167aca989d441e020","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":21830,"upload_time":"2023-08-26T18:38:41","upload_time_iso_8601":"2023-08-26T18:38:41.576069Z","url":"https://files.pythonhosted.org/packages/d0/09/89d6f11f17833d20af896173389adb89799f7350f508118a32ea7957959a/flash-attention-softmax-n-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.3":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.3/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"0e2b5d9a20a0cf2ae141033fcaefa6c69e1f0864176228cb45f03ea60ea3f458","md5":"f381f89b9930e5f9f5b2a5ef70fa1c98","sha256":"41e31dbc2e51d237570ae80c840ae9e777d7d9d1ef65e288ad828662d86f21cf"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"f381f89b9930e5f9f5b2a5ef70fa1c98","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":23526,"upload_time":"2023-08-28T14:28:43","upload_time_iso_8601":"2023-08-28T14:28:43.123772Z","url":"https://files.pythonhosted.org/packages/0e/2b/5d9a20a0cf2ae141033fcaefa6c69e1f0864176228cb45f03ea60ea3f458/flash_attention_softmax_n-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ea22ccad6b7ffa8e981f73d732fdead7c2f94db7e1c94da9be59661947ee3c72","md5":"773b6201834114c2320926334836fbc8","sha256":"61db521553f9f4696a14294608a50665d22e28605e88c89ada3818c71bdb439b"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.3.tar.gz","has_sig":false,"md5_digest":"773b6201834114c2320926334836fbc8","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":23699,"upload_time":"2023-08-28T14:28:44","upload_time_iso_8601":"2023-08-28T14:28:44.775078Z","url":"https://files.pythonhosted.org/packages/ea/22/ccad6b7ffa8e981f73d732fdead7c2f94db7e1c94da9be59661947ee3c72/flash-attention-softmax-n-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.4":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.1.4/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.1.4","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"fa0a9445d4a6183a06054891f20e58588adebd3f9345bb3bf13afd5f2159897f","md5":"8126caa45e95381e6983560014ac8459","sha256":"703c67e0fa65d2da55947fcfc953bb0c968d5dfddd1968d27bf7a6557a2e3302"},"downloads":-1,"filename":"flash_attention_softmax_n-0.1.4-py3-none-any.whl","has_sig":false,"md5_digest":"8126caa45e95381e6983560014ac8459","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":23566,"upload_time":"2023-08-28T23:45:32","upload_time_iso_8601":"2023-08-28T23:45:32.025741Z","url":"https://files.pythonhosted.org/packages/fa/0a/9445d4a6183a06054891f20e58588adebd3f9345bb3bf13afd5f2159897f/flash_attention_softmax_n-0.1.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5e9f8c2ecc68d2f1a185d9f58e37fbbab2acfca4efd2b3d5d634408e6b546811","md5":"858869605346bef8a16cd8d922901295","sha256":"5922abc0a5756bb936e389d995a5277241e754ab40662cc104835b301881877d"},"downloads":-1,"filename":"flash-attention-softmax-n-0.1.4.tar.gz","has_sig":false,"md5_digest":"858869605346bef8a16cd8d922901295","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":23746,"upload_time":"2023-08-28T23:45:33","upload_time_iso_8601":"2023-08-28T23:45:33.670004Z","url":"https://files.pythonhosted.org/packages/5e/9f/8c2ecc68d2f1a185d9f58e37fbbab2acfca4efd2b3d5d634408e6b546811/flash-attention-softmax-n-0.1.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.0":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.2.0/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.2.0","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"8aac0db302116371171c20ba166039827434b1ac9e20e5d0c669213761792052","md5":"512d952714d8c31842c7fc814ea1c8a9","sha256":"7b8ed4aed5b1f1e0627d8a13b01993f3f06aec49b1aaf5fa27f09b32f5c1f63d"},"downloads":-1,"filename":"flash_attention_softmax_n-0.2.0-py3-none-any.whl","has_sig":false,"md5_digest":"512d952714d8c31842c7fc814ea1c8a9","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":26814,"upload_time":"2023-08-29T23:47:18","upload_time_iso_8601":"2023-08-29T23:47:18.456371Z","url":"https://files.pythonhosted.org/packages/8a/ac/0db302116371171c20ba166039827434b1ac9e20e5d0c669213761792052/flash_attention_softmax_n-0.2.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"fd4936da5f1f1d83cf8315da92f8fd7adc055b243814f8c795b39015bf5f7dad","md5":"ec4efe492bf15c78befd7e114e16d481","sha256":"88c62a43aabea49947d8e1b06bdbce04e0b5b60f8d79c0da45b79e8c2bd40f9d"},"downloads":-1,"filename":"flash-attention-softmax-n-0.2.0.tar.gz","has_sig":false,"md5_digest":"ec4efe492bf15c78befd7e114e16d481","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":26130,"upload_time":"2023-08-29T23:47:20","upload_time_iso_8601":"2023-08-29T23:47:20.159463Z","url":"https://files.pythonhosted.org/packages/fd/49/36da5f1f1d83cf8315da92f8fd7adc055b243814f8c795b39015bf5f7dad/flash-attention-softmax-n-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.1":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.2.1/","requires_dist":["torch >=2.0.0","einops >=0.6.1","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.2.1","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"fdb8e381de523f3927c56c43011e0b2f5a2f39bd50012087a2b35d12278315c0","md5":"c91723f325b0d2fdc09bed922db15c1d","sha256":"5af4a1af3a5a8af6ba44e95029f6b37ad8d9767514e4f030ed3f07fb25d20f41"},"downloads":-1,"filename":"flash_attention_softmax_n-0.2.1-py3-none-any.whl","has_sig":false,"md5_digest":"c91723f325b0d2fdc09bed922db15c1d","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":27024,"upload_time":"2023-08-30T14:16:21","upload_time_iso_8601":"2023-08-30T14:16:21.970518Z","url":"https://files.pythonhosted.org/packages/fd/b8/e381de523f3927c56c43011e0b2f5a2f39bd50012087a2b35d12278315c0/flash_attention_softmax_n-0.2.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5bbc90fdc74a3e8dc1c0d1911b7f7a0cffc94e86c9a5ce5cf652d8b13477a34a","md5":"9c547503f013e9ad401c5b9f812436ef","sha256":"eb415fc9e5bf40bf402b0dc0b5006c00b38820d1b79ced0ec257fcf634ffb827"},"downloads":-1,"filename":"flash-attention-softmax-n-0.2.1.tar.gz","has_sig":false,"md5_digest":"9c547503f013e9ad401c5b9f812436ef","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":26314,"upload_time":"2023-08-30T14:16:23","upload_time_iso_8601":"2023-08-30T14:16:23.485100Z","url":"https://files.pythonhosted.org/packages/5b/bc/90fdc74a3e8dc1c0d1911b7f7a0cffc94e86c9a5ce5cf652d8b13477a34a/flash-attention-softmax-n-0.2.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.0":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.3.0/","requires_dist":["torch >=2.0.0","einops >=0.6.1","mosaicml >=0.16.0 ; extra == 'surgery'","transformers <4.33,>=4.11 ; extra == 'surgery'","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.3.0","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"ba49924b5963fcae64e70221f5096659be812ee088d885026cc506036219320c","md5":"43942f4547d3c7e9003502f4980782bc","sha256":"327f3ed8e8a2270d1d2621b3b0c6c1ec1749999ff6116fb7d362d9194cbeb7c3"},"downloads":-1,"filename":"flash_attention_softmax_n-0.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"43942f4547d3c7e9003502f4980782bc","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":33438,"upload_time":"2023-09-05T15:57:33","upload_time_iso_8601":"2023-09-05T15:57:33.783339Z","url":"https://files.pythonhosted.org/packages/ba/49/924b5963fcae64e70221f5096659be812ee088d885026cc506036219320c/flash_attention_softmax_n-0.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"971dc1a27b13be8ddaf60daf86d26e639a314004563c4d8bf4702b50cd1255b4","md5":"40ed656d42b521be4adc308900378c55","sha256":"5b721d18b8fb932a6c33ca8adfea0586b80a100f1af266a93cc30fc25eb79263"},"downloads":-1,"filename":"flash-attention-softmax-n-0.3.0.tar.gz","has_sig":false,"md5_digest":"40ed656d42b521be4adc308900378c55","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":31716,"upload_time":"2023-09-05T15:57:34","upload_time_iso_8601":"2023-09-05T15:57:34.926388Z","url":"https://files.pythonhosted.org/packages/97/1d/c1a27b13be8ddaf60daf86d26e639a314004563c4d8bf4702b50cd1255b4/flash-attention-softmax-n-0.3.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.1":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.3.1/","requires_dist":["torch >=2.0.0","einops >=0.6.1","mosaicml >=0.16.0 ; extra == 'surgery'","transformers <4.33,>=4.11 ; extra == 'surgery'","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.3.1","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"ac97ae7da8ea4fc016b323c91ae0e3b5fbd6015207146d8ec40e07856a0cb1f6","md5":"6927423ec908439c39b27cb328c57a1f","sha256":"268600236c0da6ee33ac62e8020e719d45057fdd2e1a6c1a704d9136483249c9"},"downloads":-1,"filename":"flash_attention_softmax_n-0.3.1-py3-none-any.whl","has_sig":false,"md5_digest":"6927423ec908439c39b27cb328c57a1f","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":33459,"upload_time":"2023-09-23T19:06:47","upload_time_iso_8601":"2023-09-23T19:06:47.473632Z","url":"https://files.pythonhosted.org/packages/ac/97/ae7da8ea4fc016b323c91ae0e3b5fbd6015207146d8ec40e07856a0cb1f6/flash_attention_softmax_n-0.3.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"d248b8cda31048d822604c18ecb469716f379fdde3b2a4ea8765cecab86f10ea","md5":"9e44342fb8c311b66860c897f2f65ba2","sha256":"8b2da1cfb218a8724323c0e0463dcd2307d2f17f463773414ce7a7abaaf96698"},"downloads":-1,"filename":"flash-attention-softmax-n-0.3.1.tar.gz","has_sig":false,"md5_digest":"9e44342fb8c311b66860c897f2f65ba2","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":31736,"upload_time":"2023-09-23T19:06:49","upload_time_iso_8601":"2023-09-23T19:06:49.161302Z","url":"https://files.pythonhosted.org/packages/d2/48/b8cda31048d822604c18ecb469716f379fdde3b2a4ea8765cecab86f10ea/flash-attention-softmax-n-0.3.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.2":{"info":{"author":"Christopher W. Murphy","author_email":"murphtron5000@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU General Public License v3 (GPLv3)","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/softmax1/Flash-Attention-Softmax-N","keywords":"artificial intelligence,attention mechanism,transformers","license":"GPLv3","maintainer":"","maintainer_email":"","name":"flash-attention-softmax-n","package_url":"https://pypi.org/project/flash-attention-softmax-n/","platform":null,"project_url":"https://pypi.org/project/flash-attention-softmax-n/","project_urls":{"Homepage":"https://github.com/softmax1/Flash-Attention-Softmax-N"},"provides_extra":null,"release_url":"https://pypi.org/project/flash-attention-softmax-n/0.3.2/","requires_dist":["torch >=2.0.0","einops >=0.6.1","mosaicml >=0.16.0 ; extra == 'surgery'","transformers <4.33,>=4.11 ; extra == 'surgery'","triton >=2.0.0 ; extra == 'triton'"],"requires_python":">=3.9","summary":"CUDA and Triton implementations of Flash Attention with SoftmaxN.","version":"0.3.2","yanked":false,"yanked_reason":null},"last_serial":20732920,"urls":[{"comment_text":"","digests":{"blake2b_256":"cda6ff3a922ebc2ca1a51e7974112fb685411e18fed718f5eb428242f3048b26","md5":"8637e06aea92f8e2c51b60fe9fdeaffd","sha256":"f41d9dabe136d0c74a35ba247bb88b4f97cb281b4c5c5a1249af0cc790ae9596"},"downloads":-1,"filename":"flash_attention_softmax_n-0.3.2-py3-none-any.whl","has_sig":false,"md5_digest":"8637e06aea92f8e2c51b60fe9fdeaffd","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.9","size":34627,"upload_time":"2023-11-21T14:15:27","upload_time_iso_8601":"2023-11-21T14:15:27.993227Z","url":"https://files.pythonhosted.org/packages/cd/a6/ff3a922ebc2ca1a51e7974112fb685411e18fed718f5eb428242f3048b26/flash_attention_softmax_n-0.3.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c1f24d5520611214fe18a8163f798532b5b7d43fca191b11a811f0441135cc83","md5":"69fabf8d40f247fc9183d65e24a36c89","sha256":"81a488745c58aa4ab6915a49eb24ea5b3e4db72e9454db282f9b618f62bb1ae7"},"downloads":-1,"filename":"flash-attention-softmax-n-0.3.2.tar.gz","has_sig":false,"md5_digest":"69fabf8d40f247fc9183d65e24a36c89","packagetype":"sdist","python_version":"source","requires_python":">=3.9","size":32525,"upload_time":"2023-11-21T14:15:29","upload_time_iso_8601":"2023-11-21T14:15:29.597552Z","url":"https://files.pythonhosted.org/packages/c1/f2/4d5520611214fe18a8163f798532b5b7d43fca191b11a811f0441135cc83/flash-attention-softmax-n-0.3.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}