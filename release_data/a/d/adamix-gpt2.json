{"0.0.1":{"info":{"author":"Sahaj Agarwal","author_email":"sahagar@microsoft.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/AdaMix","keywords":"python,adamix,peft","license":"","maintainer":"","maintainer_email":"","name":"adamix-gpt2","package_url":"https://pypi.org/project/adamix-gpt2/","platform":null,"project_url":"https://pypi.org/project/adamix-gpt2/","project_urls":{"Homepage":"https://github.com/microsoft/AdaMix"},"provides_extra":null,"release_url":"https://pypi.org/project/adamix-gpt2/0.0.1/","requires_dist":["transformers (==3.3.1)","spacy","tqdm","tensorboard","progress","loralib (==0.1.1)"],"requires_python":">=3.6","summary":"PyTorch implementation of low-rank adaptation (LoRA) and Adamix, a parameter-efficient approach to adapt a large pre-trained deep learning model which obtains performance better than full fine-tuning.","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":14968492,"urls":[{"comment_text":"","digests":{"blake2b_256":"7f7f240da6f7929f765db6f0142c0ebd6dc3493877890652708a1b14ac047768","md5":"d3988b70e552e659be8e10fe00f3fce7","sha256":"694eb83399c74529aa34b6103b2e51c1fbc5e0ee89257a33d97eac7300c40421"},"downloads":-1,"filename":"adamix_gpt2-0.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"d3988b70e552e659be8e10fe00f3fce7","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6","size":37359,"upload_time":"2022-08-31T21:36:45","upload_time_iso_8601":"2022-08-31T21:36:45.399935Z","url":"https://files.pythonhosted.org/packages/7f/7f/240da6f7929f765db6f0142c0ebd6dc3493877890652708a1b14ac047768/adamix_gpt2-0.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9901ec94715301ad1454b275c555b2358b339d9ed7e745925b0f26cec01e7e5a","md5":"39eedda2435cf76a30f92f389c7823c3","sha256":"b3d10e2eed7cf229195556bbcb46880bb2f67b1a7a9af880b5f0d79d50ad2b2b"},"downloads":-1,"filename":"adamix_gpt2-0.0.1.tar.gz","has_sig":false,"md5_digest":"39eedda2435cf76a30f92f389c7823c3","packagetype":"sdist","python_version":"source","requires_python":">=3.6","size":31436,"upload_time":"2022-08-31T21:36:46","upload_time_iso_8601":"2022-08-31T21:36:46.827265Z","url":"https://files.pythonhosted.org/packages/99/01/ec94715301ad1454b275c555b2358b339d9ed7e745925b0f26cec01e7e5a/adamix_gpt2-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.0.2":{"info":{"author":"Sahaj Agarwal","author_email":"sahagar@microsoft.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/AdaMix","keywords":"python,adamix,peft","license":"","maintainer":"","maintainer_email":"","name":"adamix-gpt2","package_url":"https://pypi.org/project/adamix-gpt2/","platform":null,"project_url":"https://pypi.org/project/adamix-gpt2/","project_urls":{"Homepage":"https://github.com/microsoft/AdaMix"},"provides_extra":null,"release_url":"https://pypi.org/project/adamix-gpt2/0.0.2/","requires_dist":["transformers (==3.3.1)","spacy","tqdm","tensorboard","progress","loralib (==0.1.1)"],"requires_python":">=3.6","summary":"PyTorch implementation of low-rank adaptation (LoRA) and Adamix, a parameter-efficient approach to adapt a large pre-trained deep learning model which obtains performance better than full fine-tuning.","version":"0.0.2","yanked":false,"yanked_reason":null},"last_serial":14968492,"urls":[{"comment_text":"","digests":{"blake2b_256":"e7f2f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe","md5":"0c462b0f65f4801603da9d4f3fff53e7","sha256":"9fa4b6a30be905d5b0fd4dc733d78a5cddc7d112a579610a61befbf14eafb4f4"},"downloads":-1,"filename":"adamix_gpt2-0.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"0c462b0f65f4801603da9d4f3fff53e7","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6","size":37369,"upload_time":"2022-09-02T00:08:45","upload_time_iso_8601":"2022-09-02T00:08:45.017432Z","url":"https://files.pythonhosted.org/packages/e7/f2/f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe/adamix_gpt2-0.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ab5c625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea","md5":"ab60efd5a06831ba82f937a9e84d227b","sha256":"b7d4db5ee496b3ba60dd951d03e1b9cba4cac175152202994df1b349baab9ab5"},"downloads":-1,"filename":"adamix_gpt2-0.0.2.tar.gz","has_sig":false,"md5_digest":"ab60efd5a06831ba82f937a9e84d227b","packagetype":"sdist","python_version":"source","requires_python":">=3.6","size":31455,"upload_time":"2022-09-02T00:08:46","upload_time_iso_8601":"2022-09-02T00:08:46.753896Z","url":"https://files.pythonhosted.org/packages/ab/5c/625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea/adamix_gpt2-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}