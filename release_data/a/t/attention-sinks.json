{"0.0.1":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.0.1/","requires_dist":["torch","transformers >=4.32.0","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.0.1","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"1bc661443d3cfdc7d1509d74526a0d25f50756e5878c3b712eac40c57879e1c1","md5":"ecd42d7318d2a7cfc5b1a2e8192800e8","sha256":"59fc883ae53eb807e45114e4e5b39b84a3e23d12126f2deff7195cd8d8c7bc9d"},"downloads":-1,"filename":"attention_sinks-0.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"ecd42d7318d2a7cfc5b1a2e8192800e8","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":11988,"upload_time":"2023-10-02T20:32:42","upload_time_iso_8601":"2023-10-02T20:32:42.419010Z","url":"https://files.pythonhosted.org/packages/1b/c6/61443d3cfdc7d1509d74526a0d25f50756e5878c3b712eac40c57879e1c1/attention_sinks-0.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"0f65ce134e9bc0c6f0afa2fc924128d1ae2f6687480509f8708c7e2679e1c4dc","md5":"94e8bc8071bafe8e11b4f1cc258ab89a","sha256":"d1019c0a691374538acded0ffc8914f81c05bda92c1d3f898506797ea8d2ed62"},"downloads":-1,"filename":"attention_sinks-0.0.1.tar.gz","has_sig":false,"md5_digest":"94e8bc8071bafe8e11b4f1cc258ab89a","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":11541,"upload_time":"2023-10-02T20:33:23","upload_time_iso_8601":"2023-10-02T20:33:23.120327Z","url":"https://files.pythonhosted.org/packages/0f/65/ce134e9bc0c6f0afa2fc924128d1ae2f6687480509f8708c7e2679e1c4dc/attention_sinks-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.0":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.1.0/","requires_dist":["torch","transformers >=4.32.0","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.1.0","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"af579ba47c603785d5ab02e2330985cae9bcf330247e90b7a201aba8ea3f788d","md5":"d857f9ebc6adc3897e5ad4c05b6ce9e8","sha256":"d39adc01663526bba2379cd3b8afa298d4188b49f1fc6b26b7977dbe9d2739e6"},"downloads":-1,"filename":"attention_sinks-0.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"d857f9ebc6adc3897e5ad4c05b6ce9e8","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":17102,"upload_time":"2023-10-03T09:47:23","upload_time_iso_8601":"2023-10-03T09:47:23.565032Z","url":"https://files.pythonhosted.org/packages/af/57/9ba47c603785d5ab02e2330985cae9bcf330247e90b7a201aba8ea3f788d/attention_sinks-0.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"993ab84f66018333c68e52128787e08fe40c3701bd1d2b7a3141ca9a82515e29","md5":"dcda6985791ecf710accaa0296213069","sha256":"4b62deb27d8210035c11f866b9f4251fcd4400670003f169a60defca92026739"},"downloads":-1,"filename":"attention_sinks-0.1.0.tar.gz","has_sig":false,"md5_digest":"dcda6985791ecf710accaa0296213069","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":15674,"upload_time":"2023-10-03T09:47:25","upload_time_iso_8601":"2023-10-03T09:47:25.095317Z","url":"https://files.pythonhosted.org/packages/99/3a/b84f66018333c68e52128787e08fe40c3701bd1d2b7a3141ca9a82515e29/attention_sinks-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.0":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.2.0/","requires_dist":["torch","transformers >=4.34.0","tokenizers <0.15,>=0.14","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.2.0","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"211b6208ba90b00c8a89c3b92173555131335790d0e51e9c75ea89225e917c5c","md5":"2e3137884074f0975f41104f28b90493","sha256":"065c1748a0d3a31fb11aa88b2c56d828f06a6c7d2f949aba69dd8bdfff76bbeb"},"downloads":-1,"filename":"attention_sinks-0.2.0-py3-none-any.whl","has_sig":false,"md5_digest":"2e3137884074f0975f41104f28b90493","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":26560,"upload_time":"2023-10-03T17:38:45","upload_time_iso_8601":"2023-10-03T17:38:45.612955Z","url":"https://files.pythonhosted.org/packages/21/1b/6208ba90b00c8a89c3b92173555131335790d0e51e9c75ea89225e917c5c/attention_sinks-0.2.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f6c2b075fb485d2ae8d0ff2e2d6fe5d1356da0dad8f92703a1083e03b6a653f8","md5":"76b9fc36ccafa14fc41af030151b60d3","sha256":"756153942e181ffa8820e93bc51f8d9ff7cd3de627f215e7631f99b7e8ca381c"},"downloads":-1,"filename":"attention_sinks-0.2.0.tar.gz","has_sig":false,"md5_digest":"76b9fc36ccafa14fc41af030151b60d3","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":19684,"upload_time":"2023-10-03T17:38:47","upload_time_iso_8601":"2023-10-03T17:38:47.283665Z","url":"https://files.pythonhosted.org/packages/f6/c2/b075fb485d2ae8d0ff2e2d6fe5d1356da0dad8f92703a1083e03b6a653f8/attention_sinks-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.2":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.2.2/","requires_dist":["torch","transformers ==4.34.0","tokenizers <0.15,>=0.14","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.2.2","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"9f18e785e18b87033fcc2df8c16a1ff308758bd8c4ef2a6106b39f8f23a423b6","md5":"f2c3282a49e1fe0088a28b597f63a059","sha256":"9a74bb51f4900c7fe096d25ecff326ccb2ebeb57906601d82a5d743f01857c9a"},"downloads":-1,"filename":"attention_sinks-0.2.2-py3-none-any.whl","has_sig":false,"md5_digest":"f2c3282a49e1fe0088a28b597f63a059","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":28920,"upload_time":"2023-10-03T23:16:55","upload_time_iso_8601":"2023-10-03T23:16:55.566710Z","url":"https://files.pythonhosted.org/packages/9f/18/e785e18b87033fcc2df8c16a1ff308758bd8c4ef2a6106b39f8f23a423b6/attention_sinks-0.2.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"acc96b6da3d17436f2778a838e74d97042250193d73c54344e41c731fb1bfcb4","md5":"1ba2ca334e00cbe8822e81c4b42141ef","sha256":"dce40ca01b9c155949c29ccf562563a63e152f974ec015970e6785ff32b83e6c"},"downloads":-1,"filename":"attention_sinks-0.2.2.tar.gz","has_sig":false,"md5_digest":"1ba2ca334e00cbe8822e81c4b42141ef","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":22732,"upload_time":"2023-10-03T23:16:57","upload_time_iso_8601":"2023-10-03T23:16:57.177879Z","url":"https://files.pythonhosted.org/packages/ac/c9/6b6da3d17436f2778a838e74d97042250193d73c54344e41c731fb1bfcb4/attention_sinks-0.2.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.2.3":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.2.3/","requires_dist":["torch","transformers ==4.34.0","tokenizers <0.15,>=0.14","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.2.3","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"b392e147b47ea2df019b3922d431b7a7dffb89c117a064a60c027b816f1b2f36","md5":"931de6c2ac1fc42d00afab4767ccffd2","sha256":"578d4d0c3ba22072c12403a63b356686b857cd7d194a33de86de95bd37a4af33"},"downloads":-1,"filename":"attention_sinks-0.2.3-py3-none-any.whl","has_sig":false,"md5_digest":"931de6c2ac1fc42d00afab4767ccffd2","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":35089,"upload_time":"2023-10-10T18:27:26","upload_time_iso_8601":"2023-10-10T18:27:26.608188Z","url":"https://files.pythonhosted.org/packages/b3/92/e147b47ea2df019b3922d431b7a7dffb89c117a064a60c027b816f1b2f36/attention_sinks-0.2.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f50463fd78a3f9efece8cdcf9b27d61763836267630154d1bb63425071fc7122","md5":"b4d0036ed77660e7f27505ae2b839aa9","sha256":"d1506a2563ab6a580db46feb2577a60c23e1f65ca196af8e0cc3cc8b6d7a5561"},"downloads":-1,"filename":"attention_sinks-0.2.3.tar.gz","has_sig":false,"md5_digest":"b4d0036ed77660e7f27505ae2b839aa9","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":29365,"upload_time":"2023-10-10T18:27:27","upload_time_iso_8601":"2023-10-10T18:27:27.766845Z","url":"https://files.pythonhosted.org/packages/f5/04/63fd78a3f9efece8cdcf9b27d61763836267630154d1bb63425071fc7122/attention_sinks-0.2.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.3.0":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.3.0/","requires_dist":["torch","transformers ==4.34.0","tokenizers <0.15,>=0.14","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.3.0","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"f7a33d33ea70ad70556bb1cf48c9afb59403f44d1ce500b8bfc0e75c76afb7b1","md5":"0d4dfee1583a5b3e4518494228a15780","sha256":"8aa03da234039d587abc2644a41c3916c7c4ca772528417389ee260d46bf8028"},"downloads":-1,"filename":"attention_sinks-0.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"0d4dfee1583a5b3e4518494228a15780","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":33612,"upload_time":"2023-10-19T08:20:42","upload_time_iso_8601":"2023-10-19T08:20:42.303666Z","url":"https://files.pythonhosted.org/packages/f7/a3/3d33ea70ad70556bb1cf48c9afb59403f44d1ce500b8bfc0e75c76afb7b1/attention_sinks-0.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"fb712a0e82e2194693b054616afd10f9ec5fb64e94d1d5f3a9c5cf497aea90df","md5":"542ad9272cee17d9052e4182f96fb67c","sha256":"3cf28dc13df8b04171c8a58bd78f003b545796ed1e1cc7a4cfa13ea93c2b2d0d"},"downloads":-1,"filename":"attention_sinks-0.3.0.tar.gz","has_sig":false,"md5_digest":"542ad9272cee17d9052e4182f96fb67c","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":31161,"upload_time":"2023-10-19T08:20:44","upload_time_iso_8601":"2023-10-19T08:20:44.007549Z","url":"https://files.pythonhosted.org/packages/fb/71/2a0e82e2194693b054616afd10f9ec5fb64e94d1d5f3a9c5cf497aea90df/attention_sinks-0.3.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.4.0":{"info":{"author":"Tom Aarsen","author_email":"","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers","license":"Apache-2.0","maintainer":"Tom Aarsen","maintainer_email":"","name":"attention-sinks","package_url":"https://pypi.org/project/attention-sinks/","platform":null,"project_url":"https://pypi.org/project/attention-sinks/","project_urls":{"Repository":"https://github.com/tomaarsen/attention_sinks"},"provides_extra":null,"release_url":"https://pypi.org/project/attention-sinks/0.4.0/","requires_dist":["torch","transformers ==4.34.0","tokenizers <0.15,>=0.14","pre-commit ; extra == 'dev'","ruff ; extra == 'dev'","black ; extra == 'dev'","pytest ; extra == 'dev'","pytest-cov ; extra == 'dev'","spacy ; extra == 'dev'"],"requires_python":">=3.8","summary":"Extend LLMs to infinite length without sacrificing efficiency and performance, without retraining","version":"0.4.0","yanked":false,"yanked_reason":null},"last_serial":20764990,"urls":[{"comment_text":"","digests":{"blake2b_256":"fe33ca8ed6c8fbd72c18fac6f9f692698f96378d1e2e7d09986a51a23d87f0fc","md5":"d15c559b080fa53112bc788664424313","sha256":"e305b536376dab8c2cf40ee363805c54d7949a720c7bf6e5fb0fc825e0b127f2"},"downloads":-1,"filename":"attention_sinks-0.4.0-py3-none-any.whl","has_sig":false,"md5_digest":"d15c559b080fa53112bc788664424313","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.8","size":35799,"upload_time":"2023-11-23T12:11:01","upload_time_iso_8601":"2023-11-23T12:11:01.211763Z","url":"https://files.pythonhosted.org/packages/fe/33/ca8ed6c8fbd72c18fac6f9f692698f96378d1e2e7d09986a51a23d87f0fc/attention_sinks-0.4.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5125c64ff4e91bb78ef0dd30d5e8b0c46e26c48a4675fb7291fa9ef9a2c13b15","md5":"9666ce79e9ccf20c42e82efacfe37b02","sha256":"d27f0238f00128dd036a6b209a65890654b08490249f48f335a8e42191fa21c8"},"downloads":-1,"filename":"attention_sinks-0.4.0.tar.gz","has_sig":false,"md5_digest":"9666ce79e9ccf20c42e82efacfe37b02","packagetype":"sdist","python_version":"source","requires_python":">=3.8","size":32548,"upload_time":"2023-11-23T12:11:05","upload_time_iso_8601":"2023-11-23T12:11:05.174647Z","url":"https://files.pythonhosted.org/packages/51/25/c64ff4e91bb78ef0dd30d5e8b0c46e26c48a4675fb7291fa9ef9a2c13b15/attention_sinks-0.4.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}