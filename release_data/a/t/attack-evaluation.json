{"0.1":{"info":{"author":"Sahbaaz Ansari","author_email":"mlsecadversarialattack@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Cyberus-MLSec/eval_adv_attack","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"attack-evaluation","package_url":"https://pypi.org/project/attack-evaluation/","platform":null,"project_url":"https://pypi.org/project/attack-evaluation/","project_urls":{"Homepage":"https://github.com/Cyberus-MLSec/eval_adv_attack"},"provides_extra":null,"release_url":"https://pypi.org/project/attack-evaluation/0.1/","requires_dist":["torch","torchattacks","matplotlib","numpy"],"requires_python":null,"summary":"A package for evaluating adversarial attacks on deep learning models","version":"0.1","yanked":false,"yanked_reason":null},"last_serial":24250827,"urls":[{"comment_text":"","digests":{"blake2b_256":"024ca77d5df3d09be961d9f5e983df8c00ded82aba103ebe81fae46164f7113b","md5":"dc344d0583b1588f2e297b7903280d4d","sha256":"dec10f4f99a58217c1b756aa9d69d8a693c48c880a5c580b6efccb69af8810ad"},"downloads":-1,"filename":"attack_evaluation-0.1-py3-none-any.whl","has_sig":false,"md5_digest":"dc344d0583b1588f2e297b7903280d4d","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":2578,"upload_time":"2024-07-23T08:11:15","upload_time_iso_8601":"2024-07-23T08:11:15.650815Z","url":"https://files.pythonhosted.org/packages/02/4c/a77d5df3d09be961d9f5e983df8c00ded82aba103ebe81fae46164f7113b/attack_evaluation-0.1-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]},"0.1.1":{"info":{"author":"Sahbaaz Ansari","author_email":"mlsecadversarialattack@gmail.com","bugtrack_url":null,"classifiers":[],"description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/Cyberus-MLSec/eval_adv_attack","keywords":null,"license":null,"maintainer":null,"maintainer_email":null,"name":"attack-evaluation","package_url":"https://pypi.org/project/attack-evaluation/","platform":null,"project_url":"https://pypi.org/project/attack-evaluation/","project_urls":{"Homepage":"https://github.com/Cyberus-MLSec/eval_adv_attack"},"provides_extra":null,"release_url":"https://pypi.org/project/attack-evaluation/0.1.1/","requires_dist":["torch","torchattacks","matplotlib","numpy"],"requires_python":null,"summary":"A package for evaluating adversarial attacks on deep learning models","version":"0.1.1","yanked":false,"yanked_reason":null},"last_serial":24250827,"urls":[{"comment_text":"","digests":{"blake2b_256":"0c608b4fbb2173f9ff7e8c4e4898746a8613195e792fa82932f2b62ce01b24b3","md5":"8e6393f607b4ead56364ee9ecd01b59e","sha256":"1f749a9a80ca26063458329f0866b7ef63fa28e6e54a67f34828e7776ab98f9b"},"downloads":-1,"filename":"attack_evaluation-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"8e6393f607b4ead56364ee9ecd01b59e","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":2597,"upload_time":"2024-07-23T08:13:11","upload_time_iso_8601":"2024-07-23T08:13:11.429127Z","url":"https://files.pythonhosted.org/packages/0c/60/8b4fbb2173f9ff7e8c4e4898746a8613195e792fa82932f2b62ce01b24b3/attack_evaluation-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}}